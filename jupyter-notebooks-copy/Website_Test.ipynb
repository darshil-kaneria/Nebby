{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbb5279d",
   "metadata": {},
   "source": [
    "# Code to run website tests\n",
    "### The model parameters are already trained using the Training Visualization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79f4abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_i=1\n",
    "post_i=2\n",
    "bw_i=3\n",
    "bf_i=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bcd677",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.fft import rfft, rfftfreq\n",
    "from scipy.fft import irfft\n",
    "import numpy as np\n",
    "import statistics\n",
    "from statistics import mean, pstdev\n",
    "import pandas as pd\n",
    "\n",
    "yellow = '\\033[93m'\n",
    "green = '\\033[92m'\n",
    "red = '\\033[91m'\n",
    "blue = '\\033[94m'\n",
    "pink = '\\033[95m'\n",
    "black = '\\033[90m'\n",
    "\n",
    "def smoothen(time, data, rtt):\n",
    "    # Smoothening \n",
    "    left = 0\n",
    "    right = 0\n",
    "    run_sum = 0\n",
    "    avg_data = []\n",
    "    new_time = []\n",
    "    roll_time = time\n",
    "    roll_data = data\n",
    "    while right < len(roll_time):\n",
    "        while(right < len(roll_time) and (roll_time[right]-roll_time[left] < 2*rtt)):\n",
    "            run_sum+=roll_data[right]\n",
    "            right+=1\n",
    "        new_time.append(float(roll_time[right-1]+roll_time[left])/2)\n",
    "        avg_data.append(float(run_sum)/(right-left))\n",
    "        run_sum-=roll_data[left]\n",
    "        left+=1\n",
    "    return new_time, avg_data\n",
    "\n",
    "\n",
    "def get_fft(data):\n",
    "    n = len(data)\n",
    "    data_step = 0.002\n",
    "    yf = rfft(data)\n",
    "    xf = rfftfreq(n,data_step)\n",
    "    return yf,xf\n",
    "\n",
    "def get_fft_smoothening(data, time, ax,rtt,p):\n",
    "    rtt=rtt\n",
    "    yf, xf = get_fft(data)\n",
    "    thresh  = (1/rtt)\n",
    "    thresh_ind = 0\n",
    "    for i in range(len(xf)) :\n",
    "        freq = xf[i]\n",
    "        if(freq > thresh):\n",
    "            thresh_ind = i\n",
    "            break\n",
    "            \n",
    "    yf_clean = yf\n",
    "    yf_clean[thresh_ind+1:] = 0\n",
    "    new_f_clean = irfft(yf_clean)\n",
    "    start_len = len(time) - len(new_f_clean)\n",
    "\n",
    "    plot_data = new_f_clean\n",
    "#     if p==\"y\":\n",
    "#         ax.plot(time[start_len:], plot_data, 'k', label='FFT smoothening', linewidth=1.5)\n",
    "\n",
    "    plot_time = time[start_len:] \n",
    "    return plot_time, plot_data\n",
    "\n",
    "def plot_d(ax, time, data, c, l, alpha=1):\n",
    "    ax.plot(time, data, color=c, lw=2, label = l,alpha=alpha)\n",
    "\n",
    "\n",
    "def plot_one_bt(f, p,t=1):\n",
    "#     print(f)\n",
    "    file_name = f.split(\"/\")[-1]\n",
    "    fs = file_name.split(\"-\")\n",
    "\n",
    "    pre = int(fs[1])\n",
    "    post = int(fs[2])\n",
    "    rtt = float(((pre+post)*2))/1000\n",
    "    ax = 0\n",
    "    if t==1:\n",
    "        data, time, retrans = get_window(f,\"n\",t)\n",
    "    elif t==2:\n",
    "        data, time, retrans, OOA, DA = get_window(f,\"n\",t)\n",
    "    if p == 'y':\n",
    "        fig, ax = plt.subplots(1,1, figsize=(15,8))\n",
    "        for t in retrans :\n",
    "            plt.axvline(x = t, color = 'm',alpha=0.5)\n",
    "        if t == 2:\n",
    "            for t in OOA :\n",
    "                plt.axvline(x = t, color = 'k', lw=2)\n",
    "            for t in DA:\n",
    "                plt.axvline(x = t, color = 'g', lw=0.5, alpha = 0.5)\n",
    "        plot_d(ax,time,data, \"r\",\"Original\")\n",
    "    time, data = get_fft_smoothening(data, time, ax,rtt,\"y\")\n",
    "\n",
    "    #         plot_d(ax,time,data,\"b\",\"FFT Smoothened\" )\n",
    "#         print(len(time), len(data))\n",
    "    time, data = smoothen(time, data, rtt)\n",
    "#         print(len(time), len(data))\n",
    "    if p == 'y':\n",
    "        plot_d(ax, time, data, \"b\", \"Smoothened\",alpha=0.5)\n",
    "        ax.legend()\n",
    "#             plt.savefig(\"./plots/\"+f+\".png\")\n",
    "        plt.show()\n",
    "#     return time, data, grad_time, grad_data, rtt\n",
    "#     print(\"Black : OOA, Green : DA, Magenta : RP\")\n",
    "    return time, data, retrans, rtt \n",
    "\n",
    "\n",
    "def get_time_features(retrans,time,rtt):\n",
    "    time_thresh = 20*rtt\n",
    "    features = []\n",
    "    for i in range(1, len(retrans)):\n",
    "        if retrans[i]-retrans[i-1] >= time_thresh:\n",
    "            features.append([retrans[i-1], retrans[i]])\n",
    "    # add a feature that finished when the experiment ends\n",
    "    if len(retrans)>0:\n",
    "        if time[-1] - retrans[-1] > 20*rtt :\n",
    "            features.append([retrans[-1],time[-1]])\n",
    "    return features\n",
    "\n",
    "def get_features(time, features):\n",
    "    left = 0\n",
    "    right = 0\n",
    "    feature_index = 0\n",
    "    in_feature = 0\n",
    "    index_features = []\n",
    "    while right < len(time) and feature_index < len(features): \n",
    "        if in_feature == 0 and time[right]>=features[feature_index][0]:\n",
    "            in_feature = 1\n",
    "            left = right\n",
    "        elif in_feature == 1 and time[right] > features[feature_index][1]:\n",
    "            in_feature = 0\n",
    "            index_features.append([left, right-1])\n",
    "            feature_index+=1\n",
    "        right+=1\n",
    "    if in_feature == 1:\n",
    "        index_features.append([left, right-1])\n",
    "    return index_features\n",
    "\n",
    "def get_plot_features(curr_file, p):\n",
    "    time, data, retrans, rtt = plot_one_bt(curr_file,p=p,t=1)\n",
    "    time_features = get_time_features(retrans,time,rtt)\n",
    "    features = get_features(time, time_features)    \n",
    "    if p == 'y':\n",
    "        fig, ax = plt.subplots(1,1, figsize=(15,8))\n",
    "        plot_d(ax, time, data, \"b\", \"Smoothened\")\n",
    "        for ft in features : \n",
    "#             print(time[ft[1]]-time[ft[0]])\n",
    "            ax.plot(time[ft[0]:ft[1]+1], data[ft[0]:ft[1]+1], color = 'r')\n",
    "        plt.show()\n",
    "    return time, data, features\n",
    "\n",
    "\n",
    "SHOW=True\n",
    "MULTI_GRAPH=False\n",
    "SMOOTHENING=False\n",
    "ONLY_STATS=False\n",
    "s_factor=0.9\n",
    "\n",
    "\n",
    "PKT_SIZE = 88\n",
    "\n",
    "\n",
    "'''\n",
    "TODO: \n",
    "o Add functionality where you only plot flows that send more than x bytes of data\n",
    "o Sort stats and graphs by flow size\n",
    "o Organize plots by flow size (larger flows have larger graphs)\n",
    "o Custom smoothening function\n",
    "'''\n",
    "\n",
    "fields=[\"time\", \"frame_time_rel\", \"tcp_time_rel\", \"frame_num\", \"frame_len\", \"ip_src\", \"src_port\", \"ip_dest\", \"dest_port\", \"tcp_len\", \"seq\", \"ack\"]\n",
    "\n",
    "class pkt:\n",
    "    contents=[]\n",
    "    def __init__(self, fields) -> None:\n",
    "        self.contents=[]\n",
    "        for f in fields:\n",
    "            self.contents.append(f)\n",
    "\n",
    "    def get(self, field):\n",
    "        return self.contents[fields.index(field)]\n",
    "        \n",
    "\n",
    "def process_flows(cc, dir,p=\"y\"):\n",
    "    name = dir+cc+\"-tcp.csv\"\n",
    "    with open(name) as csv_file:\n",
    "        print(\"Reading \"+name+\"...\")\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "        line_count = 0\n",
    "        total_bytes=0\n",
    "        '''\n",
    "        Flow tracking:\n",
    "        o Identify all packets that are either sourced from or headed to 100.64.0.2\n",
    "        o Group different flows by client's port\n",
    "        '''\n",
    "        flows={}\n",
    "        data_sent=0\n",
    "        # ACK and RTX measurement\n",
    "        ooa = set()\n",
    "        rp = set()\n",
    "        ooaCount=0\n",
    "        rpCount=0\n",
    "        daCount=0\n",
    "        backAck=0\n",
    "        for row in csv_reader:\n",
    "            reTx=0\n",
    "            \n",
    "            ooAck=0\n",
    "            dupAck=0\n",
    "            retPacket=0\n",
    "            \n",
    "            packet=pkt(row)\n",
    "            ackPkt=False\n",
    "            validPkt=False\n",
    "            if line_count==0:\n",
    "                # reject the header\n",
    "                line_count+=1\n",
    "                continue\n",
    "            if data_sent == 0 : \n",
    "                if \"100.64.0.\" in packet.get(\"ip_src\"):\n",
    "                    num = int(packet.get(\"ip_src\")[-1])\n",
    "                    if num%2==0:\n",
    "                        data_sent=1\n",
    "                        host_port=packet.get(\"ip_src\")\n",
    "                if \"100.64.0.\" in packet.get(\"ip_dest\"):\n",
    "                    num = int(packet.get(\"ip_dest\")[-1])\n",
    "                    if num%2==0:\n",
    "                        data_sent=1\n",
    "                        host_port=packet.get(\"ip_dest\")\n",
    "                if data_sent == 0:\n",
    "                    continue\n",
    "            if packet.get(\"ip_src\")==host_port and packet.get(\"frame_time_rel\")!='' and packet.get(\"ack\")!='': \n",
    "                # we care about this ACK packet\n",
    "                validPkt=True\n",
    "                ackPkt=True\n",
    "                port=packet.get(\"src_port\")\n",
    "                #PORTCHECK\n",
    "#                 if int(port) != 50468:\n",
    "#                     continue\n",
    "                if port not in flows:\n",
    "                    flows[port]={\"OOA\":[],\"DA\":[],\"max_seq\":0,\"loss_bif\":0,\"max_ack\":int(packet.get(\"ack\")),\"serverip\":packet.get(\"ip_dest\"), \"serverport\":packet.get(\"dest_port\"), \"act_times\":[],\"times\":[], \"windows\":[], \"cwnd\":[], \"bif\":0, \"last_ack\":0, \"last_seq\":0, \"pif\":0, \"drop\":[], \"next\":0, \"retrans\":[]}\n",
    "                else:\n",
    "                    # check for Out of Order Ack (OOA)\n",
    "                    if int(packet.get(\"ack\")) <= int(flows[port][\"max_ack\"]):\n",
    "                        if int(packet.get(\"ack\")) == backAck :\n",
    "                            dupAck = True\n",
    "                            flows[port][\"DA\"].append(float(packet.get(\"frame_time_rel\")))\n",
    "                        else :\n",
    "                            ooAck = True\n",
    "                            flows[port][\"OOA\"].append(float(packet.get(\"frame_time_rel\")))\n",
    "                        backAck = int(packet.get(\"ack\"))\n",
    "                    # update max_ack\n",
    "                    flows[port][\"max_ack\"] = max(flows[port][\"max_ack\"], int(packet.get(\"ack\")))\n",
    "                    if int(packet.get(\"seq\")) < flows[port][\"max_ack\"]:\n",
    "                        reTx += int(packet.get(\"tcp_len\"))\n",
    "#                     flows[port][\"times\"].append(float(packet.get(\"frame_time_rel\")) )\n",
    "                    \n",
    "            elif packet.get(\"ip_dest\")==host_port and packet.get(\"frame_time_rel\")!='' and packet.get(\"seq\")!='':\n",
    "                #we care about this Data packet\n",
    "                validPkt=True\n",
    "                port=packet.get(\"dest_port\")\n",
    "                #PORTCHECK\n",
    "#                 if int(port) != 50468:\n",
    "#                     continue\n",
    "                seq=int(packet.get(\"seq\"))\n",
    "                tcp_len=int(packet.get(\"tcp_len\"))\n",
    "                if port not in flows:\n",
    "                    flows[port]={\"OOA\":[],\"DA\":[],\"max_seq\":int(packet.get(\"seq\")),\"loss_bif\":0,\"max_ack\":0,\"serverip\":packet.get(\"ip_src\"), \"serverport\":packet.get(\"src_port\"),\"act_times\":[], \"times\":[], \"windows\":[], \"cwnd\":[], \"bif\":0, \"last_ack\":0, \"last_seq\":0, \"pif\":0, \"drop\":[], \"next\":0, \"retrans\":[]}\n",
    "                \n",
    "                else:\n",
    "                    flows[port][\"max_seq\"] = max(flows[port][\"max_seq\"], int(packet.get(\"seq\")))\n",
    "                \n",
    "                \n",
    "                if int(packet.get(\"seq\")) < flows[port][\"max_seq\"] :\n",
    "                    retPacket = True\n",
    "                    flows[port][\"retrans\"].append(flows[port][\"times\"][-1])\n",
    "                    \n",
    "            if validPkt==True:\n",
    "                bif = 0\n",
    "                normal_est_bif = int(flows[port][\"max_seq\"]) - int(flows[port][\"max_ack\"]) + PKT_SIZE#+ reTx\n",
    "                loss_est_bif = flows[port][\"loss_bif\"]\n",
    "                if ackPkt and dupAck and len(flows[port][\"windows\"]) > 10:\n",
    "                    if dupAck:\n",
    "                        # if we have received a duplicate ack then we need to reduce the bytes in flight by packet size\n",
    "                        # we also increase max ack to correct for the consolidated ack being sent later\n",
    "                        loss_est_bif = int(flows[port][\"windows\"][-1]) - PKT_SIZE\n",
    "                        flows[port][\"max_ack\"] += PKT_SIZE\n",
    "                        \n",
    "                        bif = min( normal_est_bif, loss_est_bif)\n",
    "                        if p == \"y\" :\n",
    "                            print(green+\"Duplicate Ack\",int(packet.get(\"ack\")),\"Max Ack\",flows[port][\"max_ack\"],\"BIF\",bif)\n",
    "                        \n",
    "#                     elif ooAck:\n",
    "#                         # first out of order ack that we have recieved not a duplicated ack\n",
    "#                         # the reason would be restransimitted packet so dont need to correct for this\n",
    "                        \n",
    "                elif ackPkt :\n",
    "                    loss_est_bif = normal_est_bif \n",
    "                    bif = normal_est_bif    \n",
    "                    if ooAck :\n",
    "                        ooaCount+=1\n",
    "                        if p == \"y\":\n",
    "                            print(red+\"Out of Order Ack\",int(packet.get(\"ack\")),\"Max Ack\",flows[port][\"max_ack\"],\"BIF\",normal_est_bif)\n",
    "                        ooa.add(int(packet.get(\"ack\")))\n",
    "                    else:\n",
    "                        if p == \"y\":\n",
    "                            print(black+\"Inorder Ack\",int(packet.get(\"ack\")),\"Max Seq\",flows[port][\"max_seq\"],\"BIF\",normal_est_bif)\n",
    "                else :\n",
    "                    bif = normal_est_bif\n",
    "                    if retPacket==True:\n",
    "                        rpCount+=1\n",
    "                        rp.add(int(packet.get(\"seq\")))\n",
    "                        if p == \"y\":\n",
    "                            print(pink+\"Retransmitted Packet\",int(packet.get(\"seq\")), \"Next\", flows[port][\"max_seq\"]+PKT_SIZE, \"BIF\",bif)\n",
    "                    else :\n",
    "                        if p == \"y\":\n",
    "                            print(blue+\"Inorder Packet\", int(packet.get(\"seq\")), \"Next\", flows[port][\"max_seq\"]+PKT_SIZE, \"BIF\",bif)\n",
    "                flows[port][\"loss_bif\"] = loss_est_bif\n",
    "                flows[port][\"windows\"].append( int(bif) )\n",
    "                flows[port][\"times\"].append( float(packet.get(\"frame_time_rel\")) )\n",
    "                \n",
    "#                 if ackPkt and dupAck and len(flows[port][\"windows\"]) > 10: # we have received atleast the first window\n",
    "# #                     if len(flows[port][\"windows\"]) < 2000: # print reTx in first 200 packets\n",
    "# #                         print( packet.get(\"ack\"), flows[port][\"max_ack\"])\n",
    "#                     loss_est_bif = int(flows[port][\"windows\"][-1]) - PKT_SIZE\n",
    "#                     flows[port][\"max_ack\"] += PKT_SIZE\n",
    "#                     bif = min( normal_est_bif, loss_est_bif )\n",
    "#                 elif ackPkt:\n",
    "#                     loss_est_bif = normal_est_bif \n",
    "#                     bif = normal_est_bif\n",
    "#                 else:\n",
    "#                     bif = normal_est_bif\n",
    "#                 flows[port][\"loss_bif\"] = loss_est_bif\n",
    "#                 flows[port][\"windows\"].append( int(bif) )\n",
    "            \n",
    "            \n",
    "            line_count+=1\n",
    "            total_bytes+=int(packet.get(\"frame_len\"))\n",
    "            #print(line_count, total_bytes)\n",
    "            \n",
    "#         print(\"total bytes processed:\", total_bytes/1000, \"KBytes for\", cc, \"(unlimited)\")\n",
    "        if p == \"y\":\n",
    "            print(\"Out of Order Acks\",len(ooa),\"Retransmitted Packets\",len(rp))\n",
    "            print(\"Count Out of Order Acks\",ooaCount,\"Retransmitted Packets\",rpCount)\n",
    "            print(\"OOA\",ooa,\"RP\",rp)\n",
    "    return flows\n",
    "\n",
    "def custom_smooth_function():\n",
    "    pass\n",
    "\n",
    "def get_flow_stats(flows):\n",
    "    num=len(flows.keys())\n",
    "    print(\"FLOW STATISTICS: \\nNumber of flows: \", num)\n",
    "    print(\"------------------------------------------------------------------------------\")\n",
    "    print('%6s'%\"port\", '%15s'%\"SrcIP\", '%8s'%\"SrcPort\",  '%8s'%\"duration\",  '%8s'%\"start\",  '%8s'%\"end\", '%8s'%\"Sent (B)\", '%8s'%\"Recv (B)\",)\n",
    "    for k in flows.keys():\n",
    "        print('%6s'%k, '%15s'%flows[k][\"serverip\"], '%8s'%flows[k][\"serverport\"], '%8s'%str('%.2f'%(flows[k][\"times\"][-1]-flows[k][\"times\"][0])), '%8s'%str('%.2f'%flows[k][\"times\"][0]), '%8s'%str('%.2f'%flows[k][\"times\"][-1]), '%8s'%flows[k][\"last_seq\"], '%8s'%flows[k][\"last_ack\"])\n",
    "        #print(\"    * Flow \"+str(k)+\": \", flows[k][\"last_ack\"], \" \", flows[k][\"last_seq\"], \" bytes transfered.\")\n",
    "    return num\n",
    "\n",
    "def run(files):\n",
    "    flows = {}\n",
    "    for f in files:\n",
    "        algo_cc=f\n",
    "        #Get the data for all the flows\n",
    "        print(\"==============================================================================\")\n",
    "        print(\"opening trace ../measurements/\"+algo_cc+\".csv...\")\n",
    "        flows = process_flows(algo_cc, \"./Nebby/measurements-new-btl/50-200-2-60/\")\n",
    "        #decide on final graph layout\n",
    "        num = get_flow_stats(flows)\n",
    "\n",
    "        if ONLY_STATS:\n",
    "            sys.exit()\n",
    "\n",
    "        if num==1:\n",
    "            MULTI_GRAPH=False\n",
    "        #grid size\n",
    "        if MULTI_GRAPH:\n",
    "            size=(0,0)\n",
    "            grids={1:(2,2), 2:(2,2), 4:(2,2), 6:(2,3), 9:(3,3), 12:(3,4), 15:(3,5), 16:(4,4), 20:(5,4), 24:(6,4), 30:(6,5), 36:(6,6), 40:(8,5), 42:(8,7), 49:(7,7)}\n",
    "            g=num\n",
    "            while g<=49 and g not in grids:\n",
    "                g+=1\n",
    "            if g in grids.keys():\n",
    "                size=grids[g]\n",
    "            else:\n",
    "                size=grids[49]  \n",
    "            fig, axs = plt.subplots(size[0], size[1])\n",
    "            for i in range(size[0]):\n",
    "                for j in range(size[1]):\n",
    "                    #axs[i][j].legend(loc=\"lower right\")\n",
    "                    if i==size[0]-1:\n",
    "                        axs[i][j].set_xlabel(\"Time (s)\")\n",
    "                    if j==0:\n",
    "                        axs[i][j].set_ylabel(\"Bytes in flight\")\n",
    "        else:\n",
    "            plt.xlabel(\"Time (s)\")\n",
    "            plt.ylabel(\"Bytes in flight\")\n",
    "        counter=0\n",
    "        for port in flows.keys():\n",
    "            if MULTI_GRAPH:  \n",
    "                axs[counter%size[0]][(counter//size[0])%size[1]].scatter(flows[port][\"times\"], flows[port][\"windows\"], color=\"#858585\")\n",
    "                axs[counter%size[0]][(counter//size[0])%size[1]].plot(flows[port][\"times\"], flows[port][\"windows\"], label=str(port), linestyle=\"solid\")\n",
    "            else:\n",
    "                plt.plot(flows[port][\"times\"], flows[port][\"windows\"], label=str(port), linestyle=\"solid\")\n",
    "                plt.scatter(flows[port][\"times\"], flows[port][\"windows\"], color=\"#858585\")\n",
    "            counter+=1\n",
    "        if MULTI_GRAPH:\n",
    "            counter=0\n",
    "            for port in flows.keys():\n",
    "                axs[counter%size[0]][(counter//size[0])%size[1]].legend()\n",
    "                counter+=1\n",
    "        else:\n",
    "            plt.legend()\n",
    "        if MULTI_GRAPH:\n",
    "            fig.set_size_inches(16, 12)\n",
    "        if SHOW:\n",
    "            plt.show()\n",
    "        else:\n",
    "            plt.savefig(\"../logs/results/\"+algo_cc+\".png\", dpi=600, bbox_inches='tight', pad_inches=0)\n",
    "    return flows\n",
    "\n",
    "\n",
    "def split_path(f):\n",
    "    path = f.split(\"/\")\n",
    "    file_name = path[-1][:-8]\n",
    "    folder_path = \"/\".join(path[:-1])\n",
    "    folder_path = folder_path + \"/\"\n",
    "    algo_cc = file_name\n",
    "    return algo_cc, folder_path\n",
    "\n",
    "def get_window(f,p,t=1):\n",
    "    algo_cc, folder_path = split_path(f)\n",
    "    flows = process_flows(algo_cc, folder_path,p=p)\n",
    "#     flows = process_flows(algo_cc, \"../measurements/m/\",p=p)\n",
    "#     flows = process_flows(algo_cc, \"./Nebby/measurements/\",p=p)    \n",
    "#     flows = process_flows(algo_cc, \"./Nebby/measurements-new-btl/50-200-2-60/\",p=p)\n",
    "#     flows = process_flows(algo_cc, \"./Nebby/measurements-100-150/\",p=p)\n",
    "    params = algo_cc.split(\"-\")\n",
    "    data = []\n",
    "    time = []\n",
    "    drops = []\n",
    "    retrans = []\n",
    "    OOA = []\n",
    "    DA = []\n",
    "    use_port = 0\n",
    "    maxx = 0\n",
    "    print(\"All Ports : \", flows.keys())\n",
    "    for port in flows.keys():\n",
    "        if len(flows[port]['windows']) > maxx:\n",
    "            maxx = len(flows[port]['windows'])\n",
    "            use_port = port\n",
    "    print(\"Port\",use_port)\n",
    "    data = flows[use_port]['windows']\n",
    "    time = flows[use_port]['times']\n",
    "    retrans = flows[use_port]['retrans']\n",
    "    OOA = flows[use_port]['OOA']\n",
    "    DA = flows[use_port]['DA']\n",
    "    if p == \"y\":\n",
    "        plt.plot(time, data)\n",
    "#     time_index = len(time)-1\n",
    "#     for index in range(len(flows[use_port]['times'])-1):\n",
    "#         if flows[use_port]['times'][index+1] - flows[use_port]['times'][index] > :\n",
    "#             time_index = index\n",
    "#     time_last = flows[use_port]['times'][time_index]\n",
    "#     data = data[:time_index+1]\n",
    "#     time = time[:time_index+1]\n",
    "    if t==2:\n",
    "        return data, time, retrans, OOA, DA\n",
    "    if t==1:\n",
    "        return data, time, retrans\n",
    "\n",
    "\n",
    "def getProbes(time, data, rtt, bdp, bw=200):\n",
    "    if bw==200:\n",
    "        thresh = 8*rtt\n",
    "        st_thresh = 0.025\n",
    "        error = 0.08\n",
    "        alpha = 1.10\n",
    "    if bw==1000:\n",
    "        thresh = 4*rtt\n",
    "        st_thresh = 0.025\n",
    "        error = 0.02\n",
    "        alpha = 1\n",
    "    probe_index = []\n",
    "    left = 0\n",
    "    right = 0\n",
    "    bdp_thresh = bdp/2\n",
    "    prev_right = 0\n",
    "    end = 0\n",
    "    while right < len(data):\n",
    "        while right < len(data) and (time[right]-time[left]) < thresh:\n",
    "            right+=1\n",
    "        if right == len(data):\n",
    "            end = 1\n",
    "            right-=1\n",
    "        mid = math.floor(left + (right - left)/2)\n",
    "        mid_val = 0\n",
    "        left_mid = mid\n",
    "        right_mid = mid\n",
    "#         print(left,right)\n",
    "#         while left_mid > 0 and abs(time[mid]-time[left_mid])<rtt:\n",
    "#             if data[left_mid] > data[left] and data[left_mid] > data[right]:\n",
    "#                 mid_val+=1\n",
    "#             left_mid-=1\n",
    "#         while right_mid < len(data)-1 and abs(time[right_mid]-time[mid])<rtt:\n",
    "#             if data[right_mid] > data[left] and data[right_mid] > data[right]:\n",
    "#                 mid_val+=1\n",
    "#             right_mid+=1\n",
    "#         total = right_mid - left_mid\n",
    "#         peak = 0\n",
    "#         if round(float(mid_val)/total,2) > 0.99 :\n",
    "#             print(round(float(mid_val)/total,2))\n",
    "#             peak=1\n",
    "        go = 0\n",
    "        if data[mid] > data[left] and data[mid] > data[right]:\n",
    "#         if peak == 1:\n",
    "            #this  is  peak\n",
    "#             go = 1\n",
    "            t_l = left\n",
    "            t_r = right\n",
    "            while(t_l > 0 and time[left]-time[t_l] < thresh/2):\n",
    "                t_l-=1\n",
    "            while(t_r < len(data)-1 and time[t_r]-time[right] < thresh/2):\n",
    "                t_r+=1\n",
    "            left_sd = round(np.std(data[t_l:left])/(bdp_thresh*2),3)\n",
    "            right_sd = round(np.std(data[right:t_r])/(bdp_thresh*2),3)\n",
    "            if float(abs(data[left]-data[right]))/data[left] < error:\n",
    "                # this has the left and right points not too different from each other\n",
    "                side_avg = float((data[left]+data[right]))/2\n",
    "                local_max = max(data[left:right+1])\n",
    "#                 go = 1\n",
    "#                 print(\"Yes\")\n",
    "                if float(local_max)/side_avg > alpha and local_max > bdp_thresh: \n",
    "                    # this means that the peak is quite steep\n",
    "#                     print(\"This\")\n",
    "#                     go = 1\n",
    "                    if (left_sd < st_thresh) and (right_sd < st_thresh):\n",
    "                        # this means that the lest and right are quite stable respectvely\n",
    "                        go = 1\n",
    "#                         print(\"Out\", left_sd, right_sd, st_thresh)\n",
    "        if go == 1: \n",
    "            try :\n",
    "                probe_index.append([left,right,float(local_max)/side_avg, time[right]-time[left], left_sd, right_sd, t_l,t_r])\n",
    "            except :\n",
    "                probe_index.append([left,right])\n",
    "            #Once you have found something you directly move past it\n",
    "            left = right-1\n",
    "        if end:\n",
    "            right+=1\n",
    "        left+=1\n",
    "    return probe_index\n",
    "\n",
    "\n",
    "def checkBBR(files,p=\"n\"):\n",
    "    classi = []\n",
    "    for f in files:\n",
    "        file_name = f.split(\"/\")[-1]\n",
    "        para = file_name.split(\"-\")\n",
    "        rtt = int(para[2])*2\n",
    "        bw = int(para[3])\n",
    "        bf = int(para[4])\n",
    "        bdp = float(bw*rtt*bf)/8\n",
    "        if bw == 1000 :\n",
    "            l=5\n",
    "            r=15\n",
    "        if bw == 200:\n",
    "            l=10\n",
    "            r=20\n",
    "        try:\n",
    "            time, data, retrans,rtt = plot_one_bt(f,p=\"n\",t=1)\n",
    "            probe_index = getProbes(time, data, rtt, bdp, bw)\n",
    "            if p==\"y\":\n",
    "                print_red(time, data, probe_index)\n",
    "            prev = 0\n",
    "            time_dis = []\n",
    "            for p in probe_index:\n",
    "                curr_ind = data.index(max(data[p[0]:p[1]+1]))\n",
    "                if curr_ind > p[1] or curr_ind < p[0]:\n",
    "                    print(\"Something Wrong\")\n",
    "                if prev != 0:\n",
    "                    time_dis.append(abs(time[curr_ind]-prev))\n",
    "                prev = time[curr_ind]\n",
    "#             print(time_dis)\n",
    "            count = 0\n",
    "            isBBR = 0\n",
    "            for t in time_dis : \n",
    "                if  t > l*rtt and t < r*rtt :\n",
    "                    count+=1\n",
    "                    if count >= 2: # there are three peaks consecutively\n",
    "                        isBBR=1\n",
    "                        break\n",
    "                else:\n",
    "                    count = 0\n",
    "            if isBBR:\n",
    "                classi.append(\"YES BBR\")\n",
    "            else:\n",
    "                if len(probe_index) <= 1 :\n",
    "                    classi.append(\"NO BBR\")\n",
    "                else:\n",
    "                    classi.append(\"MAYBE BBR\")\n",
    "        except Exception as ex:  \n",
    "            template = \"An exception of type {0} occurred. Arguments:\\n{1!r}\"\n",
    "            message = template.format(type(ex).__name__, ex.args)\n",
    "            new_message = \"NC \" + message\n",
    "            classi.append(new_message)\n",
    "    return classi\n",
    "\n",
    "def print_red(time,data,probe_index):\n",
    "    fig, ax = plt.subplots(1,1, figsize=(15,8))\n",
    "    offset=0\n",
    "    ax.plot(time[offset:],data[offset:])\n",
    "    # for t in retrans :\n",
    "    #     if t > time[offset]:\n",
    "    #         plt.axvline(x = t, color = 'm',alpha=0.5)\n",
    "    # ax.plot(new_time[offset:], new_data[offset:])\n",
    "    for p in probe_index:\n",
    "        ax.plot(time[p[0]:p[1]+1], data[p[0]:p[1]+1], color='r', lw=2)\n",
    "    plt.show()\n",
    "\n",
    "def getDivision(classi, test_files):\n",
    "    yes = []\n",
    "    no = []\n",
    "    maybe = []\n",
    "    nan = []\n",
    "    for i in range(len(classi)):\n",
    "        if classi[i] == \"YES BBR\":\n",
    "            yes.append(test_files[i])\n",
    "        elif classi[i] == \"NO BBR\":\n",
    "            no.append(test_files[i])\n",
    "        elif classi[i] == \"MAYBE BBR\":\n",
    "            maybe.append(test_files[i])\n",
    "        else:\n",
    "            nan.append(test_files[i])\n",
    "    return yes, no,maybe, nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c94219",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bisect\n",
    "def lower_bound(arr, target):\n",
    "    index = bisect.bisect_left(arr, target)\n",
    "    return index\n",
    "\n",
    "def sample_data_time(time, data, ss, m):\n",
    "    curr_time, curr_data = adjust(time, data)\n",
    "    tp = curr_time[len(curr_time)-1] - curr_time[0]\n",
    "    step = tp/m\n",
    "    samp_time = [curr_time[0] + i*step for i in range(m)]\n",
    "    x = np.random.uniform(0,math.pi,ss)\n",
    "    tr_x = np.cos(x)\n",
    "    tr_x += 1\n",
    "    tr_x *= (m-1)/2\n",
    "    ind = [int(round(e, 0)) for e in tr_x]\n",
    "    sort_ind = sorted(ind)\n",
    "    tr_time = [samp_time[i] for i in sort_ind]\n",
    "    new_time = []\n",
    "    new_data = []\n",
    "    for t in tr_time :\n",
    "        i = lower_bound(curr_time, t)\n",
    "        temp_t = 0\n",
    "        temp_d = 0\n",
    "        if round(t,6) == round(curr_time[i],6):\n",
    "            temp_t = curr_time[i]\n",
    "            temp_d = curr_data[i]\n",
    "        else : \n",
    "            if i == 0 :\n",
    "                temp_t = curr_time[i]\n",
    "                temp_d = curr_data[i]\n",
    "            elif i == len(curr_time)-1 :\n",
    "                temp_t = curr_time[i]\n",
    "                temp_d = curr_data[i]\n",
    "            else :\n",
    "                temp_t = (curr_time[i-1] + curr_time[i])/2\n",
    "                temp_d = (curr_data[i-1] + curr_data[i])/2\n",
    "        new_time.append(temp_t)\n",
    "        new_data.append(temp_d)\n",
    "    new_data.insert(0,curr_data[0])\n",
    "    new_data.append(curr_data[-1])\n",
    "    new_time.insert(0,curr_time[0])\n",
    "    new_time.append(curr_time[-1])\n",
    "#     return curr_time, curr_data\n",
    "    return new_time, new_data\n",
    "\n",
    "def adjust(time, data):\n",
    "    start = data.index(min(data[:int(len(data)/2)]))\n",
    "    end = data.index(max(data[int(len(data)/2):]))\n",
    "#     print(\"Difference in max and min \", end-start)\n",
    "    if end - start <= 0: \n",
    "        return time, data\n",
    "    new_time = time[start:end+1]\n",
    "    new_data = data[start:end+1]\n",
    "    return new_time, new_data\n",
    "\n",
    "# Taking 100 as the threshold is fine \n",
    "# Now we have to see how the graphs look if we use it. \n",
    "# var = [\"reno\", \"cubic\", \"bbr\"]\n",
    "def getRed_R(files,ss=125,p=\"y\", ft_thresh=100):\n",
    "    results = []\n",
    "    for curr_file in files : \n",
    "        print(curr_file)\n",
    "        file, folder_path = split_path(curr_file)\n",
    "        f_split = file.split(\"-\")\n",
    "        v = f_split[0]\n",
    "        rtt = float((int(f_split[pre_i]) + int(f_split[post_i]))*2)/1000\n",
    "        bdp = float(rtt*1000*int(f_split[bw_i])*int(f_split[bf_i]))/8\n",
    "        time, data, features = get_plot_features(curr_file, p=p)\n",
    "        count = 1\n",
    "        for ft in features : \n",
    "            if count > ft_thresh:\n",
    "                break\n",
    "            curr_time = time[ft[0]:ft[1]+1]\n",
    "            curr_data = data[ft[0]:ft[1]+1]\n",
    "            tr_time, tr_data = sample_data_time(curr_time, curr_data, ss, 1000)\n",
    "            tr_time_pd = pd.DataFrame(tr_time)\n",
    "            tr_data_pd = pd.DataFrame(tr_data)\n",
    "            tr_time = list(tr_time_pd.rolling(25, center=True).mean().dropna()[0])\n",
    "            tr_data = list(tr_data_pd.rolling(25, center=True).mean().dropna()[0])\n",
    "            print(\"Feature Length \", len(tr_data))\n",
    "            if p == \"y\" :\n",
    "                plt.plot(curr_time, curr_data, c='b', alpha = 0.5, lw = 5)\n",
    "                plt.plot(tr_time, tr_data, c='r', alpha = 1)\n",
    "                plt.scatter(tr_time, tr_data, c='k')\n",
    "#                 plt.scatter(tr_time, tr_data, c='r', s=10)\n",
    "                plt.title(v)\n",
    "                plt.show()\n",
    "            results.append(\n",
    "                {v+\"_\"+\"data\"+\"_\"+str(count):tr_data,\n",
    "                     v+\"_\"+\"time\"+\"_\"+str(count):tr_time,\n",
    "                        v+\"_\"+\"rtt\"+\"_\"+str(count):rtt, \n",
    "                             v+\"_\"+\"bdp\"+\"_\"+str(count):bdp})\n",
    "            count+=1\n",
    "    return results\n",
    "\n",
    "def getRed(files,ss=125,p=\"y\", ft_thresh=100):\n",
    "    results = []\n",
    "    for file in files :   \n",
    "        f_split = file.split(\"-\")\n",
    "        v = f_split[0] + \"-\" + f_split[1]\n",
    "        rtt = float((int(f_split[2]) + int(f_split[3]))*2)/1000\n",
    "        bdp = float(rtt*1000*int(f_split[4])*int(f_split[5]))/8\n",
    "        print(file)\n",
    "        print(\"RTT\",rtt,\"BDP\",bdp)\n",
    "        time, data, features = get_plot_features(file, p=p)\n",
    "        count = 1\n",
    "        for ft in features : \n",
    "            if count > ft_thresh:\n",
    "                break\n",
    "            curr_time = time[ft[0]:ft[1]+1]\n",
    "            curr_data = data[ft[0]:ft[1]+1]\n",
    "            tr_time, tr_data = sample_data_time(curr_time, curr_data, ss, 1000)\n",
    "            tr_time_pd = pd.DataFrame(tr_time)\n",
    "            tr_data_pd = pd.DataFrame(tr_data)\n",
    "            tr_time = list(tr_time_pd.rolling(25, center=True).mean().dropna()[0])\n",
    "            tr_data = list(tr_data_pd.rolling(25, center=True).mean().dropna()[0])\n",
    "            print(\"Feature Length \", len(tr_data))\n",
    "            if p == \"y\" :\n",
    "                plt.plot(curr_time, curr_data, c='b', alpha = 0.5, lw = 5)\n",
    "                plt.plot(tr_time, tr_data, c='r', alpha = 1)\n",
    "                plt.scatter(tr_time, tr_data, c='k')\n",
    "#                 plt.scatter(tr_time, tr_data, c='r', s=10)\n",
    "                plt.title(v)\n",
    "                plt.show()\n",
    "            results.append(\n",
    "                {v+\"_\"+\"data\"+\"_\"+str(count):tr_data,\n",
    "                     v+\"_\"+\"time\"+\"_\"+str(count):tr_time,\n",
    "                        v+\"_\"+\"rtt\"+\"_\"+str(count):rtt, \n",
    "                             v+\"_\"+\"bdp\"+\"_\"+str(count):bdp})\n",
    "            count+=1\n",
    "    return results\n",
    "# results = getRed(var)\n",
    "\n",
    "def get_degree_all(time,data, p=\"n\", max_deg=3):\n",
    "    p_net = []\n",
    "    mse_l = []\n",
    "    fit_net = []\n",
    "    for d in range(1,max_deg+1):\n",
    "        p_temp = np.polyfit(time,data,d)\n",
    "        p_net.append(p_temp)\n",
    "        fit_net.append(np.polyval(p_temp,time))\n",
    "        mse_l.append(mse(data,fit_net[-1]))\n",
    "    if p =='y':\n",
    "#         print(\"1 \", p1, \"MSE \", mse(data, fit_l))\n",
    "        plt.plot(time, data,c='k',label='Truth')\n",
    "#         plt.plot(time, fit_l)\n",
    "        for d in range(0, max_deg):\n",
    "            plot_label = \"degree\"+str(d+1)\n",
    "            plt.plot(time, fit_net[d],label=\"degree\" + str(d+1))\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    return max_deg,p_net, mse_l\n",
    "\n",
    "MAX_DEG=3\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "def get_degree(time,data, p=\"n\", max_deg=MAX_DEG):\n",
    "    p_net = []\n",
    "    mse_l = []\n",
    "    fit_net = []\n",
    "    for d in range(1,max_deg+1):\n",
    "        p_temp = np.polyfit(time,data, d)\n",
    "        p_net.append(p_temp[0:-1])\n",
    "        fit_net.append(np.polyval(p_temp,time))\n",
    "        mse_l.append(mse(data,fit_net[-1]))\n",
    "    if p =='y':\n",
    "#         print(\"1 \", p1, \"MSE \", mse(data, fit_l))\n",
    "        plt.plot(time, data,c='k',label='Truth')\n",
    "#         plt.plot(time, fit_l)\n",
    "        for d in range(max_deg-1, max_deg):\n",
    "            plot_label = \"degree\"+str(d)\n",
    "            plt.plot(time, fit_net[d],label=\"degree\" + str(d+1))\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    return max_deg,p_net[max_deg-1], mse_l\n",
    "\n",
    "def normalize(time, data, rtt, bdp):\n",
    "    new_time = time - min(time)\n",
    "    new_data = data - min(data)\n",
    "    \n",
    "    new_time = (new_time/rtt)\n",
    "#     new_time = (new_time/max(new_time))*10\n",
    "    new_data = (new_data/max(new_data))*10\n",
    "    \n",
    "    return new_time, new_data\n",
    "    \n",
    "from statistics import mean \n",
    "def get_feature_degree_R(files,ss=225,p='n',ft_thresh=3,max_deg=MAX_DEG):\n",
    "    results = getRed_R(files,ss,p=p,ft_thresh=ft_thresh)\n",
    "    count_features = 0\n",
    "    mp = {}\n",
    "    for item in results :\n",
    "        for ele in list(item.keys()):\n",
    "            name_list = ele.split(\"_\")\n",
    "            website = name_list[0]\n",
    "            if \"data\"==name_list[1] :\n",
    "                curr_data = np.array(item[ele])\n",
    "            if \"time\"==name_list[1] :\n",
    "                curr_time = np.array(item[ele])\n",
    "            if \"rtt\"==name_list[1] :\n",
    "                curr_rtt = item[ele]\n",
    "            if \"bdp\"==name_list[1] :\n",
    "                curr_bdp = item[ele]\n",
    "#         print(website)\n",
    "#         print(curr_data, curr_time)\n",
    "        curr_time, curr_data = normalize(curr_time, curr_data, curr_rtt, curr_bdp)\n",
    "        count_features += 1\n",
    "        max_deg,p_net,mse_l = get_degree_all(curr_time, curr_data,p=p,max_deg=max_deg)\n",
    "        mp[website] = {\n",
    "        'data':curr_data,\n",
    "        'time':curr_time,\n",
    "        \"max_deg\":max_deg,\n",
    "        \"p_net\":p_net,\n",
    "        \"mse_l\":mse_l\n",
    "    }\n",
    "    return mp\n",
    "\n",
    "from statistics import mean \n",
    "def get_feature_degree(files,ss=225,p='n',ft_thresh=3,max_deg=MAX_DEG):\n",
    "    results = getRed(files,ss,p=p,ft_thresh=ft_thresh)\n",
    "    errors = []\n",
    "    mp = {}\n",
    "    cc_mp = {}\n",
    "    count_features = 0\n",
    "    for item in results :\n",
    "        for ele in list(item.keys()):\n",
    "            name_list = ele.split(\"_\")\n",
    "            cc = name_list[0]\n",
    "            name = name_list[0] + name_list[-1]\n",
    "            if \"data\" in ele :\n",
    "                curr_data = np.array(item[ele])\n",
    "            if \"time\" in ele :\n",
    "                curr_time = np.array(item[ele])\n",
    "            if \"rtt\" in ele :\n",
    "                curr_rtt = item[ele]\n",
    "            if \"bdp\" in ele :\n",
    "                curr_bdp = item[ele]\n",
    "        curr_time, curr_data = normalize(curr_time, curr_data, curr_rtt, curr_bdp)\n",
    "        count_features += 1\n",
    "        print(\"Name :\",name)\n",
    "        degree, coeff, error_item = get_degree(curr_time, curr_data,p=p,max_deg=max_deg)\n",
    "        # Adding time feature here\n",
    "#         coeff_list = list(coeff)\n",
    "#         coeff_list.append(abs(curr_time[-1]-curr_time[0]))\n",
    "#         coeff = np.array(coeff_list)\n",
    "#         print(coeff)\n",
    "        #Adding new for scalalble and yeah\n",
    "        cc_curr = cc.split(\"-\")[0]\n",
    "        # Adding another check for scalable and yeah\n",
    "#         if cc_curr in ['scalable','yeah']:\n",
    "#             # Look at the fifth coefficient\n",
    "#             if round(coeff[0],6) < 0.000015:\n",
    "#                 print(coeff[0])\n",
    "#                 continue\n",
    "#         coeff.append(abs(curr_time[-1]-curr_time[0]))\n",
    "        mp[name] = {'d':degree, 'coeff':coeff, 'error':error_item, 'data':curr_data, 'time':curr_time}\n",
    "        if cc not in cc_mp :\n",
    "            cc_mp[cc] = []\n",
    "        cc_mp[cc].append(mp[name])\n",
    "    return cc_mp\n",
    "\n",
    "from scipy.stats import multivariate_normal as mvn\n",
    "def getPDensity(curr, cc_gaussian_params):\n",
    "    prob = {}\n",
    "    for cc in cc_gaussian_params:\n",
    "        mn = cc_gaussian_params[cc]['mean']\n",
    "        covar = cc_gaussian_params[cc]['covar']\n",
    "#         print(\"CC being checked against\",cc)\n",
    "#         print(np.linalg.det(covar))\n",
    "        curr_p = mvn.pdf(curr,mean=mn, cov=covar, allow_singular=True)\n",
    "        prob[cc]=curr_p\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8d869e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBestDegree(nmp):\n",
    "    results = {}\n",
    "    for name in nmp.keys():\n",
    "        print(name)\n",
    "    #     curr_time, curr_data, retrans, rtt = plot_one_bt(name+\"-0-50-200-2-aws-88-60\",'y',1)\n",
    "        data = nmp[name]['data']\n",
    "        time = nmp[name]['time']\n",
    "        max_deg = nmp[name]['max_deg']\n",
    "        p_net = nmp[name]['p_net']\n",
    "        mse_l = nmp[name]['mse_l']\n",
    "    #     plt.plot(time, data,c='k',label='Truth')\n",
    "    #         plt.plot(time, fit_l)\n",
    "    #     for d in range(0, max_deg):\n",
    "    #         plot_label = \"degree\"+str(d+1)\n",
    "    #         fit_net = np.polyval(p_net[d],time)\n",
    "    #         plt.plot(time, fit_net,label=\"degree\" + str(d+1))\n",
    "    #     plt.legend()\n",
    "    #     plt.show()\n",
    "\n",
    "        names = []\n",
    "        loss = []\n",
    "        lambd = 0.09\n",
    "        for i in range(len(mse_l)):\n",
    "            loss.append((i+1)*sum(p_net[i])*lambd)\n",
    "        print(\"loss\", loss)\n",
    "        print(\"mse\", mse_l)\n",
    "        for d in range(0, max_deg):\n",
    "            names.append(\"degree \"+str(d+1))\n",
    "        plt.bar(names,mse_l,color='blue',width=0.4,label=\"mse\")\n",
    "        plt.bar(names,loss,bottom=mse_l,color='maroon',width=0.4,label=\"reg_loss\")\n",
    "\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        errors = []\n",
    "        # The code for deciding the categories\n",
    "        for i in range(0,max_deg):\n",
    "            errors.append(loss[i]+mse_l[i])\n",
    "        deg = errors.index(min(errors))+1\n",
    "    #     if deg < 3:\n",
    "    #         deg = mse_l.index(min(mse_l[0:2]))+1\n",
    "    #     print(deg)\n",
    "        current_cc = name.split(\"-\")[0]\n",
    "        results[current_cc]={\n",
    "            'deg':deg,\n",
    "            'coeff':p_net[deg-1],\n",
    "            'error':errors[deg-1]\n",
    "        }\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d703b795",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal as mvn\n",
    "def getPDensityR(curr, cc_gaussian_params):\n",
    "    curr_coeff = curr[:-1]\n",
    "    prob = {}\n",
    "    for cc in cc_gaussian_params:\n",
    "        mn = cc_gaussian_params[cc]['mean']\n",
    "        covar = cc_gaussian_params[cc]['covar']\n",
    "        if len(curr_coeff) == len(mn):\n",
    "#             print(curr_coeff)\n",
    "#             print(mn)\n",
    "#             print(covar)\n",
    "            curr_p = mvn.pdf(curr_coeff,mean=mn, cov=covar, allow_singular=True)\n",
    "            prob[cc]=curr_p\n",
    "#         print(\"CC being checked against\",cc)\n",
    "#         print(np.linalg.det(covar))\n",
    "    return prob\n",
    "\n",
    "def getWebDensity(mp, cc_gaussian_params):\n",
    "    acc_w = {}\n",
    "    for web in mp:\n",
    "        p_dense = getPDensityR(mp[web]['coeff'], cc_gaussian_params)\n",
    "        if web not in acc_w:\n",
    "            acc_w[web]={}\n",
    "        ccs = np.array(list(p_dense.keys()))\n",
    "        vals = np.array(list(p_dense.values()))\n",
    "        \n",
    "        p_ind = list(np.argsort(vals))\n",
    "        p_ind.reverse()\n",
    "        acc_w[web]['ccs'] = [ccs[i] for i in p_ind]\n",
    "        acc_w[web]['density'] = np.array([vals[i] for i in p_ind])\n",
    "        maxx = max(acc_w[web]['density'])\n",
    "        minn = min(acc_w[web]['density'])\n",
    "        acc_w[web]['relative'] = (acc_w[web]['density']-minn)/(maxx-minn) \n",
    "    return acc_w\n",
    "\n",
    "def predictCC(acc_w):\n",
    "    pred = {}\n",
    "    for web in acc_w:\n",
    "        if acc_w[web]['density'][0] < 0.01:\n",
    "            pred[web]='new'\n",
    "        elif acc_w[web]['relative'][1] > 0.01:\n",
    "            pred[web]='confused'\n",
    "        else :\n",
    "            pred[web]=acc_w[web]['ccs'][0]\n",
    "    return pred, acc_w\n",
    "\n",
    "def getPredictions(cc_mp,cc_gp,no):\n",
    "    acc_w = getWebDensity(cc_mp, cc_gp)\n",
    "    pred, acc_w = predictCC(acc_w)\n",
    "    predictions = {}\n",
    "    no_feature = []\n",
    "    for f in no:\n",
    "        tp_mp = get_feature_degree_R([f],ss=225,p=\"y\",ft_thresh=1,max_deg=3)\n",
    "        if len(cc_mp.keys()) == 0:\n",
    "            continue\n",
    "        new_f = f.split(\"/\")[-1]\n",
    "        w = new_f.split(\"-\")[0]\n",
    "        if w not in acc_w.keys():\n",
    "            print(\"No Feature\")\n",
    "            no_feature.append(w)\n",
    "            predictions[w]=\"No feature\"\n",
    "        else:\n",
    "            print(cc_mp[new_f.split(\"-\")[0]]['coeff'])\n",
    "    #         time, data, retrans, rtt= plot_one_bt(f,\"y\",1)\n",
    "            predictions[w] = {}\n",
    "            print(\"Top 5 CCS :\", acc_w[w]['ccs'][0:5])\n",
    "            predictions[w]['ccs'] = acc_w[w]['ccs'][0:5]\n",
    "            \n",
    "            print(\"Density Values:\",acc_w[w]['density'][0:5])\n",
    "            predictions[w]['density'] = acc_w[w]['density'][0:5]\n",
    "            \n",
    "            print(\"Relative Density Values:\",acc_w[w]['relative'][0:5])\n",
    "            predictions[w]['relative'] = acc_w[w]['relative'] = acc_w[w]['relative'][0:5]\n",
    "            \n",
    "            print(\"Prediction:\",pred[w])\n",
    "            predictions[w]['final'] = pred[w]\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce40c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = '../../singapore/top1k/adobe.com-0-50-200-2-tcp.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82a9c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# file=sys.argv[1]\n",
    "classi = checkBBR([file],p=\"y\")\n",
    "if classi[0] == \"YES BBR\":\n",
    "    print(\"This is BBR\")\n",
    "elif classi[0] == \"MAYBE BBR\":\n",
    "    print(\"This is maybe BBR\")\n",
    "elif \"NC\" in classi[0]:\n",
    "    print(\"Not Classified with error\", classi[0])\n",
    "else:\n",
    "    print(\"Not BBR, continuing\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e9b3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('cc_gp.txt', 'rb') as f:\n",
    "    cc_gp = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab1e370",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp = get_feature_degree_R([file],ss=225,p=\"y\",ft_thresh=1,max_deg=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01dd70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_mp = getBestDegree(mp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbdf161",
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_gp['westwood']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2b0a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_1 = getPredictions(cc_mp,cc_gp,[file])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970aff44",
   "metadata": {},
   "source": [
    "# Running the same on all files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a08c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path = \"../../singapore/top1k/\"\n",
    "files = []\n",
    "for f in os.listdir(path):\n",
    "    if \"-udp.csv\" in f:\n",
    "        continue\n",
    "    if \"1000-2\" in f:\n",
    "        continue\n",
    "    files.append(path+f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6291baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "files[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00d7768",
   "metadata": {},
   "outputs": [],
   "source": [
    "classi = checkBBR(files)\n",
    "yes,no,maybe,nan = getDivision(classi,files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f8ceff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(yes),len(no),len(maybe),len(nan))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71c45d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at yes\n",
    "classi_y = checkBBR(yes,p=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f188a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at maybe\n",
    "classi_m = checkBBR(maybe,p=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61010e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "no[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc2fa7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp = get_feature_degree_R(no,ss=225,p=\"n\",ft_thresh=1,max_deg=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49d2784",
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_mp = getBestDegree(mp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3ae976",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = getPredictions(cc_mp,cc_gp,no)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
