{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbb5279d",
   "metadata": {},
   "source": [
    "# Code to run website tests\n",
    "### The model parameters are already trained using the Training Visualization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79f4abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_i=1\n",
    "post_i=2\n",
    "bw_i=3\n",
    "bf_i=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bcd677",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.fft import rfft, rfftfreq\n",
    "from scipy.fft import irfft\n",
    "import numpy as np\n",
    "import statistics\n",
    "from statistics import mean, pstdev\n",
    "import pandas as pd\n",
    "\n",
    "yellow = '\\033[93m'\n",
    "green = '\\033[92m'\n",
    "red = '\\033[91m'\n",
    "blue = '\\033[94m'\n",
    "pink = '\\033[95m'\n",
    "black = '\\033[90m'\n",
    "\n",
    "def smoothen(time, data, rtt):\n",
    "    # Smoothening \n",
    "    left = 0\n",
    "    right = 0\n",
    "    run_sum = 0\n",
    "    avg_data = []\n",
    "    new_time = []\n",
    "    roll_time = time\n",
    "    roll_data = data\n",
    "    while right < len(roll_time):\n",
    "        while(right < len(roll_time) and (roll_time[right]-roll_time[left] < 2*rtt)):\n",
    "            run_sum+=roll_data[right]\n",
    "            right+=1\n",
    "        new_time.append(float(roll_time[right-1]+roll_time[left])/2)\n",
    "        avg_data.append(float(run_sum)/(right-left))\n",
    "        run_sum-=roll_data[left]\n",
    "        left+=1\n",
    "    return new_time, avg_data\n",
    "\n",
    "\n",
    "def get_fft(data):\n",
    "    n = len(data)\n",
    "    data_step = 0.002\n",
    "    yf = rfft(data)\n",
    "    xf = rfftfreq(n,data_step)\n",
    "    return yf,xf\n",
    "\n",
    "def get_fft_smoothening(data, time, ax,rtt,p):\n",
    "    rtt=rtt\n",
    "    yf, xf = get_fft(data)\n",
    "    thresh  = (1/rtt)\n",
    "    thresh_ind = 0\n",
    "    for i in range(len(xf)) :\n",
    "        freq = xf[i]\n",
    "        if(freq > thresh):\n",
    "            thresh_ind = i\n",
    "            break\n",
    "            \n",
    "    yf_clean = yf\n",
    "    yf_clean[thresh_ind+1:] = 0\n",
    "    new_f_clean = irfft(yf_clean)\n",
    "    start_len = len(time) - len(new_f_clean)\n",
    "\n",
    "    plot_data = new_f_clean\n",
    "#     if p==\"y\":\n",
    "#         ax.plot(time[start_len:], plot_data, 'k', label='FFT smoothening', linewidth=1.5)\n",
    "\n",
    "    plot_time = time[start_len:] \n",
    "    return plot_time, plot_data\n",
    "\n",
    "def plot_d(ax, time, data, c, l, alpha=1):\n",
    "    ax.plot(time, data, color=c, lw=2, label = l,alpha=alpha)\n",
    "\n",
    "\n",
    "def plot_one_bt(f, p,t=1):\n",
    "#     print(f)\n",
    "    file_name = f.split(\"/\")[-1]\n",
    "    fs = file_name.split(\"-\")\n",
    "\n",
    "    pre = int(fs[1])\n",
    "    post = int(fs[2])\n",
    "    rtt = float(((pre+post)*2))/1000\n",
    "    ax = 0\n",
    "    if t==1:\n",
    "        data, time, retrans = get_window(f,\"n\",t)\n",
    "    elif t==2:\n",
    "        data, time, retrans, OOA, DA = get_window(f,\"n\",t)\n",
    "    if p == 'y':\n",
    "        fig, ax = plt.subplots(1,1, figsize=(15,8))\n",
    "        for t in retrans :\n",
    "            plt.axvline(x = t, color = 'm',alpha=0.5)\n",
    "        if t == 2:\n",
    "            for t in OOA :\n",
    "                plt.axvline(x = t, color = 'k', lw=2)\n",
    "            for t in DA:\n",
    "                plt.axvline(x = t, color = 'g', lw=0.5, alpha = 0.5)\n",
    "        plot_d(ax,time,data, \"r\",\"Original\")\n",
    "    time, data = get_fft_smoothening(data, time, ax,rtt,\"y\")\n",
    "\n",
    "    #         plot_d(ax,time,data,\"b\",\"FFT Smoothened\" )\n",
    "#         print(len(time), len(data))\n",
    "    time, data = smoothen(time, data, rtt)\n",
    "#         print(len(time), len(data))\n",
    "    if p == 'y':\n",
    "        plot_d(ax, time, data, \"b\", \"Smoothened\",alpha=0.5)\n",
    "        ax.legend()\n",
    "#             plt.savefig(\"./plots/\"+f+\".png\")\n",
    "        plt.show()\n",
    "#     return time, data, grad_time, grad_data, rtt\n",
    "#     print(\"Black : OOA, Green : DA, Magenta : RP\")\n",
    "    return time, data, retrans, rtt \n",
    "\n",
    "\n",
    "def get_time_features(retrans,time,rtt):\n",
    "    time_thresh = 20*rtt\n",
    "    features = []\n",
    "    for i in range(1, len(retrans)):\n",
    "        if retrans[i]-retrans[i-1] >= time_thresh:\n",
    "            features.append([retrans[i-1], retrans[i]])\n",
    "    # add a feature that finished when the experiment ends\n",
    "    if len(retrans)>0:\n",
    "        if time[-1] - retrans[-1] > 20*rtt :\n",
    "            features.append([retrans[-1],time[-1]])\n",
    "    return features\n",
    "\n",
    "def get_features(time, features):\n",
    "    left = 0\n",
    "    right = 0\n",
    "    feature_index = 0\n",
    "    in_feature = 0\n",
    "    index_features = []\n",
    "    while right < len(time) and feature_index < len(features): \n",
    "        if in_feature == 0 and time[right]>=features[feature_index][0]:\n",
    "            in_feature = 1\n",
    "            left = right\n",
    "        elif in_feature == 1 and time[right] > features[feature_index][1]:\n",
    "            in_feature = 0\n",
    "            index_features.append([left, right-1])\n",
    "            feature_index+=1\n",
    "        right+=1\n",
    "    if in_feature == 1:\n",
    "        index_features.append([left, right-1])\n",
    "    return index_features\n",
    "\n",
    "def get_plot_features(curr_file, p):\n",
    "    time, data, retrans, rtt = plot_one_bt(curr_file,p=p,t=1)\n",
    "    time_features = get_time_features(retrans,time,rtt)\n",
    "    features = get_features(time, time_features)    \n",
    "    if p == 'y':\n",
    "        fig, ax = plt.subplots(1,1, figsize=(15,8))\n",
    "        plot_d(ax, time, data, \"b\", \"Smoothened\")\n",
    "        for ft in features : \n",
    "#             print(time[ft[1]]-time[ft[0]])\n",
    "            ax.plot(time[ft[0]:ft[1]+1], data[ft[0]:ft[1]+1], color = 'r')\n",
    "        plt.show()\n",
    "    return time, data, features\n",
    "\n",
    "\n",
    "SHOW=True\n",
    "MULTI_GRAPH=False\n",
    "SMOOTHENING=False\n",
    "ONLY_STATS=False\n",
    "s_factor=0.9\n",
    "\n",
    "\n",
    "PKT_SIZE = 88\n",
    "\n",
    "\n",
    "'''\n",
    "TODO: \n",
    "o Add functionality where you only plot flows that send more than x bytes of data\n",
    "o Sort stats and graphs by flow size\n",
    "o Organize plots by flow size (larger flows have larger graphs)\n",
    "o Custom smoothening function\n",
    "'''\n",
    "\n",
    "fields=[\"time\", \"frame_time_rel\", \"tcp_time_rel\", \"frame_num\", \"frame_len\", \"ip_src\", \"src_port\", \"ip_dest\", \"dest_port\", \"tcp_len\", \"seq\", \"ack\"]\n",
    "\n",
    "class pkt:\n",
    "    contents=[]\n",
    "    def __init__(self, fields) -> None:\n",
    "        self.contents=[]\n",
    "        for f in fields:\n",
    "            self.contents.append(f)\n",
    "\n",
    "    def get(self, field):\n",
    "        return self.contents[fields.index(field)]\n",
    "        \n",
    "\n",
    "def process_flows(cc, dir,p=\"y\"):\n",
    "    name = dir+cc+\"-tcp.csv\"\n",
    "    with open(name) as csv_file:\n",
    "        print(\"Reading \"+name+\"...\")\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "        line_count = 0\n",
    "        total_bytes=0\n",
    "        '''\n",
    "        Flow tracking:\n",
    "        o Identify all packets that are either sourced from or headed to 100.64.0.2\n",
    "        o Group different flows by client's port\n",
    "        '''\n",
    "        flows={}\n",
    "        data_sent=0\n",
    "        # ACK and RTX measurement\n",
    "        ooa = set()\n",
    "        rp = set()\n",
    "        ooaCount=0\n",
    "        rpCount=0\n",
    "        daCount=0\n",
    "        backAck=0\n",
    "        for row in csv_reader:\n",
    "            reTx=0\n",
    "            \n",
    "            ooAck=0\n",
    "            dupAck=0\n",
    "            retPacket=0\n",
    "            \n",
    "            packet=pkt(row)\n",
    "            ackPkt=False\n",
    "            validPkt=False\n",
    "            if line_count==0:\n",
    "                # reject the header\n",
    "                line_count+=1\n",
    "                continue\n",
    "            if data_sent == 0 : \n",
    "                if \"100.64.0.\" in packet.get(\"ip_src\"):\n",
    "                    num = int(packet.get(\"ip_src\")[-1])\n",
    "                    if num%2==0:\n",
    "                        data_sent=1\n",
    "                        host_port=packet.get(\"ip_src\")\n",
    "                if \"100.64.0.\" in packet.get(\"ip_dest\"):\n",
    "                    num = int(packet.get(\"ip_dest\")[-1])\n",
    "                    if num%2==0:\n",
    "                        data_sent=1\n",
    "                        host_port=packet.get(\"ip_dest\")\n",
    "                if data_sent == 0:\n",
    "                    continue\n",
    "            if packet.get(\"ip_src\")==host_port and packet.get(\"frame_time_rel\")!='' and packet.get(\"ack\")!='': \n",
    "                # we care about this ACK packet\n",
    "                validPkt=True\n",
    "                ackPkt=True\n",
    "                port=packet.get(\"src_port\")\n",
    "                #PORTCHECK\n",
    "#                 if int(port) != 50468:\n",
    "#                     continue\n",
    "                if port not in flows:\n",
    "                    flows[port]={\"OOA\":[],\"DA\":[],\"max_seq\":0,\"loss_bif\":0,\"max_ack\":int(packet.get(\"ack\")),\"serverip\":packet.get(\"ip_dest\"), \"serverport\":packet.get(\"dest_port\"), \"act_times\":[],\"times\":[], \"windows\":[], \"cwnd\":[], \"bif\":0, \"last_ack\":0, \"last_seq\":0, \"pif\":0, \"drop\":[], \"next\":0, \"retrans\":[]}\n",
    "                else:\n",
    "                    # check for Out of Order Ack (OOA)\n",
    "                    if int(packet.get(\"ack\")) <= int(flows[port][\"max_ack\"]):\n",
    "                        if int(packet.get(\"ack\")) == backAck :\n",
    "                            dupAck = True\n",
    "                            flows[port][\"DA\"].append(float(packet.get(\"frame_time_rel\")))\n",
    "                        else :\n",
    "                            ooAck = True\n",
    "                            flows[port][\"OOA\"].append(float(packet.get(\"frame_time_rel\")))\n",
    "                        backAck = int(packet.get(\"ack\"))\n",
    "                    # update max_ack\n",
    "                    flows[port][\"max_ack\"] = max(flows[port][\"max_ack\"], int(packet.get(\"ack\")))\n",
    "                    if int(packet.get(\"seq\")) < flows[port][\"max_ack\"]:\n",
    "                        reTx += int(packet.get(\"tcp_len\"))\n",
    "#                     flows[port][\"times\"].append(float(packet.get(\"frame_time_rel\")) )\n",
    "                    \n",
    "            elif packet.get(\"ip_dest\")==host_port and packet.get(\"frame_time_rel\")!='' and packet.get(\"seq\")!='':\n",
    "                #we care about this Data packet\n",
    "                validPkt=True\n",
    "                port=packet.get(\"dest_port\")\n",
    "                #PORTCHECK\n",
    "#                 if int(port) != 50468:\n",
    "#                     continue\n",
    "                seq=int(packet.get(\"seq\"))\n",
    "                tcp_len=int(packet.get(\"tcp_len\"))\n",
    "                if port not in flows:\n",
    "                    flows[port]={\"OOA\":[],\"DA\":[],\"max_seq\":int(packet.get(\"seq\")),\"loss_bif\":0,\"max_ack\":0,\"serverip\":packet.get(\"ip_src\"), \"serverport\":packet.get(\"src_port\"),\"act_times\":[], \"times\":[], \"windows\":[], \"cwnd\":[], \"bif\":0, \"last_ack\":0, \"last_seq\":0, \"pif\":0, \"drop\":[], \"next\":0, \"retrans\":[]}\n",
    "                \n",
    "                else:\n",
    "                    flows[port][\"max_seq\"] = max(flows[port][\"max_seq\"], int(packet.get(\"seq\")))\n",
    "                \n",
    "                \n",
    "                if int(packet.get(\"seq\")) < flows[port][\"max_seq\"] :\n",
    "                    retPacket = True\n",
    "                    flows[port][\"retrans\"].append(flows[port][\"times\"][-1])\n",
    "                    \n",
    "            if validPkt==True:\n",
    "                bif = 0\n",
    "                normal_est_bif = int(flows[port][\"max_seq\"]) - int(flows[port][\"max_ack\"]) + PKT_SIZE#+ reTx\n",
    "                loss_est_bif = flows[port][\"loss_bif\"]\n",
    "                if ackPkt and dupAck and len(flows[port][\"windows\"]) > 10:\n",
    "                    if dupAck:\n",
    "                        # if we have received a duplicate ack then we need to reduce the bytes in flight by packet size\n",
    "                        # we also increase max ack to correct for the consolidated ack being sent later\n",
    "                        loss_est_bif = int(flows[port][\"windows\"][-1]) - PKT_SIZE\n",
    "                        flows[port][\"max_ack\"] += PKT_SIZE\n",
    "                        \n",
    "                        bif = min( normal_est_bif, loss_est_bif)\n",
    "                        if p == \"y\" :\n",
    "                            print(green+\"Duplicate Ack\",int(packet.get(\"ack\")),\"Max Ack\",flows[port][\"max_ack\"],\"BIF\",bif)\n",
    "                        \n",
    "#                     elif ooAck:\n",
    "#                         # first out of order ack that we have recieved not a duplicated ack\n",
    "#                         # the reason would be restransimitted packet so dont need to correct for this\n",
    "                        \n",
    "                elif ackPkt :\n",
    "                    loss_est_bif = normal_est_bif \n",
    "                    bif = normal_est_bif    \n",
    "                    if ooAck :\n",
    "                        ooaCount+=1\n",
    "                        if p == \"y\":\n",
    "                            print(red+\"Out of Order Ack\",int(packet.get(\"ack\")),\"Max Ack\",flows[port][\"max_ack\"],\"BIF\",normal_est_bif)\n",
    "                        ooa.add(int(packet.get(\"ack\")))\n",
    "                    else:\n",
    "                        if p == \"y\":\n",
    "                            print(black+\"Inorder Ack\",int(packet.get(\"ack\")),\"Max Seq\",flows[port][\"max_seq\"],\"BIF\",normal_est_bif)\n",
    "                else :\n",
    "                    bif = normal_est_bif\n",
    "                    if retPacket==True:\n",
    "                        rpCount+=1\n",
    "                        rp.add(int(packet.get(\"seq\")))\n",
    "                        if p == \"y\":\n",
    "                            print(pink+\"Retransmitted Packet\",int(packet.get(\"seq\")), \"Next\", flows[port][\"max_seq\"]+PKT_SIZE, \"BIF\",bif)\n",
    "                    else :\n",
    "                        if p == \"y\":\n",
    "                            print(blue+\"Inorder Packet\", int(packet.get(\"seq\")), \"Next\", flows[port][\"max_seq\"]+PKT_SIZE, \"BIF\",bif)\n",
    "                flows[port][\"loss_bif\"] = loss_est_bif\n",
    "                flows[port][\"windows\"].append( int(bif) )\n",
    "                flows[port][\"times\"].append( float(packet.get(\"frame_time_rel\")) )\n",
    "                \n",
    "#                 if ackPkt and dupAck and len(flows[port][\"windows\"]) > 10: # we have received atleast the first window\n",
    "# #                     if len(flows[port][\"windows\"]) < 2000: # print reTx in first 200 packets\n",
    "# #                         print( packet.get(\"ack\"), flows[port][\"max_ack\"])\n",
    "#                     loss_est_bif = int(flows[port][\"windows\"][-1]) - PKT_SIZE\n",
    "#                     flows[port][\"max_ack\"] += PKT_SIZE\n",
    "#                     bif = min( normal_est_bif, loss_est_bif )\n",
    "#                 elif ackPkt:\n",
    "#                     loss_est_bif = normal_est_bif \n",
    "#                     bif = normal_est_bif\n",
    "#                 else:\n",
    "#                     bif = normal_est_bif\n",
    "#                 flows[port][\"loss_bif\"] = loss_est_bif\n",
    "#                 flows[port][\"windows\"].append( int(bif) )\n",
    "            \n",
    "            \n",
    "            line_count+=1\n",
    "            total_bytes+=int(packet.get(\"frame_len\"))\n",
    "            #print(line_count, total_bytes)\n",
    "            \n",
    "#         print(\"total bytes processed:\", total_bytes/1000, \"KBytes for\", cc, \"(unlimited)\")\n",
    "        if p == \"y\":\n",
    "            print(\"Out of Order Acks\",len(ooa),\"Retransmitted Packets\",len(rp))\n",
    "            print(\"Count Out of Order Acks\",ooaCount,\"Retransmitted Packets\",rpCount)\n",
    "            print(\"OOA\",ooa,\"RP\",rp)\n",
    "    return flows\n",
    "\n",
    "def custom_smooth_function():\n",
    "    pass\n",
    "\n",
    "def get_flow_stats(flows):\n",
    "    num=len(flows.keys())\n",
    "    print(\"FLOW STATISTICS: \\nNumber of flows: \", num)\n",
    "    print(\"------------------------------------------------------------------------------\")\n",
    "    print('%6s'%\"port\", '%15s'%\"SrcIP\", '%8s'%\"SrcPort\",  '%8s'%\"duration\",  '%8s'%\"start\",  '%8s'%\"end\", '%8s'%\"Sent (B)\", '%8s'%\"Recv (B)\",)\n",
    "    for k in flows.keys():\n",
    "        print('%6s'%k, '%15s'%flows[k][\"serverip\"], '%8s'%flows[k][\"serverport\"], '%8s'%str('%.2f'%(flows[k][\"times\"][-1]-flows[k][\"times\"][0])), '%8s'%str('%.2f'%flows[k][\"times\"][0]), '%8s'%str('%.2f'%flows[k][\"times\"][-1]), '%8s'%flows[k][\"last_seq\"], '%8s'%flows[k][\"last_ack\"])\n",
    "        #print(\"    * Flow \"+str(k)+\": \", flows[k][\"last_ack\"], \" \", flows[k][\"last_seq\"], \" bytes transfered.\")\n",
    "    return num\n",
    "\n",
    "def run(files):\n",
    "    flows = {}\n",
    "    for f in files:\n",
    "        algo_cc=f\n",
    "        #Get the data for all the flows\n",
    "        print(\"==============================================================================\")\n",
    "        print(\"opening trace ../measurements/\"+algo_cc+\".csv...\")\n",
    "        flows = process_flows(algo_cc, \"./Nebby/measurements-new-btl/50-200-2-60/\")\n",
    "        #decide on final graph layout\n",
    "        num = get_flow_stats(flows)\n",
    "\n",
    "        if ONLY_STATS:\n",
    "            sys.exit()\n",
    "\n",
    "        if num==1:\n",
    "            MULTI_GRAPH=False\n",
    "        #grid size\n",
    "        if MULTI_GRAPH:\n",
    "            size=(0,0)\n",
    "            grids={1:(2,2), 2:(2,2), 4:(2,2), 6:(2,3), 9:(3,3), 12:(3,4), 15:(3,5), 16:(4,4), 20:(5,4), 24:(6,4), 30:(6,5), 36:(6,6), 40:(8,5), 42:(8,7), 49:(7,7)}\n",
    "            g=num\n",
    "            while g<=49 and g not in grids:\n",
    "                g+=1\n",
    "            if g in grids.keys():\n",
    "                size=grids[g]\n",
    "            else:\n",
    "                size=grids[49]  \n",
    "            fig, axs = plt.subplots(size[0], size[1])\n",
    "            for i in range(size[0]):\n",
    "                for j in range(size[1]):\n",
    "                    #axs[i][j].legend(loc=\"lower right\")\n",
    "                    if i==size[0]-1:\n",
    "                        axs[i][j].set_xlabel(\"Time (s)\")\n",
    "                    if j==0:\n",
    "                        axs[i][j].set_ylabel(\"Bytes in flight\")\n",
    "        else:\n",
    "            plt.xlabel(\"Time (s)\")\n",
    "            plt.ylabel(\"Bytes in flight\")\n",
    "        counter=0\n",
    "        for port in flows.keys():\n",
    "            if MULTI_GRAPH:  \n",
    "                axs[counter%size[0]][(counter//size[0])%size[1]].scatter(flows[port][\"times\"], flows[port][\"windows\"], color=\"#858585\")\n",
    "                axs[counter%size[0]][(counter//size[0])%size[1]].plot(flows[port][\"times\"], flows[port][\"windows\"], label=str(port), linestyle=\"solid\")\n",
    "            else:\n",
    "                plt.plot(flows[port][\"times\"], flows[port][\"windows\"], label=str(port), linestyle=\"solid\")\n",
    "                plt.scatter(flows[port][\"times\"], flows[port][\"windows\"], color=\"#858585\")\n",
    "            counter+=1\n",
    "        if MULTI_GRAPH:\n",
    "            counter=0\n",
    "            for port in flows.keys():\n",
    "                axs[counter%size[0]][(counter//size[0])%size[1]].legend()\n",
    "                counter+=1\n",
    "        else:\n",
    "            plt.legend()\n",
    "        if MULTI_GRAPH:\n",
    "            fig.set_size_inches(16, 12)\n",
    "        if SHOW:\n",
    "            plt.show()\n",
    "        else:\n",
    "            plt.savefig(\"../logs/results/\"+algo_cc+\".png\", dpi=600, bbox_inches='tight', pad_inches=0)\n",
    "    return flows\n",
    "\n",
    "\n",
    "def split_path(f):\n",
    "    path = f.split(\"/\")\n",
    "    file_name = path[-1][:-8]\n",
    "    folder_path = \"/\".join(path[:-1])\n",
    "    folder_path = folder_path + \"/\"\n",
    "    algo_cc = file_name\n",
    "    return algo_cc, folder_path\n",
    "\n",
    "def get_window(f,p,t=1):\n",
    "    algo_cc, folder_path = split_path(f)\n",
    "    flows = process_flows(algo_cc, folder_path,p=p)\n",
    "#     flows = process_flows(algo_cc, \"../measurements/m/\",p=p)\n",
    "#     flows = process_flows(algo_cc, \"./Nebby/measurements/\",p=p)    \n",
    "#     flows = process_flows(algo_cc, \"./Nebby/measurements-new-btl/50-200-2-60/\",p=p)\n",
    "#     flows = process_flows(algo_cc, \"./Nebby/measurements-100-150/\",p=p)\n",
    "    params = algo_cc.split(\"-\")\n",
    "    data = []\n",
    "    time = []\n",
    "    drops = []\n",
    "    retrans = []\n",
    "    OOA = []\n",
    "    DA = []\n",
    "    use_port = 0\n",
    "    maxx = 0\n",
    "    print(\"All Ports : \", flows.keys())\n",
    "    for port in flows.keys():\n",
    "        if len(flows[port]['windows']) > maxx:\n",
    "            maxx = len(flows[port]['windows'])\n",
    "            use_port = port\n",
    "    print(\"Port\",use_port)\n",
    "    data = flows[use_port]['windows']\n",
    "    time = flows[use_port]['times']\n",
    "    retrans = flows[use_port]['retrans']\n",
    "    OOA = flows[use_port]['OOA']\n",
    "    DA = flows[use_port]['DA']\n",
    "    if p == \"y\":\n",
    "        plt.plot(time, data)\n",
    "#     time_index = len(time)-1\n",
    "#     for index in range(len(flows[use_port]['times'])-1):\n",
    "#         if flows[use_port]['times'][index+1] - flows[use_port]['times'][index] > :\n",
    "#             time_index = index\n",
    "#     time_last = flows[use_port]['times'][time_index]\n",
    "#     data = data[:time_index+1]\n",
    "#     time = time[:time_index+1]\n",
    "    if t==2:\n",
    "        return data, time, retrans, OOA, DA\n",
    "    if t==1:\n",
    "        return data, time, retrans\n",
    "\n",
    "\n",
    "def getProbes(time, data, rtt, bdp, bw=200):\n",
    "    if bw==200:\n",
    "        thresh = 8*rtt\n",
    "        st_thresh = 0.025\n",
    "        error = 0.08\n",
    "        alpha = 1.10\n",
    "    if bw==1000:\n",
    "        thresh = 4*rtt\n",
    "        st_thresh = 0.025\n",
    "        error = 0.02\n",
    "        alpha = 1\n",
    "    probe_index = []\n",
    "    left = 0\n",
    "    right = 0\n",
    "    bdp_thresh = bdp/2\n",
    "    prev_right = 0\n",
    "    end = 0\n",
    "    while right < len(data):\n",
    "        while right < len(data) and (time[right]-time[left]) < thresh:\n",
    "            right+=1\n",
    "        if right == len(data):\n",
    "            end = 1\n",
    "            right-=1\n",
    "        mid = math.floor(left + (right - left)/2)\n",
    "        mid_val = 0\n",
    "        left_mid = mid\n",
    "        right_mid = mid\n",
    "#         print(left,right)\n",
    "#         while left_mid > 0 and abs(time[mid]-time[left_mid])<rtt:\n",
    "#             if data[left_mid] > data[left] and data[left_mid] > data[right]:\n",
    "#                 mid_val+=1\n",
    "#             left_mid-=1\n",
    "#         while right_mid < len(data)-1 and abs(time[right_mid]-time[mid])<rtt:\n",
    "#             if data[right_mid] > data[left] and data[right_mid] > data[right]:\n",
    "#                 mid_val+=1\n",
    "#             right_mid+=1\n",
    "#         total = right_mid - left_mid\n",
    "#         peak = 0\n",
    "#         if round(float(mid_val)/total,2) > 0.99 :\n",
    "#             print(round(float(mid_val)/total,2))\n",
    "#             peak=1\n",
    "        go = 0\n",
    "        if data[mid] > data[left] and data[mid] > data[right]:\n",
    "#         if peak == 1:\n",
    "            #this  is  peak\n",
    "#             go = 1\n",
    "            t_l = left\n",
    "            t_r = right\n",
    "            while(t_l > 0 and time[left]-time[t_l] < thresh/2):\n",
    "                t_l-=1\n",
    "            while(t_r < len(data)-1 and time[t_r]-time[right] < thresh/2):\n",
    "                t_r+=1\n",
    "            left_sd = round(np.std(data[t_l:left])/(bdp_thresh*2),3)\n",
    "            right_sd = round(np.std(data[right:t_r])/(bdp_thresh*2),3)\n",
    "            if float(abs(data[left]-data[right]))/data[left] < error:\n",
    "                # this has the left and right points not too different from each other\n",
    "                side_avg = float((data[left]+data[right]))/2\n",
    "                local_max = max(data[left:right+1])\n",
    "#                 go = 1\n",
    "#                 print(\"Yes\")\n",
    "                if float(local_max)/side_avg > alpha and local_max > bdp_thresh: \n",
    "                    # this means that the peak is quite steep\n",
    "#                     print(\"This\")\n",
    "#                     go = 1\n",
    "                    if (left_sd < st_thresh) and (right_sd < st_thresh):\n",
    "                        # this means that the lest and right are quite stable respectvely\n",
    "                        go = 1\n",
    "#                         print(\"Out\", left_sd, right_sd, st_thresh)\n",
    "        if go == 1: \n",
    "            try :\n",
    "                probe_index.append([left,right,float(local_max)/side_avg, time[right]-time[left], left_sd, right_sd, t_l,t_r])\n",
    "            except :\n",
    "                probe_index.append([left,right])\n",
    "            #Once you have found something you directly move past it\n",
    "            left = right-1\n",
    "        if end:\n",
    "            right+=1\n",
    "        left+=1\n",
    "    return probe_index\n",
    "\n",
    "\n",
    "def checkBBR(files,p=\"n\"):\n",
    "    classi = []\n",
    "    for f in files:\n",
    "        file_name = f.split(\"/\")[-1]\n",
    "        para = file_name.split(\"-\")\n",
    "        rtt = int(para[2])*2\n",
    "        bw = int(para[3])\n",
    "        bf = int(para[4])\n",
    "        bdp = float(bw*rtt*bf)/8\n",
    "        if bw == 1000 :\n",
    "            l=5\n",
    "            r=15\n",
    "        if bw == 200:\n",
    "            l=10\n",
    "            r=20\n",
    "        try:\n",
    "            time, data, retrans,rtt = plot_one_bt(f,p=\"n\",t=1)\n",
    "            probe_index = getProbes(time, data, rtt, bdp, bw)\n",
    "            if p==\"y\":\n",
    "                print_red(time, data, probe_index)\n",
    "            prev = 0\n",
    "            time_dis = []\n",
    "            for p in probe_index:\n",
    "                curr_ind = data.index(max(data[p[0]:p[1]+1]))\n",
    "                if curr_ind > p[1] or curr_ind < p[0]:\n",
    "                    print(\"Something Wrong\")\n",
    "                if prev != 0:\n",
    "                    time_dis.append(abs(time[curr_ind]-prev))\n",
    "                prev = time[curr_ind]\n",
    "#             print(time_dis)\n",
    "            count = 0\n",
    "            isBBR = 0\n",
    "            for t in time_dis : \n",
    "                if  t > l*rtt and t < r*rtt :\n",
    "                    count+=1\n",
    "                    if count >= 2: # there are three peaks consecutively\n",
    "                        isBBR=1\n",
    "                        break\n",
    "                else:\n",
    "                    count = 0\n",
    "            if isBBR:\n",
    "                classi.append(\"YES BBR\")\n",
    "            else:\n",
    "                if len(probe_index) <= 1 :\n",
    "                    classi.append(\"NO BBR\")\n",
    "                else:\n",
    "                    classi.append(\"MAYBE BBR\")\n",
    "        except Exception as ex:  \n",
    "            template = \"An exception of type {0} occurred. Arguments:\\n{1!r}\"\n",
    "            message = template.format(type(ex).__name__, ex.args)\n",
    "            new_message = \"NC \" + message\n",
    "            classi.append(new_message)\n",
    "    return classi\n",
    "\n",
    "def print_red(time,data,probe_index):\n",
    "    fig, ax = plt.subplots(1,1, figsize=(15,8))\n",
    "    offset=0\n",
    "    ax.plot(time[offset:],data[offset:])\n",
    "    # for t in retrans :\n",
    "    #     if t > time[offset]:\n",
    "    #         plt.axvline(x = t, color = 'm',alpha=0.5)\n",
    "    # ax.plot(new_time[offset:], new_data[offset:])\n",
    "    for p in probe_index:\n",
    "        ax.plot(time[p[0]:p[1]+1], data[p[0]:p[1]+1], color='r', lw=2)\n",
    "    plt.show()\n",
    "\n",
    "def getDivision(classi, test_files):\n",
    "    yes = []\n",
    "    no = []\n",
    "    maybe = []\n",
    "    nan = []\n",
    "    for i in range(len(classi)):\n",
    "        if classi[i] == \"YES BBR\":\n",
    "            yes.append(test_files[i])\n",
    "        elif classi[i] == \"NO BBR\":\n",
    "            no.append(test_files[i])\n",
    "        elif classi[i] == \"MAYBE BBR\":\n",
    "            maybe.append(test_files[i])\n",
    "        else:\n",
    "            nan.append(test_files[i])\n",
    "    return yes, no,maybe, nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c94219",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bisect\n",
    "def lower_bound(arr, target):\n",
    "    index = bisect.bisect_left(arr, target)\n",
    "    return index\n",
    "\n",
    "def sample_data_time(time, data, ss, m):\n",
    "    curr_time, curr_data = adjust(time, data)\n",
    "    tp = curr_time[len(curr_time)-1] - curr_time[0]\n",
    "    step = tp/m\n",
    "    samp_time = [curr_time[0] + i*step for i in range(m)]\n",
    "    x = np.random.uniform(0,math.pi,ss)\n",
    "    tr_x = np.cos(x)\n",
    "    tr_x += 1\n",
    "    tr_x *= (m-1)/2\n",
    "    ind = [int(round(e, 0)) for e in tr_x]\n",
    "    sort_ind = sorted(ind)\n",
    "    tr_time = [samp_time[i] for i in sort_ind]\n",
    "    new_time = []\n",
    "    new_data = []\n",
    "    for t in tr_time :\n",
    "        i = lower_bound(curr_time, t)\n",
    "        temp_t = 0\n",
    "        temp_d = 0\n",
    "        if round(t,6) == round(curr_time[i],6):\n",
    "            temp_t = curr_time[i]\n",
    "            temp_d = curr_data[i]\n",
    "        else : \n",
    "            if i == 0 :\n",
    "                temp_t = curr_time[i]\n",
    "                temp_d = curr_data[i]\n",
    "            elif i == len(curr_time)-1 :\n",
    "                temp_t = curr_time[i]\n",
    "                temp_d = curr_data[i]\n",
    "            else :\n",
    "                temp_t = (curr_time[i-1] + curr_time[i])/2\n",
    "                temp_d = (curr_data[i-1] + curr_data[i])/2\n",
    "        new_time.append(temp_t)\n",
    "        new_data.append(temp_d)\n",
    "    new_data.insert(0,curr_data[0])\n",
    "    new_data.append(curr_data[-1])\n",
    "    new_time.insert(0,curr_time[0])\n",
    "    new_time.append(curr_time[-1])\n",
    "#     return curr_time, curr_data\n",
    "    return new_time, new_data\n",
    "\n",
    "def adjust(time, data):\n",
    "    try :\n",
    "        start = data.index(min(data[:int(len(data)/2)]))\n",
    "        end = data.index(max(data[int(len(data)/2):]))\n",
    "    except :\n",
    "        start = 0 \n",
    "        end = len(data) - 1\n",
    "    print(\"Difference in max and min \", end-start)\n",
    "    if end - start <= 0: \n",
    "        return time, data\n",
    "    new_time = time[start:end+1]\n",
    "    new_data = data[start:end+1]\n",
    "    return new_time, new_data\n",
    "\n",
    "# Taking 100 as the threshold is fine \n",
    "# Now we have to see how the graphs look if we use it. \n",
    "# var = [\"reno\", \"cubic\", \"bbr\"]\n",
    "def getRed_R(files,ss=125,p=\"y\", ft_thresh=100):\n",
    "    results = []\n",
    "    for curr_file in files : \n",
    "        print(curr_file)\n",
    "        file, folder_path = split_path(curr_file)\n",
    "        f_split = file.split(\"-\")\n",
    "        v = f_split[0]\n",
    "        rtt = float((int(f_split[pre_i]) + int(f_split[post_i]))*2)/1000\n",
    "        bdp = float(rtt*1000*int(f_split[bw_i])*int(f_split[bf_i]))/8\n",
    "        time, data, features = get_plot_features(curr_file, p=p)\n",
    "        count = 1\n",
    "        for ft in features : \n",
    "            if count > ft_thresh:\n",
    "                break\n",
    "            curr_time = time[ft[0]:ft[1]+1]\n",
    "            curr_data = data[ft[0]:ft[1]+1]\n",
    "            tr_time, tr_data = sample_data_time(curr_time, curr_data, ss, 1000)\n",
    "            tr_time_pd = pd.DataFrame(tr_time)\n",
    "            tr_data_pd = pd.DataFrame(tr_data)\n",
    "            tr_time = list(tr_time_pd.rolling(25, center=True).mean().dropna()[0])\n",
    "            tr_data = list(tr_data_pd.rolling(25, center=True).mean().dropna()[0])\n",
    "            print(\"Feature Length \", len(tr_data))\n",
    "            if p == \"y\" :\n",
    "                plt.plot(curr_time, curr_data, c='b', alpha = 0.5, lw = 5)\n",
    "                plt.plot(tr_time, tr_data, c='r', alpha = 1)\n",
    "                plt.scatter(tr_time, tr_data, c='k')\n",
    "#                 plt.scatter(tr_time, tr_data, c='r', s=10)\n",
    "                plt.title(v)\n",
    "                plt.show()\n",
    "            results.append(\n",
    "                {v+\"_\"+\"data\"+\"_\"+str(count):tr_data,\n",
    "                     v+\"_\"+\"time\"+\"_\"+str(count):tr_time,\n",
    "                        v+\"_\"+\"rtt\"+\"_\"+str(count):rtt, \n",
    "                             v+\"_\"+\"bdp\"+\"_\"+str(count):bdp})\n",
    "            count+=1\n",
    "    return results\n",
    "\n",
    "def getRed(files,ss=125,p=\"y\", ft_thresh=100):\n",
    "    results = []\n",
    "    for file in files :   \n",
    "        f_split = file.split(\"-\")\n",
    "        v = f_split[0] + \"-\" + f_split[1]\n",
    "        rtt = float((int(f_split[2]) + int(f_split[3]))*2)/1000\n",
    "        bdp = float(rtt*1000*int(f_split[4])*int(f_split[5]))/8\n",
    "        print(file)\n",
    "        print(\"RTT\",rtt,\"BDP\",bdp)\n",
    "        time, data, features = get_plot_features(file, p=p)\n",
    "        count = 1\n",
    "        for ft in features : \n",
    "            if count > ft_thresh:\n",
    "                break\n",
    "            curr_time = time[ft[0]:ft[1]+1]\n",
    "            curr_data = data[ft[0]:ft[1]+1]\n",
    "            tr_time, tr_data = sample_data_time(curr_time, curr_data, ss, 1000)\n",
    "            tr_time_pd = pd.DataFrame(tr_time)\n",
    "            tr_data_pd = pd.DataFrame(tr_data)\n",
    "            tr_time = list(tr_time_pd.rolling(25, center=True).mean().dropna()[0])\n",
    "            tr_data = list(tr_data_pd.rolling(25, center=True).mean().dropna()[0])\n",
    "            print(\"Feature Length \", len(tr_data))\n",
    "            if p == \"y\" :\n",
    "                plt.plot(curr_time, curr_data, c='b', alpha = 0.5, lw = 5)\n",
    "                plt.plot(tr_time, tr_data, c='r', alpha = 1)\n",
    "                plt.scatter(tr_time, tr_data, c='k')\n",
    "#                 plt.scatter(tr_time, tr_data, c='r', s=10)\n",
    "                plt.title(v)\n",
    "                plt.show()\n",
    "            results.append(\n",
    "                {v+\"_\"+\"data\"+\"_\"+str(count):tr_data,\n",
    "                     v+\"_\"+\"time\"+\"_\"+str(count):tr_time,\n",
    "                        v+\"_\"+\"rtt\"+\"_\"+str(count):rtt, \n",
    "                             v+\"_\"+\"bdp\"+\"_\"+str(count):bdp})\n",
    "            count+=1\n",
    "    return results\n",
    "# results = getRed(var)\n",
    "\n",
    "def get_degree_all(time,data, p=\"n\", max_deg=3):\n",
    "    p_net = []\n",
    "    mse_l = []\n",
    "    fit_net = []\n",
    "    for d in range(1,max_deg+1):\n",
    "        p_temp = np.polyfit(time,data,d)\n",
    "        p_net.append(p_temp)\n",
    "        fit_net.append(np.polyval(p_temp,time))\n",
    "        mse_l.append(mse(data,fit_net[-1]))\n",
    "    if p =='y':\n",
    "#         print(\"1 \", p1, \"MSE \", mse(data, fit_l))\n",
    "        plt.plot(time, data,c='k',label='Truth')\n",
    "#         plt.plot(time, fit_l)\n",
    "        for d in range(0, max_deg):\n",
    "            plot_label = \"degree\"+str(d+1)\n",
    "            plt.plot(time, fit_net[d],label=\"degree\" + str(d+1))\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    return max_deg,p_net, mse_l\n",
    "\n",
    "MAX_DEG=3\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "def get_degree(time,data, p=\"n\", max_deg=MAX_DEG):\n",
    "    p_net = []\n",
    "    mse_l = []\n",
    "    fit_net = []\n",
    "    for d in range(1,max_deg+1):\n",
    "        p_temp = np.polyfit(time,data, d)\n",
    "        p_net.append(p_temp[0:-1])\n",
    "        fit_net.append(np.polyval(p_temp,time))\n",
    "        mse_l.append(mse(data,fit_net[-1]))\n",
    "    if p =='y':\n",
    "#         print(\"1 \", p1, \"MSE \", mse(data, fit_l))\n",
    "        plt.plot(time, data,c='k',label='Truth')\n",
    "#         plt.plot(time, fit_l)\n",
    "        for d in range(max_deg-1, max_deg):\n",
    "            plot_label = \"degree\"+str(d)\n",
    "            plt.plot(time, fit_net[d],label=\"degree\" + str(d+1))\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    return max_deg,p_net[max_deg-1], mse_l\n",
    "\n",
    "def normalize(time, data, rtt, bdp):\n",
    "    new_time = time - min(time)\n",
    "    new_data = data - min(data)\n",
    "    new_time = (new_time/max(new_time))*10\n",
    "    new_data = (new_data/max(new_data))*10    \n",
    "    return new_time, new_data\n",
    "    \n",
    "from statistics import mean \n",
    "def get_feature_degree_R(files,ss=225,p='n',ft_thresh=3,max_deg=MAX_DEG):\n",
    "    results = getRed_R(files,ss,p=p,ft_thresh=ft_thresh)\n",
    "    count_features = 0\n",
    "    mp = {}\n",
    "    for item in results :\n",
    "        for ele in list(item.keys()):\n",
    "            name_list = ele.split(\"_\")\n",
    "            website = name_list[0]\n",
    "            if \"data\"==name_list[1] :\n",
    "                curr_data = np.array(item[ele])\n",
    "            if \"time\"==name_list[1] :\n",
    "                curr_time = np.array(item[ele])\n",
    "            if \"rtt\"==name_list[1] :\n",
    "                curr_rtt = item[ele]\n",
    "            if \"bdp\"==name_list[1] :\n",
    "                curr_bdp = item[ele]\n",
    "#         print(website)\n",
    "#         print(curr_data, curr_time)\n",
    "        curr_time, curr_data = normalize(curr_time, curr_data, curr_rtt, curr_bdp)\n",
    "        count_features += 1\n",
    "        max_deg,p_net,mse_l = get_degree_all(curr_time, curr_data,p=p,max_deg=max_deg)\n",
    "        mp[website] = {\n",
    "        'data':curr_data,\n",
    "        'time':curr_time,\n",
    "        \"max_deg\":max_deg,\n",
    "        \"p_net\":p_net,\n",
    "        \"mse_l\":mse_l\n",
    "    }\n",
    "    return mp\n",
    "\n",
    "from statistics import mean \n",
    "def get_feature_degree(files,ss=225,p='n',ft_thresh=3,max_deg=MAX_DEG):\n",
    "    results = getRed(files,ss,p=p,ft_thresh=ft_thresh)\n",
    "    errors = []\n",
    "    mp = {}\n",
    "    cc_mp = {}\n",
    "    count_features = 0\n",
    "    for item in results :\n",
    "        for ele in list(item.keys()):\n",
    "            name_list = ele.split(\"_\")\n",
    "            cc = name_list[0]\n",
    "            name = name_list[0] + name_list[-1]\n",
    "            if \"data\" in ele :\n",
    "                curr_data = np.array(item[ele])\n",
    "            if \"time\" in ele :\n",
    "                curr_time = np.array(item[ele])\n",
    "            if \"rtt\" in ele :\n",
    "                curr_rtt = item[ele]\n",
    "            if \"bdp\" in ele :\n",
    "                curr_bdp = item[ele]\n",
    "        curr_time, curr_data = normalize(curr_time, curr_data, curr_rtt, curr_bdp)\n",
    "        count_features += 1\n",
    "        print(\"Name :\",name)\n",
    "        degree, coeff, error_item = get_degree(curr_time, curr_data,p=p,max_deg=max_deg)\n",
    "        # Adding time feature here\n",
    "#         coeff_list = list(coeff)\n",
    "#         coeff_list.append(abs(curr_time[-1]-curr_time[0]))\n",
    "#         coeff = np.array(coeff_list)\n",
    "#         print(coeff)\n",
    "        #Adding new for scalalble and yeah\n",
    "        cc_curr = cc.split(\"-\")[0]\n",
    "        # Adding another check for scalable and yeah\n",
    "#         if cc_curr in ['scalable','yeah']:\n",
    "#             # Look at the fifth coefficient\n",
    "#             if round(coeff[0],6) < 0.000015:\n",
    "#                 print(coeff[0])\n",
    "#                 continue\n",
    "#         coeff.append(abs(curr_time[-1]-curr_time[0]))\n",
    "        mp[name] = {'d':degree, 'coeff':coeff, 'error':error_item, 'data':curr_data, 'time':curr_time}\n",
    "        if cc not in cc_mp :\n",
    "            cc_mp[cc] = []\n",
    "        cc_mp[cc].append(mp[name])\n",
    "    return cc_mp\n",
    "\n",
    "from scipy.stats import multivariate_normal as mvn\n",
    "def getPDensity(curr, cc_gaussian_params):\n",
    "    prob = {}\n",
    "    for cc in cc_gaussian_params:\n",
    "        mn = cc_gaussian_params[cc]['mean']\n",
    "        covar = cc_gaussian_params[cc]['covar']\n",
    "#         print(\"CC being checked against\",cc)\n",
    "#         print(np.linalg.det(covar))\n",
    "        curr_p = mvn.pdf(curr,mean=mn, cov=covar, allow_singular=True)\n",
    "        prob[cc]=curr_p\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8d869e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBestDegree(nmp):\n",
    "    results = {}\n",
    "    too_much_error = {}\n",
    "    for name in nmp.keys():\n",
    "        print(name)\n",
    "#         curr_time, curr_data, retrans, rtt = plot_one_bt(name+\"-0-50-200-2-aws-88-60\",'y',1)\n",
    "        data = nmp[name]['data']\n",
    "        time = nmp[name]['time']\n",
    "        max_deg = nmp[name]['max_deg']\n",
    "        p_net = []\n",
    "        for coeff in nmp[name]['p_net']:\n",
    "            temp_pnet = [c for c in coeff]\n",
    "            p_net.append(temp_pnet)\n",
    "        mse_l = nmp[name]['mse_l']\n",
    "        plt.plot(time, data,c='k',label='Truth')\n",
    "#         plt.plot(time, fit_l)\n",
    "        for d in range(0, max_deg):\n",
    "            plot_label = \"degree\"+str(d+1)\n",
    "            fit_net = np.polyval(p_net[d],time)\n",
    "            plt.plot(time, fit_net,label=\"degree\" + str(d+1))\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        names = []\n",
    "        loss = []\n",
    "        lambd = 0.02\n",
    "        for i in range(len(mse_l)):\n",
    "            loss.append((i+1)*sum(p_net[i])*lambd)\n",
    "        print(\"loss\", loss)\n",
    "        print(\"mse\", mse_l)\n",
    "        for d in range(0, max_deg):\n",
    "            names.append(\"degree \"+str(d+1))\n",
    "        plt.bar(names,mse_l,color='blue',width=0.4,label=\"mse\")\n",
    "        plt.bar(names,loss,bottom=mse_l,color='maroon',width=0.4,label=\"reg_loss\")\n",
    "\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        errors = []\n",
    "        # The code for deciding the categories\n",
    "        for i in range(0,max_deg):\n",
    "            errors.append(loss[i]+mse_l[i])\n",
    "        deg = errors.index(min(errors))+1\n",
    "        \n",
    "        # Error threshold\n",
    "        error_threshold = 1\n",
    "        if errors[deg-1] > error_threshold:\n",
    "            current_cc = name.split(\"-\")[0]\n",
    "            too_much_error[current_cc]={\n",
    "                'deg':deg,\n",
    "                'coeff':p_net[deg-1],\n",
    "                'error':errors[deg-1]\n",
    "            }\n",
    "            continue\n",
    "        \n",
    "    #     if deg < 3:\n",
    "    #         deg = mse_l.index(min(mse_l[0:2]))+1\n",
    "    #     print(deg)\n",
    "        current_cc = name.split(\"-\")[0]\n",
    "        results[current_cc]={\n",
    "            'deg':deg,\n",
    "            'coeff':p_net[deg-1],\n",
    "            'error':errors[deg-1]\n",
    "        }\n",
    "    return results, too_much_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d703b795",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal as mvn\n",
    "def getPDensityR(curr, cc_gaussian_params):\n",
    "    curr_coeff = curr[:-1]\n",
    "    prob = {}\n",
    "    for cc in cc_gaussian_params:\n",
    "        mn = cc_gaussian_params[cc]['mean']\n",
    "        covar = cc_gaussian_params[cc]['covar']\n",
    "        if len(curr_coeff) == len(mn):\n",
    "            print(cc)\n",
    "            print(curr_coeff)\n",
    "            print(mn)\n",
    "            print(covar)\n",
    "            curr_p = mvn.pdf(curr_coeff,mean=mn, cov=covar, allow_singular=True)\n",
    "            print(curr_p)\n",
    "            prob[cc]=curr_p\n",
    "#         print(\"CC being checked against\",cc)\n",
    "#         print(np.linalg.det(covar))\n",
    "    return prob\n",
    "\n",
    "def getWebDensity(mp, cc_gaussian_params):\n",
    "    acc_w = {}\n",
    "    for web in mp:\n",
    "        p_dense = getPDensityR(mp[web]['coeff'], cc_gaussian_params)\n",
    "        if web not in acc_w:\n",
    "            acc_w[web]={}\n",
    "        ccs = np.array(list(p_dense.keys()))\n",
    "        vals = np.array(list(p_dense.values()))\n",
    "        \n",
    "        p_ind = list(np.argsort(vals))\n",
    "        p_ind.reverse()\n",
    "        acc_w[web]['ccs'] = [ccs[i] for i in p_ind]\n",
    "        acc_w[web]['density'] = np.array([vals[i] for i in p_ind])\n",
    "        maxx = max(acc_w[web]['density'])\n",
    "        minn = min(acc_w[web]['density'])\n",
    "        acc_w[web]['relative'] = (acc_w[web]['density']-minn)/(maxx-minn) \n",
    "    return acc_w\n",
    "\n",
    "def predictCC(acc_w):\n",
    "    pred = {}\n",
    "    for web in acc_w:\n",
    "        if acc_w[web]['density'][0] < 0.01:\n",
    "            pred[web]='new'\n",
    "        elif acc_w[web]['relative'][1] > 0.01:\n",
    "            pred[web]='confused'\n",
    "        else :\n",
    "            pred[web]=acc_w[web]['ccs'][0]\n",
    "    return pred, acc_w\n",
    "\n",
    "def getPredictions(cc_mp,cc_gp,no):\n",
    "    acc_w = getWebDensity(cc_mp, cc_gp)\n",
    "    pred, acc_w = predictCC(acc_w)\n",
    "    predictions = {}\n",
    "    no_feature = []\n",
    "    for f in no:\n",
    "        tp_mp = get_feature_degree_R([f],ss=225,p=\"y\",ft_thresh=1,max_deg=3)\n",
    "        if len(cc_mp.keys()) == 0:\n",
    "            continue\n",
    "        new_f = f.split(\"/\")[-1]\n",
    "        w = new_f.split(\"-\")[0]\n",
    "        if w not in acc_w.keys():\n",
    "            print(\"No Feature\")\n",
    "            no_feature.append(w)\n",
    "            predictions[w]=\"No feature\"\n",
    "        else:\n",
    "            print(cc_mp[new_f.split(\"-\")[0]]['coeff'])\n",
    "    #         time, data, retrans, rtt= plot_one_bt(f,\"y\",1)\n",
    "            predictions[w] = {}\n",
    "            print(\"Top 5 CCS :\", acc_w[w]['ccs'][0:5])\n",
    "            predictions[w]['ccs'] = acc_w[w]['ccs'][0:5]\n",
    "            \n",
    "            print(\"Density Values:\",acc_w[w]['density'][0:5])\n",
    "            predictions[w]['density'] = acc_w[w]['density'][0:5]\n",
    "            \n",
    "            print(\"Relative Density Values:\",acc_w[w]['relative'][0:5])\n",
    "            predictions[w]['relative'] = acc_w[w]['relative'] = acc_w[w]['relative'][0:5]\n",
    "            \n",
    "            print(\"Prediction:\",pred[w])\n",
    "            predictions[w]['final'] = pred[w]\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e9b3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('cc_gp_cubicQ.txt', 'rb') as f:\n",
    "    cc_gp = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970aff44",
   "metadata": {},
   "source": [
    "# Running the same on all files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2655740d",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = '../../aws/website_measurements/26_1k_cut/mumbai/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1492467a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_filter(file):\n",
    "    if \"udp.csv\" in file:\n",
    "        return 0\n",
    "    if \"0-50-100-2\" in file:\n",
    "        return 0\n",
    "    return 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3f4289",
   "metadata": {},
   "outputs": [],
   "source": [
    "mumbai_files = []\n",
    "import os \n",
    "for f in os.listdir(folder):\n",
    "    if file_filter(f) == 1:\n",
    "        mumbai_files.append(folder+f)\n",
    "mumbai_files = sorted(mumbai_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe277a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "classi = checkBBR(mumbai_files,p=\"n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3267c63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "yes,no,maybe,nan = getDivision(classi,mumbai_files)\n",
    "print(len(yes),len(no),len(maybe),len(nan))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315ea252",
   "metadata": {},
   "outputs": [],
   "source": [
    "web_mp = get_feature_degree_R(no,ss=225,p=\"n\",ft_thresh=1,max_deg=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef841a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "web_mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387c7041",
   "metadata": {},
   "outputs": [],
   "source": [
    "web_cc_mp, too_much_error = getBestDegree(web_mp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7f648a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The ones that have too much error\", len(too_much_error.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a260f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = [0,0,0]\n",
    "for web in web_cc_mp:\n",
    "    counts[web_cc_mp[web]['deg']-1]+=1\n",
    "print(\"1,2,3\")\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71aebb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "web_cc_mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9b879b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('vals_cubicQ_new.txt', 'rb') as f:\n",
    "    vals_cubicQ = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b028fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "vals_cubicQ.pop('vegas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61929fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "vals_cubicQ.pop('nv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68aba12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not using vegas and nv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09476e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_degree = {'bic': 1,\n",
    " 'dctcp': 2,\n",
    " 'highspeed': 2,\n",
    " 'htcp': 3,\n",
    " 'lp': 2,\n",
    " 'scalable': 1,\n",
    " 'veno': 3,\n",
    " 'westwood': 2,\n",
    " 'yeah': 1,\n",
    " 'cubic': 3,\n",
    " 'reno': 2,\n",
    "'cubicQ':3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689243d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the cc_gaussian_params:\n",
    "import pickle\n",
    "with open(\"../analysis/final/cc_degree.txt\",'wb') as f:\n",
    "    pickle.dump(cc_degree,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c82e692",
   "metadata": {},
   "source": [
    "# Detecting Outliers through distance measures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a9a3bc",
   "metadata": {},
   "source": [
    "### Scaling the coeff for correct analysis according to degree and calculating mean "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f8b0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e00948d",
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees = [1,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1ac0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mm_scaled_vals = {}\n",
    "# for cc in vals_cubicQ:\n",
    "#     if cc == 'nv' :\n",
    "#         continue\n",
    "#     if cc_degree[cc] not in mm_scaled_vals:\n",
    "#         mm_scaled_vals[cc_degree[cc]] = {}\n",
    "#     mm_scaled_vals[cc_degree[cc]][cc] = vals_cubicQ[cc][1]\n",
    "\n",
    "# for d in mm_scaled_vals:\n",
    "#     temp_data = []\n",
    "#     for cc in mm_scaled_vals[d]:\n",
    "#         temp_data+=mm_scaled_vals[d][cc]\n",
    "#     temp_data = np.array(temp_data)\n",
    "#     min_data = np.min(temp_data)\n",
    "#     max_data = np.max(temp_data)\n",
    "#     for cc in mm_scaled_vals[d]:\n",
    "#         mm_scaled_vals[d][cc] = (mm_scaled_vals[d][cc]-min_data)/(max_data-min_data)\n",
    "#     mm_scaled_vals[d]['max_data'] = max_data \n",
    "#     mm_scaled_vals[d]['min_data'] = min_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330271e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_vals = {}\n",
    "for cc in vals_cubicQ:\n",
    "    if cc_degree[cc] not in scaled_vals:\n",
    "        scaled_vals[cc_degree[cc]] = {}\n",
    "    scaled_vals[cc_degree[cc]][cc] = vals_cubicQ[cc][1]\n",
    "\n",
    "for d in scaled_vals:\n",
    "    temp_data = []\n",
    "    for cc in scaled_vals[d]:\n",
    "        temp_data += scaled_vals[d][cc]\n",
    "    scaler = StandardScaler().fit(temp_data)\n",
    "    for cc in scaled_vals[d]:\n",
    "        scaled_vals[d][cc] = np.array(scaler.transform(scaled_vals[d][cc]))\n",
    "        if cc == 'scalable':\n",
    "            for i in range(len(scaled_vals[d][cc])):\n",
    "                if scaled_vals[d][cc][i] > 10 :\n",
    "                    scaled_vals[d][cc][i] = scaled_vals[d][cc][i-1]\n",
    "                    \n",
    "    scaled_vals[d]['scaler'] = scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff85679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the cc_gaussian_params:\n",
    "import pickle\n",
    "with open(\"../analysis/final/vals.txt\",'wb') as f:\n",
    "    pickle.dump(vals_cubicQ,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7741c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the cc_gaussian_params:\n",
    "import pickle\n",
    "with open(\"../analysis/final/scaled_vals.txt\",'wb') as f:\n",
    "    pickle.dump(scaled_vals,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca982d0d",
   "metadata": {},
   "source": [
    "### Plotting all the ccs with the websites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edb62b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_website_by_degree(d):\n",
    "    labels = []\n",
    "    result = []\n",
    "    for web in web_cc_mp:\n",
    "        if web_cc_mp[web]['deg'] == d:\n",
    "            labels.append(web)\n",
    "            result.append(web_cc_mp[web]['coeff'][0:d])\n",
    "    return {'labels' : labels, 'data' : result}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e67f700",
   "metadata": {},
   "outputs": [],
   "source": [
    "web_data = {}\n",
    "scaled_web_data = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9efea2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "degree = 2\n",
    "scaled_vals[degree]['combined'] = []\n",
    "for cc in ['dctcp','highspeed','lp','reno']:\n",
    "    scaled_vals[degree]['combined'] += list(scaled_vals[degree][cc])\n",
    "    scaled_vals[degree].pop(cc)\n",
    "scaled_vals[degree]['combined'] = np.array(scaled_vals[degree]['combined'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c63cf8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_vals[2].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fa5cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# web_data = {}\n",
    "# mm_scaled_web_data = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2fad15",
   "metadata": {},
   "source": [
    "#### D = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b30ea91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# not_cc = ['max_data','min_data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0ba2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "degree = 1\n",
    "web_data[degree]  = get_website_by_degree(degree)\n",
    "scaled_web_data[degree] = {'labels':web_data[degree]['labels'], 'data':np.array(scaled_vals[degree]['scaler'].transform(web_data[degree]['data']))}\n",
    "\n",
    "fig, axs = plt.subplots(nrows=1,ncols=3,figsize=(12,5))\n",
    "for cc in scaled_vals[degree]:\n",
    "    if cc != 'scaler' :\n",
    "        axs[0].scatter(scaled_vals[degree][cc][:,0],scaled_vals[degree][cc][:,0],label=cc,alpha=0.5)\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].scatter(scaled_web_data[degree]['data'][:,0], scaled_web_data[degree]['data'][:,0], label='websites',alpha=0.5)\n",
    "axs[1].legend()\n",
    "\n",
    "for cc in scaled_vals[degree]:\n",
    "    if cc != 'scaler' :\n",
    "        axs[2].scatter(scaled_vals[degree][cc][:,0],scaled_vals[degree][cc][:,0],label=cc,alpha=0.5)\n",
    "axs[2].legend()\n",
    "\n",
    "axs[2].scatter(scaled_web_data[degree]['data'][:,0], scaled_web_data[degree]['data'][:,0], label='websites',alpha=0.5)\n",
    "axs[2].legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5724f8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# degree = 1\n",
    "# web_data[degree]  = get_website_by_degree(degree)\n",
    "# mm_scaled_web_data[degree] = {'labels':web_data[degree]['labels'],\n",
    "#                               'data':(web_data[degree]['data']-mm_scaled_vals[degree]['min_data'])/(mm_scaled_vals[degree]['max_data']-mm_scaled_vals[degree]['min_data'])}\n",
    "\n",
    "# fig, axs = plt.subplots(nrows=1,ncols=3,figsize=(12,5))\n",
    "# for cc in mm_scaled_vals[degree]:\n",
    "#     if cc not in not_cc:\n",
    "#         axs[0].scatter(mm_scaled_vals[degree][cc][:,0],mm_scaled_vals[degree][cc][:,0],label=cc,alpha=0.5)\n",
    "# axs[0].legend()\n",
    "\n",
    "# axs[1].scatter(mm_scaled_web_data[degree]['data'][:,0], mm_scaled_web_data[degree]['data'][:,0], label='websites',alpha=0.5)\n",
    "# axs[1].legend()\n",
    "\n",
    "# for cc in mm_scaled_vals[degree]:\n",
    "#     if cc not in not_cc :\n",
    "#         axs[2].scatter(mm_scaled_vals[degree][cc][:,0],mm_scaled_vals[degree][cc][:,0],label=cc,alpha=0.5)\n",
    "# axs[2].legend()\n",
    "\n",
    "# axs[2].scatter(mm_scaled_web_data[degree]['data'][:,0], mm_scaled_web_data[degree]['data'][:,0], label='websites',alpha=0.5)\n",
    "# axs[2].legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f914aa92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# degree = 2\n",
    "# mm_scaled_vals[degree]['combined'] = []\n",
    "# for cc in ['dctcp','highspeed','lp','reno']:\n",
    "#     mm_scaled_vals[degree]['combined'] += list(mm_scaled_vals[degree][cc])\n",
    "# mm_scaled_vals[degree]['combined'] = np.array(mm_scaled_vals[degree]['combined'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8ac94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "degree = 2\n",
    "scaled_vals[degree]['combined'] = []\n",
    "for cc in ['dctcp','highspeed','lp','reno']:\n",
    "    scaled_vals[degree]['combined'] += list(scaled_vals[degree][cc])\n",
    "scaled_vals[degree]['combined'] = np.array(scaled_vals[degree]['combined'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddc7c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# degree = 2\n",
    "# web_data[degree]  = get_website_by_degree(degree)\n",
    "# mm_scaled_web_data[degree] = {'labels':web_data[degree]['labels'],\n",
    "#                               'data':(web_data[degree]['data']-mm_scaled_vals[degree]['min_data'])/(mm_scaled_vals[degree]['max_data']-mm_scaled_vals[degree]['min_data'])}\n",
    "\n",
    "# fig, axs = plt.subplots(nrows=1,ncols=3,figsize=(12,5))\n",
    "# for cc in mm_scaled_vals[degree]:\n",
    "#     if cc not in not_cc:\n",
    "#         axs[0].scatter(mm_scaled_vals[degree][cc][:,0],mm_scaled_vals[degree][cc][:,1],label=cc,alpha=0.5)\n",
    "# axs[0].legend()\n",
    "\n",
    "# axs[1].scatter(mm_scaled_web_data[degree]['data'][:,0], mm_scaled_web_data[degree]['data'][:,1], label='websites',alpha=0.5)\n",
    "# axs[1].legend()\n",
    "\n",
    "# for cc in mm_scaled_vals[degree]:\n",
    "#     if cc not in not_cc :\n",
    "#         axs[2].scatter(mm_scaled_vals[degree][cc][:,0],mm_scaled_vals[degree][cc][:,1],label=cc,alpha=0.5)\n",
    "# axs[2].legend()\n",
    "\n",
    "# axs[2].scatter(mm_scaled_web_data[degree]['data'][:,0], mm_scaled_web_data[degree]['data'][:,1], label='websites',alpha=0.5)\n",
    "# axs[2].legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304fcee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "degree = 2\n",
    "web_data[degree]  = get_website_by_degree(degree)\n",
    "scaled_web_data[degree] = {'labels':web_data[degree]['labels'], 'data':np.array(scaled_vals[degree]['scaler'].transform(web_data[degree]['data']))}\n",
    "\n",
    "fig, axs = plt.subplots(nrows=1,ncols=3,figsize=(12,5))\n",
    "for cc in scaled_vals[degree]:\n",
    "    if cc != 'scaler' :\n",
    "        axs[0].scatter(scaled_vals[degree][cc][:,0],scaled_vals[degree][cc][:,1],label=cc,alpha=0.5)\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].scatter(scaled_web_data[degree]['data'][:,0], scaled_web_data[degree]['data'][:,1], label='websites',alpha=0.5)\n",
    "axs[1].legend()\n",
    "\n",
    "for cc in scaled_vals[degree]:\n",
    "    if cc != 'scaler' :\n",
    "        axs[2].scatter(scaled_vals[degree][cc][:,0],scaled_vals[degree][cc][:,1],label=cc,alpha=0.5)\n",
    "axs[2].legend()\n",
    "\n",
    "axs[2].scatter(scaled_web_data[degree]['data'][:,0], scaled_web_data[degree]['data'][:,1], label='websites',alpha=0.5)\n",
    "axs[2].legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4942a8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# degree = 2\n",
    "# fig = plt.figure(figsize=(10,5))\n",
    "# for cc in mm_scaled_vals[degree]:\n",
    "#     if cc not in not_cc:\n",
    "#         if cc != 'cubicQ' and cc != \"htcp\": \n",
    "            \n",
    "#             plt.scatter(mm_scaled_vals[degree][cc][:,0],mm_scaled_vals[degree][cc][:,1],label=cc,alpha=0.5)\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c0fa1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "degree = 2\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "for cc in scaled_vals[degree]:\n",
    "    if cc != 'scaler':\n",
    "        if cc != 'cubicQ' and cc != \"htcp\":\n",
    "            plt.scatter(scaled_vals[degree][cc][:,0],scaled_vals[degree][cc][:,1],label=cc,alpha=0.5)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2619d3e5",
   "metadata": {},
   "source": [
    "Combining dctcp, highspeed, lp, reno and westwood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74676011",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea133cdb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# degree = 3\n",
    "# web_data[degree]  = get_website_by_degree(degree)\n",
    "# mm_scaled_web_data[degree] = {'labels':web_data[degree]['labels'],\n",
    "#                               'data':(web_data[degree]['data']-mm_scaled_vals[degree]['min_data'])/(mm_scaled_vals[degree]['max_data']-mm_scaled_vals[degree]['min_data'])}\n",
    "\n",
    "# fig = plt.figure(figsize=(15,8))\n",
    "# ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# for cc in mm_scaled_vals[degree]:\n",
    "#     if cc not in not_cc :\n",
    "#         ax.scatter(mm_scaled_vals[degree][cc][:,0],mm_scaled_vals[degree][cc][:,1], mm_scaled_vals[degree][cc][:,2], label=cc,alpha=0.5)\n",
    "# ax.legend()\n",
    "\n",
    "\n",
    "# fig = plt.figure(figsize=(15,8))\n",
    "# ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# ax.scatter(mm_scaled_web_data[degree]['data'][:,0], mm_scaled_web_data[degree]['data'][:,1], mm_scaled_web_data[degree]['data'][:,2],  label='websites',alpha=0.5)\n",
    "# ax.legend()\n",
    "\n",
    "\n",
    "# fig = plt.figure(figsize=(15,8))\n",
    "# ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# for cc in mm_scaled_vals[degree]:\n",
    "#     if cc not in not_cc :\n",
    "#         ax.scatter(mm_scaled_vals[degree][cc][:,0],mm_scaled_vals[degree][cc][:,1],mm_scaled_vals[degree][cc][:,2],label=cc,alpha=0.5)\n",
    "# ax.legend()\n",
    "\n",
    "# ax.scatter(mm_scaled_web_data[degree]['data'][:,0], mm_scaled_web_data[degree]['data'][:,1],mm_scaled_web_data[degree]['data'][:,2], label='websites',alpha=0.5)\n",
    "# ax.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64ba37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "degree = 3\n",
    "web_data[degree]  = get_website_by_degree(degree)\n",
    "scaled_web_data[degree] = {'labels':web_data[degree]['labels'], 'data':np.array(scaled_vals[degree]['scaler'].transform(web_data[degree]['data']))}\n",
    "\n",
    "fig = plt.figure(figsize=(15,8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "for cc in scaled_vals[degree]:\n",
    "    if cc != 'scaler' :\n",
    "        ax.scatter(scaled_vals[degree][cc][:,0],scaled_vals[degree][cc][:,1], scaled_vals[degree][cc][:,2], label=cc,alpha=0.5)\n",
    "ax.legend()\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(15,8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "ax.scatter(scaled_web_data[degree]['data'][:,0], scaled_web_data[degree]['data'][:,1], scaled_web_data[degree]['data'][:,2],  label='websites',alpha=0.5)\n",
    "ax.legend()\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(15,8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "for cc in scaled_vals[degree]:\n",
    "    if cc != 'scaler' :\n",
    "        ax.scatter(scaled_vals[degree][cc][:,0],scaled_vals[degree][cc][:,1], scaled_vals[degree][cc][:,2],label=cc,alpha=0.5)\n",
    "ax.legend()\n",
    "\n",
    "ax.scatter(scaled_web_data[degree]['data'][:,0], scaled_web_data[degree]['data'][:,1],scaled_web_data[degree]['data'][:,2], label='websites',alpha=0.5)\n",
    "ax.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478aef58",
   "metadata": {},
   "source": [
    "### Detecting outliers (different thresholds for different degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa61129",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_thresh(thresh,degree):\n",
    "    return np.array(degree*[thresh])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d7880f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calc_thresh(degree,weight=[1]):\n",
    "#     total_sum = 0\n",
    "#     total_len = 0\n",
    "#     for cc in scaled_vals[degree]:\n",
    "#         if cc in not_cc:\n",
    "#             continue\n",
    "#         mn = np.mean(scaled_vals[degree][cc])\n",
    "# #         print(mn)\n",
    "#         dist = weight*(abs(scaled_vals[degree][cc]-mn))\n",
    "#         dist = np.sqrt(np.sum((dist**2),axis=1))\n",
    "# #         print(dist)\n",
    "#         total_sum += np.sum(dist)\n",
    "#         total_len += len(dist)\n",
    "# #         print(total_sum,total_len)\n",
    "#     return total_sum/total_len\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0e3bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_non_outliers(degree,thresh,weight=[1]):\n",
    "    non_outliers_index = []\n",
    "    \n",
    "    for cc in scaled_vals[degree]:\n",
    "        if cc == 'scaler':\n",
    "            continue\n",
    "        for i in range(len(x)):\n",
    "            isNo = 1\n",
    "            for d in range(degree):\n",
    "                if abs(x[i][d]) > thresh:\n",
    "                    isNo = 0\n",
    "            if isNo:\n",
    "                non_outliers_index.append(i)\n",
    "    non_outliers_index = list(set(non_outliers_index))\n",
    "    return non_outliers_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d324c602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_non_outliers(degree,weight=[1]):\n",
    "#     non_outliers_index = []\n",
    "#     for cc in scaled_vals[degree]:\n",
    "#         if cc in not_cc:\n",
    "#             continue\n",
    "#         mn = np.mean(scaled_vals[degree][cc],axis=0)\n",
    "#         dist = weight*(abs(x-mn))\n",
    "#         dist = np.sqrt(np.sum(dist**2,axis=1))\n",
    "#         for i in range(len(x)):\n",
    "#             print(dist[i],dist[i]<thresh)\n",
    "#             if dist[i] < thresh:\n",
    "#                 non_outliers_index.append(i)\n",
    "#     non_outliers_index = list(set(non_outliers_index))\n",
    "#     return non_outliers_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bf3a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_index(out,l,co='b'):\n",
    "    n_rows = math.ceil(len(out)/3)\n",
    "    if n_rows == 0:\n",
    "        return\n",
    "    fig, ax = plt.subplots(n_rows,3,figsize=(10,3*n_rows))\n",
    "    c=0\n",
    "    if n_rows == 1:\n",
    "        for j in range(3):\n",
    "            if c < len(out):\n",
    "                ax[j].plot(web_mp[l[out[c]]]['time'],web_mp[l[out[c]]]['data'],color=co)\n",
    "                ax[j].set_title(label=l[out[c]])\n",
    "                c+=1\n",
    "        return\n",
    "        \n",
    "    for i in range(n_rows):\n",
    "        for j in range(3):\n",
    "            if c < len(out):\n",
    "                ax[i][j].plot(web_mp[l[out[c]]]['time'],web_mp[l[out[c]]]['data'],color=co)\n",
    "                ax[i][j].set_title(label=l[out[c]])\n",
    "                c+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0cbe42",
   "metadata": {},
   "outputs": [],
   "source": [
    "usable_web = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d2c543",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "degree = 1\n",
    "thresh = calc_thresh(degree,3)\n",
    "print(\"Thresh\", thresh)\n",
    "non_outliers_index = []\n",
    "x = scaled_web_data[degree]['data']\n",
    "l = scaled_web_data[degree]['labels']\n",
    "print(\"Total\",len(l))\n",
    "non_outliers_index = get_non_outliers(degree,3)\n",
    "print(\"Non outliers\", len(non_outliers_index))\n",
    "fig = plt.figure(figsize=(7,7))\n",
    "for cc in scaled_vals[degree]:\n",
    "    if cc=='scaler':\n",
    "        continue\n",
    "    plt.scatter(scaled_vals[degree][cc][:,0],scaled_vals[degree][cc][:,0],label=cc,alpha=0.5)\n",
    "pts = np.array([scaled_web_data[degree]['data'][i] for i in non_outliers_index])\n",
    "plt.scatter(pts[:,0], pts[:,0],label='web',alpha=0.5)\n",
    "plt.legend()\n",
    "\n",
    "plot_index(non_outliers_index,l)\n",
    "# n_rows = math.ceil(len(non_outliers_index)/3)\n",
    "# fig, ax = plt.subplots(n_rows,3,figsize=(10,3*n_rows))\n",
    "# c=0\n",
    "# for i in range(n_rows):\n",
    "#     for j in range(3):\n",
    "#         if c < len(non_outliers_index):\n",
    "#             ax[i][j].plot(web_mp[l[non_outliers_index[c]]]['time'],web_mp[l[non_outliers_index[c]]]['data'],label=l[c])\n",
    "#             c+=1\n",
    "# plt.legend()\n",
    "outliers_index = [i for i in range(len(l)) if i not in non_outliers_index]\n",
    "print(\"Outliers\",len(outliers_index))\n",
    "plot_index(outliers_index,l,'r')\n",
    "\n",
    "# n_rows = math.ceil(len(outliers_index)/3)\n",
    "# fig, ax = plt.subplots(n_rows,3,figsize=(10,3*n_rows))\n",
    "# c = 0\n",
    "# for i in range(n_rows):\n",
    "#     for j in range(3):\n",
    "#         if c < len(l):\n",
    "#             ax[i][j].plot(web_mp[l[c]]['time'],web_mp[l[c]]['data'],color='r')\n",
    "#             c+=1\n",
    "usable_web[degree] = non_outliers_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4d8056",
   "metadata": {},
   "outputs": [],
   "source": [
    "degree = 2\n",
    "thresh = calc_thresh(degree,3)\n",
    "print(\"Thresh\", thresh)\n",
    "non_outliers_index = []\n",
    "x = scaled_web_data[degree]['data']\n",
    "l = scaled_web_data[degree]['labels']\n",
    "print(\"Total\",len(l))\n",
    "non_outliers_index = get_non_outliers(degree,15,[0.67,0.33])\n",
    "print(\"Non outliers\", len(non_outliers_index))\n",
    "\n",
    "fig = plt.figure(figsize=(7,7))\n",
    "for cc in scaled_vals[degree]:\n",
    "    if cc=='scaler':\n",
    "        continue\n",
    "    if cc in ['reno','lp','dctcp','highspeed']:\n",
    "        continue\n",
    "    plt.scatter(scaled_vals[degree][cc][:,0],scaled_vals[degree][cc][:,1],label=cc,alpha=0.5)\n",
    "pts = np.array([scaled_web_data[degree]['data'][i] for i in non_outliers_index])\n",
    "if len(pts) > 0:\n",
    "    plt.scatter(pts[:,0], pts[:,1],label='web',alpha=0.5)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plot_index(non_outliers_index,l)\n",
    "# n_rows = math.ceil(len(non_outliers_index)/3)\n",
    "# fig, ax = plt.subplots(n_rows,3,figsize=(10,3*n_rows))\n",
    "# c=0\n",
    "# for i in range(n_rows):\n",
    "#     for j in range(3):\n",
    "#         if c < len(non_outliers_index):\n",
    "#             ax[i][j].plot(web_mp[l[non_outliers_index[c]]]['time'],web_mp[l[non_outliers_index[c]]]['data'],label=l[c])\n",
    "#             c+=1\n",
    "# plt.legend()\n",
    "outliers_index = [i for i in range(len(l)) if i not in non_outliers_index]\n",
    "print(\"Outliers\",len(outliers_index))\n",
    "plot_index(outliers_index,l,'r')\n",
    "# n_rows = math.ceil(len(outliers_index)/3)\n",
    "# fig, ax = plt.subplots(n_rows,3,figsize=(10,3*n_rows))\n",
    "# c = 0\n",
    "# for i in range(n_rows):\n",
    "#     for j in range(3):\n",
    "#         if c < len(l):\n",
    "#             ax[i][j].plot(web_mp[l[c]]['time'],web_mp[l[c]]['data'],color='r')\n",
    "#             c+=1\n",
    "usable_web[degree] = non_outliers_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea14370",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"inquirer.net\" \n",
    "new_name = \"../../aws/website_measurements/26_1k_cut/mumbai/\" + name + \"-0-50-200-2-tcp.csv\"\n",
    "plot_one_bt(new_name,p=\"y\",t=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfffee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "degree = 3\n",
    "thresh = calc_thresh(degree,3)\n",
    "print(\"Thresh\", thresh)\n",
    "non_outliers_index = []\n",
    "x = scaled_web_data[degree]['data']\n",
    "l = scaled_web_data[degree]['labels']\n",
    "print(\"Total\",len(l))\n",
    "\n",
    "non_outliers_index = get_non_outliers(degree,3,[0.50,0.33,0.17])\n",
    "\n",
    "print(\"Non outliers\", len(non_outliers_index))\n",
    "\n",
    "fig = plt.figure(figsize=(15,8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "for cc in scaled_vals[degree]:\n",
    "    if cc=='scaler':\n",
    "        continue\n",
    "    ax.scatter(scaled_vals[degree][cc][:,0],scaled_vals[degree][cc][:,1],scaled_vals[degree][cc][:,2],label=cc,alpha=0.5)\n",
    "pts = np.array([scaled_web_data[degree]['data'][i] for i in non_outliers_index])\n",
    "ax.scatter(pts[:,0], pts[:,1], pts[:,2],label='web',alpha=0.5)\n",
    "ax.legend()\n",
    "plt.show()\n",
    "plot_index(non_outliers_index,l)\n",
    "# n_rows = math.ceil(len(non_outliers_index)/3)\n",
    "# fig, ax = plt.subplots(n_rows,3,figsize=(10,3*n_rows))\n",
    "# c=0\n",
    "# for i in range(n_rows):\n",
    "#     for j in range(3):\n",
    "#         if c < len(non_outliers_index):\n",
    "#             ax[i][j].plot(web_mp[l[non_outliers_index[c]]]['time'],web_mp[l[non_outliers_index[c]]]['data'],label=l[c])\n",
    "#             c+=1\n",
    "# plt.legend()\n",
    "outliers_index = [i for i in range(len(l)) if i not in non_outliers_index]\n",
    "print(\"Outliers\",len(outliers_index))\n",
    "plot_index(outliers_index,l,'r')\n",
    "# n_rows = math.ceil(len(outliers_index)/3)\n",
    "# fig, ax = plt.subplots(n_rows,3,figsize=(10,3*n_rows))\n",
    "# c = 0\n",
    "# for i in range(n_rows):\n",
    "#     for j in range(3):\n",
    "#         if c < len(l):\n",
    "#             ax[i][j].plot(web_mp[l[c]]['time'],web_mp[l[c]]['data'],color='r')\n",
    "#             c+=1\n",
    "usable_web[degree] = non_outliers_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5b2a90",
   "metadata": {},
   "source": [
    "## Now applying gaussian model on all the website data and getting the best probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6bbcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_vals[2].pop('combined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c232b04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_data(degree):\n",
    "    data = []\n",
    "    labels = []\n",
    "    for i in usable_web[degree]:\n",
    "        data.append(scaled_web_data[degree]['data'][i])\n",
    "        labels.append(scaled_web_data[degree]['labels'][i])\n",
    "    return data,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8b8afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_data(degree):\n",
    "    X = []\n",
    "    y = []\n",
    "    count_to_map = {}\n",
    "    count = 1\n",
    "    for cc in scaled_vals[degree]:\n",
    "        if cc != 'scaler':\n",
    "            X += list(scaled_vals[degree][cc])\n",
    "            y += [count]*len(scaled_vals[degree][cc])\n",
    "            count_to_map[count] = cc\n",
    "            count+=1\n",
    "    return X, y, count_to_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d12d572",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = {}\n",
    "y = {}\n",
    "count_to_mp = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07976594",
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in [1,2,3]:\n",
    "    X[d], y[d], count_to_mp[d] = create_train_data(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcd11c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = {}\n",
    "for i in [1,2,3]:\n",
    "    data_test[i] = {}\n",
    "    data_test[i]['data'], data_test[i]['labels'] = create_test_data(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd432bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_test(out,l,predict_cc, co='b'):\n",
    "    n_rows = math.ceil(len(out)/3)\n",
    "    if n_rows == 0:\n",
    "        return\n",
    "    fig, ax = plt.subplots(n_rows,3,figsize=(10,3*n_rows))\n",
    "    c=0\n",
    "    if n_rows == 1:\n",
    "        for j in range(3):\n",
    "            if c < len(out):\n",
    "                ax[j].plot(web_mp[l[out[c]]]['time'],web_mp[l[out[c]]]['data'],color=co)\n",
    "                ax[j].set_title(label = predict_cc[c] + \" \" + l[out[c]])\n",
    "                c+=1\n",
    "        return\n",
    "        \n",
    "    for i in range(n_rows):\n",
    "        for j in range(3):\n",
    "            if c < len(out):\n",
    "                ax[i][j].plot(web_mp[l[out[c]]]['time'],web_mp[l[out[c]]]['data'],color=co)\n",
    "                ax[i][j].set_title(label = predict_cc[c] + \" \" + l[out[c]])\n",
    "                c+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af5f48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To ADD :\n",
    "# Count and prior probability check\n",
    "# Result storing and showing on the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88824fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9340db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "degree = 3\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "clf = GaussianNB()\n",
    "X_t = np.array(X[degree])\n",
    "y_t = np.array(y[degree])\n",
    "clf.fit(X_t,y_t)\n",
    "classifiers[degree] = clf\n",
    "estimates = clf.predict(data_test[degree]['data'])\n",
    "prob = clf.predict_proba(data_test[degree]['data'])\n",
    "inde = [i for i in range(len(data_test[degree]['data']))]\n",
    "cc_predict = [count_to_mp[degree][i] for i in estimates]\n",
    "plot_test(inde,data_test[degree]['labels'],cc_predict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821875a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_count = {}\n",
    "for cc in cc_predict:\n",
    "    if cc not in cc_count:\n",
    "        cc_count[cc] = 0\n",
    "    cc_count[cc]+=1\n",
    "print(cc_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ca8d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "degree = 2\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "clf = GaussianNB()\n",
    "X_t = np.array(X[degree])\n",
    "y_t = np.array(y[degree])\n",
    "clf.fit(X_t,y_t)\n",
    "classifiers[degree] = clf\n",
    "estimates = clf.predict(data_test[degree]['data'])\n",
    "prob = clf.predict_proba(data_test[degree]['data'])\n",
    "inde = [i for i in range(len(data_test[degree]['data']))]\n",
    "cc_predict = [count_to_mp[degree][i] for i in estimates]\n",
    "plot_test(inde,data_test[degree]['labels'],cc_predict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edd5cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "degree = 1\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "clf = GaussianNB()\n",
    "X_t = np.array(X[degree])\n",
    "y_t = np.array(y[degree])\n",
    "clf.fit(X_t,y_t)\n",
    "classifiers[degree] = clf\n",
    "estimates = clf.predict(data_test[degree]['data'])\n",
    "prob = clf.predict_proba(data_test[degree]['data'])\n",
    "inde = [i for i in range(len(data_test[degree]['data']))]\n",
    "cc_predict = [count_to_mp[degree][i] for i in estimates]\n",
    "plot_test(inde,data_test[degree]['labels'],cc_predict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077ba54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "degree = 1\n",
    "clf = classifiers[degree]\n",
    "estimates = clf.predict(data_test[degree]['data'])\n",
    "prob = clf.predict_proba(data_test[degree]['data'])\n",
    "inde = [i for i in range(len(data_test[degree]['data']))]\n",
    "cc_predict = [count_to_mp[degree][i] for i in estimates]\n",
    "plot_test(inde,data_test[degree]['labels'],cc_predict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d874e252",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../analysis/final/classifiers.txt\",\"wb\") as f:\n",
    "    pickle.dump(classifiers,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d98f7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../analysis/final/count_to_mp.txt\",\"wb\") as f:\n",
    "    pickle.dump(count_to_mp,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf8a343",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce0a4e4",
   "metadata": {},
   "source": [
    "## Simultaneously using a model shape from the websites to help use Dynamic Time Wraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253f3137",
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 0.05\n",
    "n_t = []\n",
    "for i in range(201):\n",
    "    n_t.append(round(step*i,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab19a068",
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff_best = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5da2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cc in vals_cubicQ:\n",
    "    deg = cc_degree[cc]\n",
    "    coeff_cc = list(np.mean(vals_cubicQ[cc][1],axis=0))\n",
    "    print(cc)\n",
    "    print(coeff_cc)\n",
    "    coeff_cc.append(0)\n",
    "    y_cc = np.polyval(coeff_cc,n_t)\n",
    "    if deg not in coeff_best:\n",
    "        coeff_best[deg] = {}\n",
    "    coeff_best[deg][cc] = y_cc\n",
    "    plt.plot(n_t,y_cc)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d63530",
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b35467",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fastdtw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6343a662",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "a = np.random.random(100)\n",
    "b = np.random.random(200)\n",
    "\n",
    "distance, path = fastdtw(a, b, dist=euclidean)\n",
    "print(f\"DTW Distance: {distance}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8856bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "web_cc_mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6599408",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastdtw import fastdtw\n",
    "from scipy.spatial.distance import euclidean\n",
    "import numpy as np\n",
    "\n",
    "a = np.random.random(100)\n",
    "b = np.random.random(200)\n",
    "\n",
    "distance, path = fastdtw(a, b)\n",
    "print(f\"DTW Distance: {distance}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af527e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastdtw import fastdtw\n",
    "from scipy.spatial.distance import euclidean\n",
    "import numpy as np\n",
    "\n",
    "web_dtw = {}\n",
    "for web in web_mp:\n",
    "    dist = []\n",
    "    cc_ts = []\n",
    "    test_ts = web_mp[web]['data']\n",
    "    if web in web_cc_mp:\n",
    "        deg = web_cc_mp[web]['deg']\n",
    "    else:\n",
    "        continue\n",
    "    for cc in coeff_best[deg]:\n",
    "        cc_ts.append(cc)\n",
    "        orig_ts = coeff_best[deg][cc]\n",
    "#         print(test_ts)\n",
    "#         print(orig_ts)\n",
    "        distance, path = fastdtw(test_ts, orig_ts)\n",
    "        dist.append(distance)\n",
    "    if deg not in web_dtw:\n",
    "        web_dtw[deg] = {}\n",
    "    web_dtw[deg][web] = {\n",
    "        'cc' : cc_ts,\n",
    "        'dist' : dist\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e3bfb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "web_dtw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e181743f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for deg in web_dtw:\n",
    "    for web in web_dtw[deg]:\n",
    "        min_ind = web_dtw[deg][web]['dist'].index(min(web_dtw[deg][web]['dist']))\n",
    "        cc_ind =  web_dtw[deg][web]['cc'][min_ind]b\n",
    "        plt.plot(web_mp[web]['time'], web_mp[web]['data'])\n",
    "        plt.title(cc_ind+\" \"+web)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da418522",
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_web_dtw = web_dtw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281d7f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install dtw-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1376c99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dtw import *\n",
    "import numpy as np\n",
    "\n",
    "web_dtw = {}\n",
    "for web in web_mp:\n",
    "    dist = []\n",
    "    cc_ts = []\n",
    "    test_ts = web_mp[web]['data']\n",
    "    if web in web_cc_mp:\n",
    "        deg = web_cc_mp[web]['deg']\n",
    "    else:\n",
    "        continue\n",
    "    for cc in coeff_best[deg]:\n",
    "        cc_ts.append(cc)\n",
    "        orig_ts = coeff_best[deg][cc]\n",
    "#         print(test_ts)\n",
    "#         print(orig_ts)\n",
    "        alignment = dtw(test_ts,orig_ts,dist_method=euclidean)\n",
    "#         plt.plot(alignment.index1, alignment.index2)  \n",
    "#         plt.show()\n",
    "        dist.append(alignment.distance)\n",
    "    if deg not in web_dtw:\n",
    "        web_dtw[deg] = {}\n",
    "    web_dtw[deg][web] = {\n",
    "        'cc' : cc_ts,\n",
    "        'dist' : dist\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ece809a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_count = {}\n",
    "for deg in web_dtw:\n",
    "    for web in web_dtw[deg]:\n",
    "        min_ind = web_dtw[deg][web]['dist'].index(min(web_dtw[deg][web]['dist']))\n",
    "        cc_ind =  web_dtw[deg][web]['cc'][min_ind]\n",
    "        if cc_ind not in cc_count:\n",
    "            cc_count[cc_ind]=0\n",
    "        cc_count[cc_ind]+=1\n",
    "        print(web_dtw[deg][web]['dist'])\n",
    "        plt.plot(web_mp[web]['time'], web_mp[web]['data'])\n",
    "        plt.title(cc_ind+\" \"+web)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa318ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cc_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f60435e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbfd060",
   "metadata": {},
   "outputs": [],
   "source": [
    "for deg in web_dtw:\n",
    "    for web in web_dtw[deg]:\n",
    "        min_ind = web_dtw[deg][web]['dist'].index(min(web_dtw[deg][web]['dist']))\n",
    "        cc_ind =  web_dtw[deg][web]['cc'][min_ind]\n",
    "        print(web_dtw[deg][web]['dist'])\n",
    "        plt.plot(web_mp[web]['time'], web_mp[web]['data'])\n",
    "        plt.title(cc_ind+\" \"+web)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b0d178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEGREE 3\n",
    "x_train = []\n",
    "y_train = []\n",
    "count = 0\n",
    "count_cc_mp = {}\n",
    "for cc in cc_degree:\n",
    "    if cc == 'nv':\n",
    "        continue\n",
    "    if cc_degree[cc] == 3:\n",
    "        length = len(vals_cubicQ[cc][1])\n",
    "        count_cc_mp[count]=cc\n",
    "        y_t = [count] * length\n",
    "        x_t = [list(arr) for arr in vals_cubicQ[cc][1]]\n",
    "        x_train += x_t\n",
    "        y_train += y_t\n",
    "        count+=1\n",
    "\n",
    "print(x_train[0:5])\n",
    "print(y_train[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4af26f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_cc_mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96dd3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_grid = [{'kernel': ['rbf'], 'gamma': [1e-1, 10],\n",
    "                     'C': [1, 10, 100, 500,1000,2000]},\n",
    "                    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1df1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_model = GridSearchCV(SVC(), params_grid, cv=5)\n",
    "svm_model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6297124b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best score for training data:', svm_model.best_score_,\"\\n\") \n",
    "\n",
    "# View the best parameters for the model found using grid search\n",
    "print('Best C:',svm_model.best_estimator_.C,\"\\n\") \n",
    "print('Best Kernel:',svm_model.best_estimator_.kernel,\"\\n\")\n",
    "print('Best Gamma:',svm_model.best_estimator_.gamma,\"\\n\")\n",
    "\n",
    "final_model = svm_model.best_estimator_\n",
    "y_pred = final_model.predict(x_train)\n",
    "for i in range(len(x_train)):\n",
    "    print(count_cc_mp[y_train[i]], count_cc_mp[y_pred[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef030c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "web_train = []\n",
    "website = []\n",
    "for web in cc_mp:\n",
    "    if cc_mp[web]['deg'] == 3:\n",
    "        web_train.append(list(cc_mp[web]['coeff'][0:3]))\n",
    "        website.append(web)\n",
    "print(web_train[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97abcc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_web = final_model.predict(web_train)\n",
    "# pred_prob = final_model.predict_proba(web_train)\n",
    "pred_cc = [count_cc_mp[i] for i in pred_web]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ac9ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,4):\n",
    "    print(count_cc_mp[i], list(pred_web).count(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4598c113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 3D figure\n",
    "fig = plt.figure(figsize=(15,8))\n",
    "\n",
    "# Create a 3D axis\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "count = 0 \n",
    "for web in cc_mp:\n",
    "    if cc_mp[web]['deg'] == 3:\n",
    "        x_points = []\n",
    "        y_points = []\n",
    "        z_points = []\n",
    "        points = cc_mp[web]['coeff']\n",
    "        x_points.append(points[0])\n",
    "        y_points.append(points[1])\n",
    "        z_points.append(points[2])\n",
    "        x_points = np.array(x_points)\n",
    "        y_points = np.array(y_points)\n",
    "        z_points = np.array(z_points)\n",
    "        ax.scatter(x_points, y_points, z_points, label = ,color='k')\n",
    "\n",
    "for cc in cc_degree:\n",
    "    if cc == \"nv\" :\n",
    "        continue\n",
    "    if cc_degree[cc] == 3:\n",
    "        x_points = []\n",
    "        y_points = []\n",
    "        z_points = []\n",
    "        for points in vals_cubicQ[cc][1]:\n",
    "            x_points.append(points[0])\n",
    "            y_points.append(points[1])\n",
    "            z_points.append(points[2])\n",
    "        x_points = np.array(x_points)\n",
    "        y_points = np.array(y_points)\n",
    "        z_points = np.array(z_points)\n",
    "        ax.scatter(x_points, y_points, z_points, label=cc)\n",
    "    plt.legend()\n",
    "\n",
    "        \n",
    "#     plt.legend()\n",
    "# Plot the 3D surface\n",
    "# surf = ax.plot_surface(x, y, z, cmap='viridis')\n",
    "\n",
    "# Add a color bar which maps values to colors\n",
    "# fig.colorbar(surf)\n",
    "\n",
    "# Set axis labels\n",
    "ax.set_xlabel('X-axis')\n",
    "ax.set_ylabel('Y-axis')\n",
    "ax.set_zlabel('Z-axis')\n",
    "\n",
    "# Set a title\n",
    "plt.title('3D Surface Plot')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91cfca1",
   "metadata": {},
   "source": [
    "# Lets test for the Quadratic Case which will be visually discernable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea9517e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEGREE 2\n",
    "x_train = []\n",
    "y_train = []\n",
    "count_cc_mp = {}\n",
    "for cc in cc_degree:\n",
    "    if cc_degree[cc] == 2:\n",
    "        if cc == \"westwood\":\n",
    "            count=1 \n",
    "        elif cc == \"cubicQ\":\n",
    "            count=3\n",
    "        else :\n",
    "            count=2\n",
    "        length = len(vals_cubicQ[cc][1])\n",
    "        count_cc_mp[count]=cc\n",
    "        y_t = [count] * length\n",
    "        x_t = [list(arr) for arr in vals_cubicQ[cc][1]]\n",
    "        x_train += x_t\n",
    "        y_train += y_t\n",
    "        count+=1\n",
    "\n",
    "print(x_train[0:5])\n",
    "print(y_train[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b8fcee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# params_grid = [{'kernel': ['rbf','poly'], 'gamma': [1e-1, 10],\n",
    "#                      'C': [1, 10, 100, 500,1000,2000]},\n",
    "#                     ]\n",
    "# svm_model = GridSearchCV(SVC(), params_grid, cv=5)\n",
    "svm_model = SVC(gamma=100,C=100)\n",
    "fit_model = svm_model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979acec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Best score for training data:', svm_model.best_score_,\"\\n\") \n",
    "\n",
    "# # View the best parameters for the model found using grid search\n",
    "# print('Best C:',svm_model.best_estimator_.C,\"\\n\") \n",
    "# print('Best Kernel:',svm_model.best_estimator_.kernel,\"\\n\")\n",
    "# print('Best Gamma:',svm_model.best_estimator_.gamma,\"\\n\")\n",
    "\n",
    "# final_model = svm_model.best_estimator_\n",
    "# y_pred = final_model.predict(x_train)\n",
    "y_pred = fit_model.predict(x_train)\n",
    "# for i in range(len(x_train)):\n",
    "#     print(count_cc_mp[y_train[i]], count_cc_mp[y_pred[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffccd933",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_meshgrid(x, y, h=.002):\n",
    "    x_min, x_max = x.min() - 0.01, x.max() + 0.01\n",
    "    y_min, y_max = y.min() -0.01, y.max() + 0.01\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    return xx, yy\n",
    "\n",
    "def plot_contours(ax, clf, xx, yy, **params):\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    out = ax.contourf(xx, yy, Z, **params)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1455dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,8))\n",
    "# title for the plots\n",
    "title = ('Decision surface of linear SVC ')\n",
    "# Set-up grid for plotting.\n",
    "X0 = np.array([x_i[0] for x_i in x_train])\n",
    "X1 = np.array([x_i[1] for x_i in x_train])\n",
    "xx, yy = make_meshgrid(X0, X1)\n",
    "\n",
    "plot_contours(ax, fit_model, xx, yy, cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "ax.scatter(X0, X1, c=y_train, cmap=plt.cm.coolwarm, s=20, edgecolors='k')\n",
    "ax.set_ylabel('y label here')\n",
    "ax.set_xlabel('x label here')\n",
    "ax.set_xticks(())\n",
    "ax.set_yticks(())\n",
    "ax.set_title(title)\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e29776",
   "metadata": {},
   "outputs": [],
   "source": [
    "web_train = []\n",
    "website = []\n",
    "for web in cc_mp:\n",
    "    if cc_mp[web]['deg'] == 3:\n",
    "        web_train.append(list(cc_mp[web]['coeff'][0:3]))\n",
    "        website.append(web)\n",
    "print(web_train[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e627fe43",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\n",
    "    \"Nearest Neighbors\",\n",
    "    \"Linear SVM\",\n",
    "    \"RBF SVM\",\n",
    "    \"Gaussian Process\",\n",
    "    \"Decision Tree\",\n",
    "    \"Random Forest\",\n",
    "    \"Neural Net\",\n",
    "    \"AdaBoost\",\n",
    "    \"Naive Bayes\",\n",
    "    \"QDA\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbf7e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec1bd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e1adb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.show_versions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55828ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "data = []\n",
    "for cc in cc_degree:\n",
    "    if cc_degree[cc] == 2:\n",
    "        for points in vals_cubicQ[cc][1]:\n",
    "            data.append(list(points))\n",
    "        labels.append([cc]*len(vals_cubicQ[cc][1]))\n",
    "# print(data)\n",
    "# print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8c3d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2d92cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8901c8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.fit(data)\n",
    "new_data = scaler.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d555623c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(15,8))\n",
    "\n",
    "ax.scatter([i[0] for i in new_data],[i[1] for i in new_data], label = [\"cubic\",\"quad\"], alpha=0.5)\n",
    "plt.legend()\n",
    "\n",
    "#         else :\n",
    "#             ax.scatter(x_points, y_points, label=cc,alpha=0.5)\n",
    "#     plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb941b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(15,8))\n",
    "ax.scatter([i[0] for i in data],[i[1] for i in data],alpha=0.5)\n",
    "plt.legend()\n",
    "\n",
    "#         else :\n",
    "#             ax.scatter(x_points, y_points, label=cc,alpha=0.5)\n",
    "#     plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f3468b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(15,8))\n",
    "for cc in cc_degree:\n",
    "#     if cc == 'cubicQ':\n",
    "#         continue\n",
    "    if cc_degree[cc] == 2:\n",
    "        x_points = []\n",
    "        y_points = []\n",
    "        for points in vals_cubicQ[cc][1]:\n",
    "            x_points.append(points[0])\n",
    "            y_points.append(points[1])\n",
    "        if cc in [\"dctcp\", \"lp\", \"reno\",\"highspeed\"]:\n",
    "            color = \"k\"\n",
    "            \n",
    "            ax.scatter(x_points, y_points, label=cc,alpha=0.5, color=color)\n",
    "        else :\n",
    "            ax.scatter(x_points, y_points, label=cc,alpha=0.5)\n",
    "    plt.legend()\n",
    "\n",
    "#         else :\n",
    "#             ax.scatter(x_points, y_points, label=cc,alpha=0.5)\n",
    "#     plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3219aa4",
   "metadata": {},
   "source": [
    "# Model Making Again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da77b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data \n",
    "ccs = []\n",
    "labels = []\n",
    "data = []\n",
    "for cc in cc_degree:\n",
    "    # choose the degree\n",
    "    if cc_degree[cc] == 2:\n",
    "        for points in vals_cubicQ[cc][1]:\n",
    "            data.append(list(points))\n",
    "        labels += [cc]*len(vals_cubicQ[cc][1])\n",
    "        if cc not in ccs:\n",
    "            ccs.append(cc)\n",
    "print(len(data))\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b647506",
   "metadata": {},
   "outputs": [],
   "source": [
    "web_2 = []\n",
    "for web in web_cc_mp:\n",
    "    if web_cc_mp[web]['deg'] == 2:\n",
    "        x_points = []\n",
    "        y_points = []\n",
    "        points = web_cc_mp[web]['coeff']\n",
    "        web_2.append([points[0], points[1]])\n",
    "web_2 = np.array(web_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f8335a",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_as_count = []\n",
    "labels_to_count = {\n",
    "    \"dctcp\" : 1,\n",
    "    \"reno\" : 1,\n",
    "    \"lp\" : 1,\n",
    "    \"highspeed\" : 1,\n",
    "    \"westwood\" : 2,\n",
    "    \"cubicQ\" : 0 \n",
    "}\n",
    "count_to_labels = {}\n",
    "for l in labels:\n",
    "    labels_as_count.append(labels_to_count[l])\n",
    "    if labels_to_count[l] not in count_to_labels.keys():\n",
    "        count_to_labels[labels_to_count[l]] = l\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b7b0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff_to_web = {}\n",
    "for w in web_cc_mp :\n",
    "    co = web_cc_mp[w]['coeff']\n",
    "    coeff_to_web[(co[0],co[1])] = w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701288f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(labels_as_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b427aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(data)\n",
    "scaled_data = scaler.transform(data)\n",
    "scaled_web_2 = scaler.transform(web_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1646d5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b3cf1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ploting Normal Vs Scaled\n",
    "fig, ax = plt.subplots(1,1, figsize=(15,8))\n",
    "\n",
    "for cc in ccs :\n",
    "    x_ = []\n",
    "    y_ = []\n",
    "    for i in range(len(labels)):\n",
    "        if labels[i] == cc:\n",
    "            x_.append(data[i][0])\n",
    "            y_.append(data[i][1])\n",
    "    ax.scatter(x_,y_,label=cc, alpha=0.5)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(15,8))\n",
    "\n",
    "for cc in ccs :\n",
    "    x_ = []\n",
    "    y_ = []\n",
    "    for i in range(len(labels)):\n",
    "        if labels[i] == cc:\n",
    "            x_.append(scaled_data[i][0])\n",
    "            y_.append(scaled_data[i][1])\n",
    "    ax.scatter(x_,y_,label=cc, alpha=0.5)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f180d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ploting Normal Vs Scaled\n",
    "fig, ax = plt.subplots(1,1, figsize=(15,8))\n",
    "\n",
    "for cc in ccs :\n",
    "    x_ = []\n",
    "    y_ = []\n",
    "    for i in range(len(labels)):\n",
    "        if labels[i] == cc:\n",
    "            x_.append(data[i][0])\n",
    "            y_.append(data[i][1])\n",
    "    ax.scatter(x_,y_,label=cc, alpha=0.5)\n",
    "\n",
    "\n",
    "ax.scatter(web_2[:,0],web_2[:,1],color='k', alpha=0.5)\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(15,8))\n",
    "\n",
    "for cc in ccs :\n",
    "    x_ = []\n",
    "    y_ = []\n",
    "    for i in range(len(labels)):\n",
    "        if labels[i] == cc:\n",
    "            x_.append(scaled_data[i][0])\n",
    "            y_.append(scaled_data[i][1])\n",
    "    ax.scatter(x_,y_,label=cc, alpha=0.5)\n",
    "\n",
    "ax.scatter(scaled_web_2[:,0],scaled_web_2[:,1],color='k', alpha=0.5)\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e69d54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6404c1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = [p[0] for p in scaled_data]\n",
    "y_data = [p[1] for p in scaled_data]\n",
    "x_data, lt = stats.yeojohnson(x_data)\n",
    "y_data, lt = stats.yeojohnson(y_data)\n",
    "yj_data = []\n",
    "for i in range(len(x_data)):\n",
    "    yj_data.append([x_data[i],y_data[i]])\n",
    "yj_data = np.array(yj_data)\n",
    "fig, ax = plt.subplots(1,1, figsize=(15,8))\n",
    "\n",
    "for cc in ccs :\n",
    "    x_ = []\n",
    "    y_ = []\n",
    "    for i in range(len(labels)):\n",
    "        if labels[i] == cc:\n",
    "            x_.append(x_data[i])\n",
    "            y_.append(y_data[i])\n",
    "    ax.scatter(x_,y_,label=cc, alpha=0.5)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a24d9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = [p[0] for p in scaled_data]\n",
    "y_data = [p[1] for p in scaled_data]\n",
    "x_data, lt_x = stats.yeojohnson(x_data)\n",
    "y_data, lt_y = stats.yeojohnson(y_data)\n",
    "yj_data = []\n",
    "for i in range(len(x_data)):\n",
    "    yj_data.append([x_data[i],y_data[i]])\n",
    "yj_data = np.array(yj_data)\n",
    "fig, ax = plt.subplots(1,1, figsize=(15,8))\n",
    "\n",
    "for cc in ccs :\n",
    "    x_ = []\n",
    "    y_ = []\n",
    "    for i in range(len(labels)):\n",
    "        if labels[i] == cc:\n",
    "            x_.append(x_data[i])\n",
    "            y_.append(y_data[i])\n",
    "    ax.scatter(x_,y_,label=cc, alpha=0.5)\n",
    "\n",
    "    \n",
    "web_2_x = [p[0] for p in web_2]\n",
    "web_2_y = [p[1] for p in web_2]\n",
    "\n",
    "x_data = stats.yeojohnson(web_2_x, lmbda=lt_x)\n",
    "y_data = stats.yeojohnson(web_2_y, lmbda=lt_y)\n",
    "yj_web_2 = []\n",
    "for i in range(len(x_data)):\n",
    "    yj_web_2.append([x_data[i],y_data[i]])\n",
    "\n",
    "yj_web_2 = np.array(yj_web_2)\n",
    "\n",
    "ax.scatter(yj_web_2[:,0],yj_web_2[:,1],color='k', alpha=0.5)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854d0a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "names = [\n",
    "    \"Gaussian Process\",\n",
    "    \"RBF SVM\",\n",
    "    \"Naive Bayes\",\n",
    "    \"QDA\",\n",
    "]\n",
    "classifiers = [\n",
    "    GaussianProcessClassifier(random_state=42),\n",
    "    SVC(kernel=\"linear\",random_state=42),\n",
    "    GaussianNB(),\n",
    "    QuadraticDiscriminantAnalysis()\n",
    "]\n",
    "X = np.array(data)\n",
    "y = np.array(labels_as_count)\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "\n",
    "cm_bright = ListedColormap([\"#FF0000\", \"#00FFFF\", \"#0000FF\"])\n",
    "cm = plt.cm.RdYlBu\n",
    "i = 1\n",
    "\n",
    "figure = plt.figure(figsize=(12, 12))\n",
    "ax = plt.axes()\n",
    "\n",
    "ax.scatter(\n",
    "    X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright, edgecolors=\"k\"\n",
    ")\n",
    "\n",
    "for name, clf in zip(names, classifiers):\n",
    "    figure = plt.figure(figsize=(12, 12))\n",
    "    ax = plt.axes()\n",
    "#     ax = plt.subplot(1, len(classifiers) + 1,i)\n",
    "\n",
    "    clf = make_pipeline(StandardScaler(), clf)\n",
    "    clf.fit(X_train, y_train)\n",
    "    score = clf.score(X_test, y_test)\n",
    "    DecisionBoundaryDisplay.from_estimator(\n",
    "        clf, X, cmap=cm, alpha=0.8, ax=ax, eps=0.5\n",
    "    )\n",
    "\n",
    "    # Plot the training points\n",
    "    ax.scatter(\n",
    "        X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright, edgecolors=\"k\"\n",
    "    )\n",
    "    # Plot the testing points\n",
    "    ax.scatter(\n",
    "        X_test[:, 0],\n",
    "        X_test[:, 1],\n",
    "        c=y_test,\n",
    "        cmap=cm_bright,\n",
    "        edgecolors=\"k\",\n",
    "        alpha=0.6,\n",
    "    )\n",
    "#     ax.set_xlim(x_min, x_max)\n",
    "#     ax.set_ylim(y_min, y_max)\n",
    "#     ax.set_xticks(())\n",
    "#     ax.set_yticks(())\n",
    "#     if ds_cnt == 0:\n",
    "    ax.set_title(name)\n",
    "    ax.text(\n",
    "        x_max - 0.3,\n",
    "        y_min + 0.3,\n",
    "        (\"%.2f\" % score).lstrip(\"0\"),\n",
    "        size=15,\n",
    "        horizontalalignment=\"right\",\n",
    "    )\n",
    "    i+=1\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be88ecb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "names = [\n",
    "    \"Gaussian Process\",\n",
    "    \"RBF SVM\",\n",
    "    \"Naive Bayes\",\n",
    "    \"QDA\",\n",
    "]\n",
    "classifiers = [\n",
    "    GaussianProcessClassifier(random_state=42),\n",
    "    SVC(kernel=\"linear\",random_state=42),\n",
    "    GaussianNB(),\n",
    "    QuadraticDiscriminantAnalysis()\n",
    "]\n",
    "X = np.array(scaled_data)\n",
    "y = np.array(labels_as_count)\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "\n",
    "cm_bright = ListedColormap([\"#FF0000\", \"#00FFFF\", \"#0000FF\"])\n",
    "cm = plt.cm.RdYlBu\n",
    "i = 1\n",
    "\n",
    "figure = plt.figure(figsize=(12, 12))\n",
    "ax = plt.axes()\n",
    "\n",
    "ax.scatter(\n",
    "    X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright, edgecolors=\"k\"\n",
    ")\n",
    "\n",
    "for name, clf in zip(names, classifiers):\n",
    "    figure = plt.figure(figsize=(12, 12))\n",
    "    ax = plt.axes()\n",
    "#     ax = plt.subplot(1, len(classifiers) + 1,i)\n",
    "\n",
    "    clf = make_pipeline(StandardScaler(), clf)\n",
    "    clf.fit(X_train, y_train)\n",
    "    score = clf.score(X_test, y_test)\n",
    "    DecisionBoundaryDisplay.from_estimator(\n",
    "        clf, X, cmap=cm, alpha=0.8, ax=ax, eps=0.5\n",
    "    )\n",
    "\n",
    "    # Plot the training points\n",
    "    ax.scatter(\n",
    "        X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright, edgecolors=\"k\"\n",
    "    )\n",
    "    # Plot the testing points\n",
    "    ax.scatter(\n",
    "        X_test[:, 0],\n",
    "        X_test[:, 1],\n",
    "        c=y_test,\n",
    "        cmap=cm_bright,\n",
    "        edgecolors=\"k\",\n",
    "        alpha=0.6,\n",
    "    )\n",
    "#     ax.set_xlim(x_min, x_max)\n",
    "#     ax.set_ylim(y_min, y_max)\n",
    "#     ax.set_xticks(())\n",
    "#     ax.set_yticks(())\n",
    "#     if ds_cnt == 0:\n",
    "    ax.set_title(name)\n",
    "    ax.text(\n",
    "        x_max - 0.3,\n",
    "        y_min + 0.3,\n",
    "        (\"%.2f\" % score).lstrip(\"0\"),\n",
    "        size=15,\n",
    "        horizontalalignment=\"right\",\n",
    "    )\n",
    "    i+=1\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd1b278",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "X = np.array(scaled_data)\n",
    "y = np.array(labels_as_count)\n",
    "web_data = scaled_web_2\n",
    "gm = GaussianMixture(n_components = 3, covariance_type = 'full', random_state=0, )\n",
    "gm.fit(X)\n",
    "\n",
    "plt.figure(figsize=(10,20))\n",
    "for i in range(3):\n",
    "    plt.subplot(3,1,i+1)\n",
    "    plt.scatter(X[:,0],X[:,1],c=gm.predict_proba(X)[:,i],cmap='viridis',marker='x')\n",
    "    plt.scatter(web_data[:,0],web_data[:,1],c=gm.predict_proba(web_data)[:,i],cmap='viridis',marker='x')\n",
    "    plt.colorbar()\n",
    "#     plt.show()\n",
    "prob = gm.predict_proba(web_data)\n",
    "val = []\n",
    "# for i in prob :\n",
    "#     val.append(1-((1-i[0])*(1-i[1])*(1-i[2])))\n",
    "# plt.plot(val)\n",
    "# print(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1348004a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "X = np.array(scaled_data)\n",
    "y = np.array(labels_as_count)\n",
    "web_data = scaled_web_2\n",
    "gm = GaussianMixture(n_components = 3, covariance_type = 'full', random_state=0, )\n",
    "gm.fit(X)\n",
    "\n",
    "plt.figure(figsize=(10,20))\n",
    "for i in range(3):\n",
    "    plt.subplot(3,1,i+1)\n",
    "    plt.scatter(X[:,0],X[:,1],c=gm.predict_proba(X)[:,i],cmap='viridis',marker='x')\n",
    "    plt.scatter(web_data[:,0],web_data[:,1],c=gm.predict_proba(web_data)[:,i],cmap='viridis',marker='x')\n",
    "    plt.colorbar()\n",
    "#     plt.show()\n",
    "prob = gm.predict_proba(web_data)\n",
    "val = []\n",
    "# for i in prob :\n",
    "#     val.append(1-((1-i[0])*(1-i[1])*(1-i[2])))\n",
    "# plt.plot(val)\n",
    "# print(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45159b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ellipses(gmm, ax):\n",
    "    for n, color in enumerate(colors):\n",
    "        if gmm.covariance_type == \"full\":\n",
    "            covariances = gmm.covariances_[n][:2, :2]\n",
    "        elif gmm.covariance_type == \"tied\":\n",
    "            covariances = gmm.covariances_[:2, :2]\n",
    "        elif gmm.covariance_type == \"diag\":\n",
    "            covariances = np.diag(gmm.covariances_[n][:2])\n",
    "        elif gmm.covariance_type == \"spherical\":\n",
    "            covariances = np.eye(gmm.means_.shape[1]) * gmm.covariances_[n]\n",
    "        v, w = np.linalg.eigh(covariances)\n",
    "        u = w[0] / np.linalg.norm(w[0])\n",
    "        angle = np.arctan2(u[1], u[0])\n",
    "        angle = 180 * angle / np.pi  # convert to degrees\n",
    "        v = 2.0 * np.sqrt(2.0) * np.sqrt(v)\n",
    "        ell = mpl.patches.Ellipse(\n",
    "            gmm.means_[n, :2], v[0], v[1], angle=180 + angle, color=color\n",
    "        )\n",
    "        ell.set_clip_box(ax.bbox)\n",
    "        ell.set_alpha(0.5)\n",
    "        ax.add_artist(ell)\n",
    "        ax.set_aspect(\"equal\", \"datalim\")\n",
    "        \n",
    "from sklearn.mixture import GaussianMixture\n",
    "X = np.array(scaled_data)\n",
    "y = np.array(labels_as_count)\n",
    "\n",
    "n_classes = 3\n",
    "\n",
    "estimators = {\n",
    "    cov_type: GaussianMixture(\n",
    "        n_components=n_classes, covariance_type=cov_type, max_iter=100, random_state=0\n",
    "    )\n",
    "    for cov_type in [\"spherical\", \"diag\", \"tied\", \"full\"]\n",
    "}\n",
    "\n",
    "for index, (name, estimator) in enumerate(estimators.items()):\n",
    "    # Since we have class labels for the training data, we can\n",
    "    # initialize the GMM parameters in a supervised manner.\n",
    "    estimator.means_init = np.array(\n",
    "        [X_train[y_train == i].mean(axis=0) for i in range(n_classes)]\n",
    "    )\n",
    "\n",
    "    # Train the other parameters using the EM algorithm.\n",
    "    estimator.fit(X_train)\n",
    "\n",
    "    h = plt.subplot(2, n_estimators // 2, index + 1)\n",
    "    make_ellipses(estimator, h)\n",
    "\n",
    "    for n, color in enumerate(colors):\n",
    "        data = iris.data[iris.target == n]\n",
    "        plt.scatter(\n",
    "            data[:, 0], data[:, 1], s=0.8, color=color, label=iris.target_names[n]\n",
    "        )\n",
    "    # Plot the test data with crosses\n",
    "    for n, color in enumerate(colors):\n",
    "        data = X_test[y_test == n]\n",
    "        plt.scatter(data[:, 0], data[:, 1], marker=\"x\", color=color)\n",
    "\n",
    "    y_train_pred = estimator.predict(X_train)\n",
    "    train_accuracy = np.mean(y_train_pred.ravel() == y_train.ravel()) * 100\n",
    "    plt.text(0.05, 0.9, \"Train accuracy: %.1f\" % train_accuracy, transform=h.transAxes)\n",
    "\n",
    "    y_test_pred = estimator.predict(X_test)\n",
    "    test_accuracy = np.mean(y_test_pred.ravel() == y_test.ravel()) * 100\n",
    "    plt.text(0.05, 0.8, \"Test accuracy: %.1f\" % test_accuracy, transform=h.transAxes)\n",
    "\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "    plt.title(name)\n",
    "\n",
    "plt.legend(scatterpoints=1, loc=\"lower right\", prop=dict(size=12))\n",
    "\n",
    "\n",
    "plt.show()\n",
    "gm = GaussianMixture(n_components = 3, covariance_type = 'full', random_state=0, )\n",
    "gm.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c991d13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "names = [\n",
    "    \"Gaussian Process\",\n",
    "    \"RBF SVM\",\n",
    "    \"Naive Bayes\",\n",
    "    \"QDA\",\n",
    "]\n",
    "classifiers = [\n",
    "    GaussianProcessClassifier(random_state=42),\n",
    "    SVC(kernel=\"linear\",random_state=42),\n",
    "    GaussianNB(),\n",
    "    QuadraticDiscriminantAnalysis()\n",
    "]\n",
    "X = np.array(yj_data)\n",
    "y = np.array(labels_as_count)\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "\n",
    "cm_bright = ListedColormap([\"#FF0000\", \"#00FFFF\", \"#0000FF\"])\n",
    "cm = plt.cm.RdYlBu\n",
    "i = 1\n",
    "\n",
    "figure = plt.figure(figsize=(12, 12))\n",
    "ax = plt.axes()\n",
    "\n",
    "ax.scatter(\n",
    "    X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright, edgecolors=\"k\"\n",
    ")\n",
    "\n",
    "for name, clf in zip(names, classifiers):\n",
    "    figure = plt.figure(figsize=(12, 12))\n",
    "    ax = plt.axes()\n",
    "#     ax = plt.subplot(1, len(classifiers) + 1,i)\n",
    "\n",
    "    clf = make_pipeline(StandardScaler(), clf)\n",
    "    clf.fit(X_train, y_train)\n",
    "    score = clf.score(X_test, y_test)\n",
    "    DecisionBoundaryDisplay.from_estimator(\n",
    "        clf, X, cmap=cm, alpha=0.8, ax=ax, eps=0.5\n",
    "    )\n",
    "\n",
    "    # Plot the training points\n",
    "    ax.scatter(\n",
    "        X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright, edgecolors=\"k\"\n",
    "    )\n",
    "    # Plot the testing points\n",
    "    ax.scatter(\n",
    "        X_test[:, 0],\n",
    "        X_test[:, 1],\n",
    "        c=y_test,\n",
    "        cmap=cm_bright,\n",
    "        edgecolors=\"k\",\n",
    "        alpha=0.6,\n",
    "    )\n",
    "#     ax.set_xlim(x_min, x_max)\n",
    "#     ax.set_ylim(y_min, y_max)\n",
    "#     ax.set_xticks(())\n",
    "#     ax.set_yticks(())\n",
    "#     if ds_cnt == 0:\n",
    "    ax.set_title(name)\n",
    "    ax.text(\n",
    "        x_max - 0.3,\n",
    "        y_min + 0.3,\n",
    "        (\"%.2f\" % score).lstrip(\"0\"),\n",
    "        size=15,\n",
    "        horizontalalignment=\"right\",\n",
    "    )\n",
    "    i+=1\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fe99ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "web_2 = []\n",
    "for web in web_cc_mp:\n",
    "    if web_cc_mp[web]['deg'] == 2:\n",
    "        x_points = []\n",
    "        y_points = []\n",
    "        points = web_cc_mp[web]['coeff']\n",
    "        web_2.append([points[0], points[1]])\n",
    "web_2 = np.array(web_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62054dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_pos_def(A):\n",
    "    if np.allclose(A, A.T):\n",
    "        try:\n",
    "            np.linalg.cholesky(A)\n",
    "            return True\n",
    "        except np.linalg.LinAlgError:\n",
    "            return False\n",
    "    else:\n",
    "        return False \n",
    "\n",
    "from scipy.stats import chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190d5648",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_to_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9737ce7a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "use_vals = []\n",
    "\n",
    "for i in count_to_labels.keys(): \n",
    "    if i in [1,2]:\n",
    "        continue\n",
    "    print(i, count_to_labels[i])\n",
    "    count = i\n",
    "    X = []\n",
    "    y = []\n",
    "    for n in range(len(data)):\n",
    "        if labels_as_count[n] == i:\n",
    "            X.append(data[n])\n",
    "            y.append(labels_as_count[n])\n",
    "    x = np.array(X)\n",
    "    y = np.array(y)\n",
    "    web_data_use = np.array(web_2)\n",
    "    x_mean = x.mean(axis=0)\n",
    "    x_covar = np.cov(x, rowvar=False)\n",
    "    x_covar_inv = np.linalg.inv(x_covar)\n",
    "    x_diff = web_data_use - x_mean\n",
    "    md = []\n",
    "    for i in range(len(x_diff)):\n",
    "        md.append(x_diff[i].dot(x_covar_inv).dot(x_diff[i]))\n",
    "    \n",
    "    p = 1 - chi2.cdf(md, 2)\n",
    "    ind = []\n",
    "    thresh = 1000\n",
    "    \n",
    "    for i in range(len(md)):\n",
    "        if md[i] < thresh:\n",
    "            ind.append(i)\n",
    "#             print(black)\n",
    "            print(i,md[i],md[i]<thresh,\"-----\")\n",
    "            use_vals.append(i)\n",
    "        else :\n",
    "#             print(red)\n",
    "            print(i,md[i],md[i]<thresh)\n",
    "\n",
    "fig = plt.figure(figsize = (10,10))\n",
    "plt.scatter(np.array(data)[:,0], np.array(data)[:,1])\n",
    "new_data = np.array([web_2[i] for i in set(use_vals)])\n",
    "plt.scatter(web_2[:,0], web_2[:,1])\n",
    "\n",
    "            \n",
    "fig = plt.figure(figsize = (10,10))\n",
    "plt.scatter(np.array(data)[:,0], np.array(data)[:,1])\n",
    "new_data = np.array([web_2[i] for i in set(use_vals)])\n",
    "plt.scatter(new_data[:,0], new_data[:,1])\n",
    "# for cc in count_to_labels.keys():   \n",
    "#     temp_check = []\n",
    "#     for i in range(1,151):\n",
    "#         temp_check.append(cc+\"-\"+str(i)+\"-0-50-200-2-aws-88-60\")\n",
    "#     cc_mp = get_feature_degree(temp_check, ss=225,p=\"n\",ft_thresh=1,max_deg=cc_degree[cc])\n",
    "#     x = []\n",
    "#     for key in cc_mp.keys():\n",
    "#         x.append(cc_mp[key][0]['coeff'])\n",
    "#     x = np.array(x)\n",
    "#     x_mean = x.mean(axis=0)\n",
    "#     if cc_degree[cc] == 1:\n",
    "#         x_covar = np.array([[np.cov(x, rowvar=False)]])\n",
    "#     else :\n",
    "#         x_covar = np.cov(x, rowvar=False)\n",
    "#     x_covar_inv = np.linalg.inv(x_covar)\n",
    "#     print(x_mean)\n",
    "#     print(x_covar)\n",
    "#     print(is_pos_def(x_covar))\n",
    "#     print(is_pos_def(x_covar_inv))\n",
    "#     x_diff = x - x_mean\n",
    "#     md = []\n",
    "#     for i in range(len(x_diff)):\n",
    "#         md.append(x_diff[i].dot(x_covar_inv).dot(x_diff[i]))\n",
    "#     p = 1 - chi2.cdf(md, 4)\n",
    "#     ind = []\n",
    "#     thresh = 0.05\n",
    "#     for i in range(len(md)):\n",
    "#         if p[i] < thresh:\n",
    "#             print(red)\n",
    "#             ind.append(i)\n",
    "#         print(i,md[i],p[i],p[i]<thresh)\n",
    "#         print(black)\n",
    "#     dont_use_ind[cc] = ind\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9e43f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e136c43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_vals = []\n",
    "means = []\n",
    "\n",
    "for i in count_to_labels.keys(): \n",
    "    if i in [0,1]:\n",
    "        continue\n",
    "#     print(i, count_to_labels[i])\n",
    "    count = i\n",
    "    X = []\n",
    "    y = []\n",
    "    for n in range(len(scaled_data)):\n",
    "        if labels_as_count[n] == i:\n",
    "            X.append(scaled_data[n])\n",
    "    x = np.array(X)\n",
    "    web_data_use = np.array(scaled_web_2)\n",
    "    x_mean = x.mean(axis=0)\n",
    "    means.append(x_mean)\n",
    "#     x_covar = np.cov(x, rowvar=False)\n",
    "#     x_covar_inv = np.linalg.inv(x_covar)\n",
    "    x_diff = (web_data_use - x_mean)**2\n",
    "    md = []\n",
    "    for row in x_diff:\n",
    "        md.append(np.sqrt(np.sum(np.dot(np.array([0.66,0.34]),row))))\n",
    "    thresh = 10\n",
    "    \n",
    "    for i in range(len(md)):\n",
    "        if md[i] < thresh:\n",
    "#             print(black)\n",
    "            print(i,md[i],md[i]<thresh,\"-----\")\n",
    "            use_vals.append(i)\n",
    "        else :\n",
    "#             print(red)\n",
    "            print(i,md[i],md[i]<thresh)\n",
    "\n",
    "new_data = np.array([scaled_web_2[i] for i in set(use_vals)])\n",
    "\n",
    "fig = plt.figure(figsize = (10,10))\n",
    "plt.scatter(scaled_data[:,0], scaled_data[:,1])\n",
    "plt.scatter(scaled_web_2[:,0], scaled_web_2[:,1])\n",
    "plt.scatter(new_data[:,0], new_data[:,1])\n",
    "plt.scatter(np.array(means)[:,0], np.array(means)[:,1])\n",
    "\n",
    "# for cc in count_to_labels.keys():   \n",
    "#     temp_check = []\n",
    "#     for i in range(1,151):\n",
    "#         temp_check.append(cc+\"-\"+str(i)+\"-0-50-200-2-aws-88-60\")\n",
    "#     cc_mp = get_feature_degree(temp_check, ss=225,p=\"n\",ft_thresh=1,max_deg=cc_degree[cc])\n",
    "#     x = []\n",
    "#     for key in cc_mp.keys():\n",
    "#         x.append(cc_mp[key][0]['coeff'])\n",
    "#     x = np.array(x)\n",
    "#     x_mean = x.mean(axis=0)\n",
    "#     if cc_degree[cc] == 1:\n",
    "#         x_covar = np.array([[np.cov(x, rowvar=False)]])\n",
    "#     else :\n",
    "#         x_covar = np.cov(x, rowvar=False)\n",
    "#     x_covar_inv = np.linalg.inv(x_covar)\n",
    "#     print(x_mean)\n",
    "#     print(x_covar)\n",
    "#     print(is_pos_def(x_covar))\n",
    "#     print(is_pos_def(x_covar_inv))\n",
    "#     x_diff = x - x_mean\n",
    "#     md = []\n",
    "#     for i in range(len(x_diff)):\n",
    "#         md.append(x_diff[i].dot(x_covar_inv).dot(x_diff[i]))\n",
    "#     p = 1 - chi2.cdf(md, 4)\n",
    "#     ind = []\n",
    "#     thresh = 0.05\n",
    "#     for i in range(len(md)):\n",
    "#         if p[i] < thresh:\n",
    "#             print(red)\n",
    "#             ind.append(i)\n",
    "#         print(i,md[i],p[i],p[i]<thresh)\n",
    "#         print(black)\n",
    "#     dont_use_ind[cc] = ind\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb98a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.model_selection import train_test_split\n",
    "curr_data = np.array(data)\n",
    "curr_labels = np.array(labels_as_count)\n",
    "web_2_data = web_2\n",
    "not_outliers = {}\n",
    "for i in count_to_labels.keys():    \n",
    "    print(i, count_to_labels[i])\n",
    "    count = i\n",
    "    X = []\n",
    "    y = []\n",
    "    for n in range(len(curr_data)):\n",
    "        if labels_as_count[n] ==  i:\n",
    "            X.append(curr_data[n])\n",
    "            y.append(labels_as_count[n])\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    web_data_use = web_2_data\n",
    "    ss = StandardScaler()\n",
    "    X = ss.fit_transform(X)\n",
    "    web_2_data_use = ss.transform(web_2_data)\n",
    "    one_class_svm = OneClassSVM(nu=0.001, kernel = 'sigmoid', gamma = 1).fit(X)\n",
    "    prediction = one_class_svm.predict(web_2_data_use)\n",
    "    prediction = [1 if i==-1 else 0 for i in prediction]\n",
    "    temp = []\n",
    "    for i in range(len(web_2_data)):\n",
    "        if prediction[i] == 0:\n",
    "            temp.append(web_2_data[i])\n",
    "    not_outliers[count] = temp\n",
    "\n",
    "figure = plt.figure(figsize=(10, 10))\n",
    "ax = plt.axes()\n",
    "\n",
    "cm_bright = ListedColormap([\"#FF0000\",\"#FFFF00\",\"#0000FF\"])\n",
    "cm = plt.cm.RdYlBu\n",
    "\n",
    "ax.scatter(\n",
    "    curr_data[:, 0], curr_data[:, 1], c=curr_labels, cmap=cm_bright, edgecolors=\"k\"\n",
    ")\n",
    "\n",
    "ax.scatter(\n",
    "    web_2_data[:, 0], web_2_data[:, 1], color=\"k\", edgecolors=\"k\"\n",
    ")\n",
    "\n",
    "for i in not_outliers:\n",
    "    if len(not_outliers[i])>0:\n",
    "        new_data = np.array(not_outliers[i])\n",
    "        ax.scatter(\n",
    "            new_data[:, 0], new_data[:, 1], cmap=cm\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af35c1d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.model_selection import train_test_split\n",
    "curr_data = np.array(data)\n",
    "curr_labels = np.array(labels_as_count)\n",
    "web_2_data = web_2\n",
    "not_outliers = {}\n",
    "for i in count_to_labels.keys():    \n",
    "    print(i, count_to_labels[i])\n",
    "    count = i\n",
    "    X = []\n",
    "    y = []\n",
    "    for n in range(len(curr_data)):\n",
    "        if labels_as_count[n] ==  i:\n",
    "            X.append(curr_data[n])\n",
    "            y.append(labels_as_count[n])\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    web_data_use = web_2_data\n",
    "    ss = StandardScaler()\n",
    "    X = ss.fit_transform(X)\n",
    "    web_2_data_use = ss.transform(web_2_data)\n",
    "    one_class_svm = OneClassSVM(nu=0.001, kernel = 'sigmoid', gamma = 1).fit(X)\n",
    "    prediction = one_class_svm.predict(web_2_data_use)\n",
    "    prediction = [1 if i==-1 else 0 for i in prediction]\n",
    "    temp = []\n",
    "    for i in range(len(web_2_data)):\n",
    "        if prediction[i] == 0:\n",
    "            temp.append(web_2_data[i])\n",
    "    not_outliers[count] = temp\n",
    "\n",
    "figure = plt.figure(figsize=(10, 10))\n",
    "ax = plt.axes()\n",
    "\n",
    "cm_bright = ListedColormap([\"#FF0000\",\"#FFFF00\",\"#0000FF\"])\n",
    "cm = plt.cm.RdYlBu\n",
    "\n",
    "ax.scatter(\n",
    "    curr_data[:, 0], curr_data[:, 1], c=curr_labels, cmap=cm_bright, edgecolors=\"k\"\n",
    ")\n",
    "\n",
    "ax.scatter(\n",
    "    web_2_data[:, 0], web_2_data[:, 1], color=\"k\", edgecolors=\"k\"\n",
    ")\n",
    "\n",
    "for i in not_outliers:\n",
    "    if len(not_outliers[i])>0:\n",
    "        new_data = np.array(not_outliers[i])\n",
    "        ax.scatter(\n",
    "            new_data[:, 0], new_data[:, 1], cmap=cm\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b394bad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.covariance import EllipticEnvelope\n",
    "from sklearn.model_selection import train_test_split\n",
    "curr_data = np.array(data)\n",
    "curr_labels = np.array(labels_as_count)\n",
    "web_2_data = web_2\n",
    "not_outliers = {}\n",
    "for i in count_to_labels.keys():    \n",
    "    print(i, count_to_labels[i])\n",
    "    count = i\n",
    "    X = []\n",
    "    y = []\n",
    "    for n in range(len(curr_data)):\n",
    "        if labels_as_count[n] ==  i:\n",
    "            X.append(curr_data[n])\n",
    "            y.append(labels_as_count[n])\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    ss = StandardScaler()\n",
    "    X = ss.fit_transform(X)\n",
    "    web_2_data_use = ss.transform(web_2_data)\n",
    "    model = EllipticEnvelope(contamination=0.001,random_state=42).fit(X)\n",
    "    prediction = model.predict(web_2_data_use)\n",
    "    prediction = [1 if i==-1 else 0 for i in prediction]\n",
    "    temp = []\n",
    "    for i in range(len(web_2_data_use)):\n",
    "        if prediction[i] == 0:\n",
    "            temp.append(web_2_data_use[i])\n",
    "    not_outliers[count] = temp\n",
    "\n",
    "figure = plt.figure(figsize=(10, 10))\n",
    "ax = plt.axes()\n",
    "\n",
    "cm_bright = ListedColormap([\"#FF0000\",\"#FFFF00\",\"#0000FF\"])\n",
    "cm = plt.cm.RdYlBu\n",
    "\n",
    "ax.scatter(\n",
    "    curr_data[:, 0], curr_data[:, 1], c=curr_labels, cmap=cm_bright, edgecolors=\"k\"\n",
    ")\n",
    "\n",
    "ax.scatter(\n",
    "    web_2_data[:, 0], web_2_data[:, 1], color=\"k\", edgecolors=\"k\"\n",
    ")\n",
    "\n",
    "for i in not_outliers:\n",
    "    if len(not_outliers[i])>0:\n",
    "        print(i)\n",
    "        new_data = np.array(not_outliers[i])\n",
    "        color = 'r'\n",
    "        if i == 1:\n",
    "            color = 'y'\n",
    "        elif i == 2:\n",
    "            color = 'b'\n",
    "        ax.scatter(\n",
    "            new_data[:, 0], new_data[:, 1], c=color\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b65c781",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.model_selection import train_test_split\n",
    "curr_data = np.array(scaled_data)\n",
    "curr_labels = np.array(labels_as_count)\n",
    "web_2_data = scaled_web_2\n",
    "not_outliers = {}\n",
    "for i in count_to_labels.keys():    \n",
    "    print(i, count_to_labels[i])\n",
    "    count = i\n",
    "    X = []\n",
    "    y = []\n",
    "    for n in range(len(curr_data)):\n",
    "        if labels_as_count[n] ==  i:\n",
    "            X.append(curr_data[n])\n",
    "            y.append(labels_as_count[n])\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    one_class_svm = OneClassSVM(nu=0.001, kernel = 'rbf', gamma = 100).fit(X)\n",
    "    prediction = one_class_svm.predict(web_2_data)\n",
    "    prediction = [1 if i==-1 else 0 for i in prediction]\n",
    "    temp = []\n",
    "    for i in range(len(web_2_data)):\n",
    "        if prediction[i] == 0:\n",
    "            temp.append(web_2_data[i])\n",
    "    not_outliers[count] = temp\n",
    "\n",
    "figure = plt.figure(figsize=(10, 10))\n",
    "ax = plt.axes()\n",
    "\n",
    "cm_bright = ListedColormap([\"#FF0000\",\"#FFFF00\",\"#0000FF\"])\n",
    "cm = plt.cm.RdYlBu\n",
    "\n",
    "ax.scatter(\n",
    "    curr_data[:, 0], curr_data[:, 1], c=curr_labels, cmap=cm_bright, edgecolors=\"k\"\n",
    ")\n",
    "\n",
    "ax.scatter(\n",
    "    web_2_data[:, 0], web_2_data[:, 1], color=\"k\", edgecolors=\"k\"\n",
    ")\n",
    "i_to_c = {\n",
    "    0 : 'r',\n",
    "    1 : 'y',\n",
    "    2 : 'b'\n",
    "}\n",
    "for i in not_outliers:\n",
    "    if len(not_outliers[i])>0:\n",
    "        print(i)\n",
    "        new_data = np.array(not_outliers[i])\n",
    "        ax.scatter(\n",
    "            new_data[:, 0], new_data[:, 1], c=i_to_c[i]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d0deb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.model_selection import train_test_split\n",
    "curr_data = np.array(yj_data)\n",
    "curr_labels = np.array(labels_as_count)\n",
    "web_2_data = yj_web_2\n",
    "not_outliers = {}\n",
    "for i in count_to_labels.keys():    \n",
    "    print(i, count_to_labels[i])\n",
    "    count = i\n",
    "    X = []\n",
    "    y = []\n",
    "    for n in range(len(curr_data)):\n",
    "        if labels_as_count[n] ==  i:\n",
    "            X.append(curr_data[n])\n",
    "            y.append(labels_as_count[n])\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    one_class_svm = OneClassSVM(nu=0.001, kernel = 'rbf', gamma = 1000).fit(X)\n",
    "    prediction = one_class_svm.predict(web_2_data)\n",
    "    prediction = [1 if i==-1 else 0 for i in prediction]\n",
    "    temp = []\n",
    "    for i in range(len(web_2_data)):\n",
    "        if prediction[i] == 0:\n",
    "            temp.append(web_2_data[i])\n",
    "    not_outliers[count] = temp\n",
    "\n",
    "figure = plt.figure(figsize=(10, 10))\n",
    "ax = plt.axes()\n",
    "\n",
    "cm_bright = ListedColormap([\"#FF0000\",\"#FFFF00\",\"#0000FF\"])\n",
    "cm = plt.cm.RdYlBu\n",
    "\n",
    "ax.scatter(\n",
    "    curr_data[:, 0], curr_data[:, 1], c=curr_labels, cmap=cm_bright, edgecolors=\"k\"\n",
    ")\n",
    "\n",
    "ax.scatter(\n",
    "    web_2_data[:, 0], web_2_data[:, 1], color=\"k\", edgecolors=\"k\"\n",
    ")\n",
    "\n",
    "for i in not_outliers:\n",
    "    if len(not_outliers[i])>0:\n",
    "        print(count_to_labels[i])\n",
    "        new_data = np.array(not_outliers[i])\n",
    "        print(len(new_data))\n",
    "        ax.scatter(\n",
    "            new_data[:, 0], new_data[:, 1], c=[i]*len(new_data), cmap=cm_bright\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc5a52c",
   "metadata": {},
   "source": [
    "## Instead of this lets use a probability threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870e6e71",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "names = [\n",
    "    \"Gaussian Process\",\n",
    "    \"RBF SVM\",\n",
    "    \"Naive Bayes\",\n",
    "    \"QDA\",\n",
    "]\n",
    "classifiers = [\n",
    "    GaussianProcessClassifier(random_state=42),\n",
    "    SVC(kernel=\"linear\",random_state=42,probability=1),\n",
    "    GaussianNB(),\n",
    "    QuadraticDiscriminantAnalysis()\n",
    "]\n",
    "X = np.array(scaled_data)\n",
    "y = np.array(labels_as_count)\n",
    "data_web_2 = np.array(scaled_web_2)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "\n",
    "cm_bright = ListedColormap([\"#FF0000\", \"#FFFF00\", \"#0000FF\"])\n",
    "cm = plt.cm.RdYlBu\n",
    "i = 1\n",
    "\n",
    "figure = plt.figure(figsize=(8, 8))\n",
    "ax = plt.axes()\n",
    "\n",
    "ax.scatter(\n",
    "    X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright, edgecolors=\"k\"\n",
    ")\n",
    "ax.scatter(\n",
    "    data_web_2[:,0], data_web_2[:,1], color='k'\n",
    ")\n",
    "\n",
    "for name, clf in zip(names, classifiers):\n",
    "    figure = plt.figure(figsize=(8, 8))\n",
    "    ax = plt.axes()\n",
    "#     ax = plt.subplot(1, len(classifiers) + 1,i)\n",
    "\n",
    "    clf = make_pipeline(StandardScaler(), clf)\n",
    "    clf.fit(X_train, y_train)\n",
    "    score = clf.score(X_test, y_test)\n",
    "#     DecisionBoundaryDisplay.from_estimator(\n",
    "#         clf, X, cmap=cm, alpha=0.8, ax=ax, eps=0.5\n",
    "#     )\n",
    "#     DecisionBoundaryDisplay.from_estimator(\n",
    "#         clf, scaled_web_2, cmap=cm, alpha=0.8, ax=ax, eps=0.5\n",
    "#     )\n",
    "    \n",
    "    results = clf.predict(data_web_2)\n",
    "    results_proba = clf.predict_proba(data_web_2)\n",
    "    # Plot the training points\n",
    "    ax.scatter(\n",
    "        X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright, edgecolors=\"k\"\n",
    "    )\n",
    "    # Plot the testing points\n",
    "    ax.scatter(\n",
    "        X_test[:, 0],\n",
    "        X_test[:, 1],\n",
    "        c=y_test,\n",
    "        cmap=cm_bright,\n",
    "        edgecolors=\"k\",\n",
    "        alpha=0.6,\n",
    "    )\n",
    "#     ax.set_xlim(x_min, x_max)\n",
    "    ax.scatter(\n",
    "        data_web_2[:, 0],\n",
    "        data_web_2[:, 1],\n",
    "        c=results,\n",
    "        cmap=cm_bright,\n",
    "        edgecolors=\"k\",\n",
    "        alpha=0.6,\n",
    "    )\n",
    "#     ax.set_ylim(y_min, y_max)\n",
    "#     ax.set_xticks(())\n",
    "#     ax.set_yticks(())\n",
    "#     if ds_cnt == 0:\n",
    "    ax.set_title(name)\n",
    "    ax.text(\n",
    "        x_max - 0.3,\n",
    "        y_min + 0.3,\n",
    "        (\"%.2f\" % score).lstrip(\"0\"),\n",
    "        size=15,\n",
    "        horizontalalignment=\"right\",\n",
    "    )\n",
    "    i+=1\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3129fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "    \n",
    "names = [\n",
    "    \"Gaussian Process\",\n",
    "    \"RBF SVM\",\n",
    "    \"Naive Bayes\",\n",
    "    \"QDA\",\n",
    "    \"Decision Tree\",\n",
    "    \"Random Forest\",\n",
    "]\n",
    "classifiers = [\n",
    "    GaussianProcessClassifier(random_state=42),\n",
    "    SVC(kernel=\"linear\",random_state=42,probability=1),\n",
    "    GaussianNB(),\n",
    "    QuadraticDiscriminantAnalysis(),\n",
    "    DecisionTreeClassifier(max_depth=5, random_state=42),\n",
    "    RandomForestClassifier(\n",
    "        max_depth=5, n_estimators=10, max_features=1, random_state=42\n",
    "    ),\n",
    "]\n",
    "X = np.array(yj_data)\n",
    "y = np.array(labels_as_count)\n",
    "data_web_2 = np.array(yj_web_2)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "\n",
    "cm_bright = ListedColormap([\"#FF0000\", \"#FFFF00\", \"#0000FF\"])\n",
    "cm = plt.cm.RdYlBu\n",
    "i = 1\n",
    "\n",
    "figure = plt.figure(figsize=(8, 8))\n",
    "ax = plt.axes()\n",
    "\n",
    "ax.scatter(\n",
    "    X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright, edgecolors=\"k\"\n",
    ")\n",
    "ax.scatter(\n",
    "    data_web_2[:,0], data_web_2[:,1], color='k'\n",
    ")\n",
    "\n",
    "for name, clf in zip(names, classifiers):\n",
    "    figure = plt.figure(figsize=(8, 8))\n",
    "    ax = plt.axes()\n",
    "#     ax = plt.subplot(1, len(classifiers) + 1,i)\n",
    "\n",
    "#     clf = make_pipeline(StandardScaler(), clf)\n",
    "    clf.fit(X_train, y_train)\n",
    "    score = clf.score(X_test, y_test)\n",
    "    DecisionBoundaryDisplay.from_estimator(\n",
    "        clf, X, cmap=cm, alpha=0.8, ax=ax, eps=0.5\n",
    "    )\n",
    "#     DecisionBoundaryDisplay.from_estimator(\n",
    "#         clf, data_web_2, cmap=cm, alpha=0.8, ax=ax, eps=0.5\n",
    "#     )\n",
    "    \n",
    "    results = clf.predict(data_web_2)\n",
    "    results_proba = clf.predict_proba(data_web_2)\n",
    "    # Plot the training points\n",
    "    ax.scatter(\n",
    "        X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright, edgecolors=\"k\"\n",
    "    )\n",
    "    # Plot the testing points\n",
    "    ax.scatter(\n",
    "        X_test[:, 0],\n",
    "        X_test[:, 1],\n",
    "        c=y_test,\n",
    "        cmap=cm_bright,\n",
    "        edgecolors=\"k\",\n",
    "        alpha=0.6,\n",
    "    )\n",
    "#     ax.set_xlim(x_min, x_max)\n",
    "    ax.scatter(\n",
    "        data_web_2[:, 0],\n",
    "        data_web_2[:, 1],\n",
    "        c=results,\n",
    "        cmap=cm_bright,\n",
    "        edgecolors=\"k\",\n",
    "        alpha=0.6,\n",
    "    )\n",
    "#     ax.set_ylim(y_min, y_max)\n",
    "#     ax.set_xticks(())\n",
    "#     ax.set_yticks(())\n",
    "#     if ds_cnt == 0:\n",
    "    ax.set_title(name)\n",
    "    ax.text(\n",
    "        x_max - 0.3,\n",
    "        y_min + 0.3,\n",
    "        (\"%.2f\" % score).lstrip(\"0\"),\n",
    "        size=15,\n",
    "        horizontalalignment=\"right\",\n",
    "    )\n",
    "    i+=1\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85d4e82",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "    \n",
    "names = [\n",
    "    \"Decision Tree\",\n",
    "#     \"Random Forest\",\n",
    "]\n",
    "classifiers = [\n",
    "#     GaussianProcessClassifier(random_state=42),\n",
    "#     SVC(kernel=\"linear\",random_state=42,probability=1),\n",
    "#     GaussianNB(),\n",
    "#     QuadraticDiscriminantAnalysis(),\n",
    "    DecisionTreeClassifier(max_depth=10, random_state=42)\n",
    "#     RandomForestClassifier(\n",
    "#         max_depth=10, n_estimators=100, random_state=42\n",
    "#     ),\n",
    "]\n",
    "X = np.array(data)\n",
    "y = np.array(labels_as_count)\n",
    "data_web_2 = np.array(web_2)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "\n",
    "l_to_c = {\n",
    "    'cubicQ' : 'red',\n",
    "    'dctcp' : 'yellow',\n",
    "    'westwood' : 'blue',\n",
    "}\n",
    "cm = ListedColormap(['red','yellow','blue'])\n",
    "cm_bright = ListedColormap([\"#FF0000\", \"#FFFF00\", \"#0000FF\"])\n",
    "i = 1\n",
    "\n",
    "figure = plt.figure(figsize=(8, 8))\n",
    "ax = plt.axes()\n",
    "\n",
    "ax.scatter(\n",
    "    X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm, edgecolors=\"k\"\n",
    ")\n",
    "ax.scatter(\n",
    "    data_web_2[:,0], data_web_2[:,1], color='k'\n",
    ")\n",
    "\n",
    "for name, clf in zip(names, classifiers):\n",
    "    figure = plt.figure(figsize=(8, 8))\n",
    "    ax = plt.axes()\n",
    "#     ax = plt.subplot(1, len(classifiers) + 1,i)\n",
    "\n",
    "#     clf = make_pipeline(StandardScaler(), clf)\n",
    "    clf.fit(X_train, y_train)\n",
    "    score = clf.score(X_test, y_test)\n",
    "    DecisionBoundaryDisplay.from_estimator(\n",
    "        clf, X, cmap=cm_bright, alpha=0.8, ax=ax, eps=0.5\n",
    "    )\n",
    "#     DecisionBoundaryDisplay.from_estimator(\n",
    "#         clf, data_web_2, cmap=cm_bright, alpha=0.8, ax=ax, eps=0.5\n",
    "#     )\n",
    "    \n",
    "    results = clf.predict(data_web_2)\n",
    "    print(results)\n",
    "    results_proba = clf.predict_proba(data_web_2)\n",
    "    # Plot the training points\n",
    "    ax.scatter(\n",
    "        X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm, edgecolors=\"k\"\n",
    "    )\n",
    "    # Plot the testing points\n",
    "    ax.scatter(\n",
    "        X_test[:, 0],\n",
    "        X_test[:, 1],\n",
    "        c=y_test,\n",
    "        cmap=cm,\n",
    "        edgecolors=\"k\",\n",
    "        alpha=0.6,\n",
    "    )\n",
    "#     ax.set_xlim(x_min, x_max)\n",
    "    new_c = []\n",
    "    if len(set(results)) == 2 :\n",
    "        new_c = ['yellow', 'blue']\n",
    "        cm = ListedColormap(new_c)\n",
    "    ax.scatter(\n",
    "        data_web_2[:, 0],\n",
    "        data_web_2[:, 1],\n",
    "        c=results,\n",
    "        cmap=cm,\n",
    "        edgecolors=\"k\",\n",
    "        alpha=0.6,\n",
    "    )\n",
    "#     ax.set_ylim(y_min, y_max)\n",
    "#     ax.set_xticks(())\n",
    "#     ax.set_yticks(())\n",
    "#     if ds_cnt == 0:\n",
    "    ax.set_title(name)\n",
    "    ax.text(\n",
    "        x_max - 0.3,\n",
    "        y_min + 0.3,\n",
    "        (\"%.2f\" % score).lstrip(\"0\"),\n",
    "        size=15,\n",
    "        horizontalalignment=\"right\",\n",
    "    )\n",
    "    i+=1\n",
    "plt.show()\n",
    "\n",
    "for i in set(results):\n",
    "    print(\"Printing for \", count_to_labels[i])\n",
    "    for i in range(len(data_web_2)):\n",
    "        tcoeff = (data_web_2[i][0],data_web_2[i][1])\n",
    "        web = coeff_to_web[tcoeff]\n",
    "        tdata = web_mp[web]['data']\n",
    "        ttime = web_mp[web]['time']\n",
    "        plt.plot(ttime,tdata)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f264e021",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.covariance import EllipticEnvelope\n",
    "\n",
    "model = EllipticEnvelope(contamination=0.01, )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bfd464",
   "metadata": {},
   "source": [
    "# Clustering using DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff137024",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = scaled_data\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "db = DBSCAN(eps=0.3, min_samples=5).fit(X)\n",
    "labels = db.labels_\n",
    "\n",
    "# Number of clusters in labels, ignoring noise if present.\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_noise_ = list(labels).count(-1)\n",
    "\n",
    "print(\"Estimated number of clusters: %d\" % n_clusters_)\n",
    "print(\"Estimated number of noise points: %d\" % n_noise_)\n",
    "\n",
    "unique_labels = set(labels)\n",
    "figure = plt.figure(figsize=(8, 8))\n",
    "ax = plt.axes()\n",
    "for u in unique_labels:\n",
    "    x = []\n",
    "    for i in range(len(X)):\n",
    "        if labels[i] == u:\n",
    "            x.append(X[i])\n",
    "    x = np.array(x)\n",
    "    ax.scatter(\n",
    "        x[:, 0],\n",
    "        x[:, 1],\n",
    "        alpha=0.6,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e776ab",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.concatenate((data, web_2),axis=0)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "db = DBSCAN(eps=0.05, min_samples=5).fit(X)\n",
    "labels = db.labels_\n",
    "\n",
    "# Number of clusters in labels, ignoring noise if present.\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_noise_ = list(labels).count(-1)\n",
    "\n",
    "print(\"Estimated number of clusters: %d\" % n_clusters_)\n",
    "print(\"Estimated number of noise points: %d\" % n_noise_)\n",
    "\n",
    "unique_labels = set(labels)\n",
    "figure = plt.figure(figsize=(8, 8))\n",
    "ax = plt.axes()\n",
    "for u in unique_labels:\n",
    "    x = []\n",
    "    for i in range(len(X)):\n",
    "        if labels[i] == u:\n",
    "            x.append(X[i])\n",
    "    x = np.array(x)\n",
    "    ax.scatter(\n",
    "        x[:, 0],\n",
    "        x[:, 1],\n",
    "        alpha=0.6,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dafa9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "web_mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eaf1b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "web_cc_mp"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
