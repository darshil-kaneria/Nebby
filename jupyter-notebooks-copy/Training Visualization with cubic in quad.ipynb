{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d14cbaef",
   "metadata": {},
   "source": [
    "# Entire Training Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6879ad96",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH=\"../../control_tests_n/\" # Fodler with the files for training\n",
    "yellow = '\\033[93m'\n",
    "green = '\\033[92m'\n",
    "red = '\\033[91m'\n",
    "blue = '\\033[94m'\n",
    "pink = '\\033[95m'\n",
    "black = '\\033[90m'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b34ea2",
   "metadata": {},
   "source": [
    "### Bytes in flight trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb5514e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import math\n",
    "\n",
    "SHOW=True\n",
    "MULTI_GRAPH=False\n",
    "SMOOTHENING=False\n",
    "ONLY_STATS=False\n",
    "s_factor=0.9\n",
    "\n",
    "\n",
    "PKT_SIZE = 88\n",
    "\n",
    "\n",
    "'''\n",
    "TODO: \n",
    "o Add functionality where you only plot flows that send more than x bytes of data\n",
    "o Sort stats and graphs by flow size\n",
    "o Organize plots by flow size (larger flows have larger graphs)\n",
    "o Custom smoothening function\n",
    "'''\n",
    "\n",
    "fields=[\"time\", \"frame_time_rel\", \"tcp_time_rel\", \"frame_num\", \"frame_len\", \"ip_src\", \"src_port\", \"ip_dest\", \"dest_port\", \"tcp_len\", \"seq\", \"ack\"]\n",
    "\n",
    "class pkt:\n",
    "    contents=[]\n",
    "    def __init__(self, fields) -> None:\n",
    "        self.contents=[]\n",
    "        for f in fields:\n",
    "            self.contents.append(f)\n",
    "\n",
    "    def get(self, field):\n",
    "        return self.contents[fields.index(field)]\n",
    "        \n",
    "\n",
    "def process_flows(cc, dir,p=\"y\"):\n",
    "    name = dir+cc+\"-tcp.csv\"\n",
    "    with open(name) as csv_file:\n",
    "        print(\"Reading \"+name+\"...\")\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "        line_count = 0\n",
    "        total_bytes=0\n",
    "        '''\n",
    "        Flow tracking:\n",
    "        o Identify all packets that are either sourced from or headed to 100.64.0.2\n",
    "        o Group different flows by client's port\n",
    "        '''\n",
    "        flows={}\n",
    "        data_sent=0\n",
    "        # ACK and RTX measurement\n",
    "        ooa = set()\n",
    "        rp = set()\n",
    "        ooaCount=0\n",
    "        rpCount=0\n",
    "        daCount=0\n",
    "        backAck=0\n",
    "        for row in csv_reader:\n",
    "            reTx=0\n",
    "            \n",
    "            ooAck=0\n",
    "            dupAck=0\n",
    "            retPacket=0\n",
    "            \n",
    "            packet=pkt(row)\n",
    "            ackPkt=False\n",
    "            validPkt=False\n",
    "            if line_count==0:\n",
    "                # reject the header\n",
    "                line_count+=1\n",
    "                continue\n",
    "            if data_sent == 0 : \n",
    "                if \"100.64.0.\" in packet.get(\"ip_src\"):\n",
    "                    num = int(packet.get(\"ip_src\")[-1])\n",
    "                    if num%2==0:\n",
    "                        data_sent=1\n",
    "                        host_port=packet.get(\"ip_src\")\n",
    "                if \"100.64.0.\" in packet.get(\"ip_dest\"):\n",
    "                    num = int(packet.get(\"ip_dest\")[-1])\n",
    "                    if num%2==0:\n",
    "                        data_sent=1\n",
    "                        host_port=packet.get(\"ip_dest\")\n",
    "                if data_sent == 0:\n",
    "                    continue\n",
    "            if packet.get(\"ip_src\")==host_port and packet.get(\"frame_time_rel\")!='' and packet.get(\"ack\")!='': \n",
    "                # we care about this ACK packet\n",
    "                validPkt=True\n",
    "                ackPkt=True\n",
    "                port=packet.get(\"src_port\")\n",
    "                #PORTCHECK\n",
    "#                 if int(port) != 50468:\n",
    "#                     continue\n",
    "                if port not in flows:\n",
    "                    flows[port]={\"OOA\":[],\"DA\":[],\"max_seq\":0,\"loss_bif\":0,\"max_ack\":int(packet.get(\"ack\")),\"serverip\":packet.get(\"ip_dest\"), \"serverport\":packet.get(\"dest_port\"), \"act_times\":[],\"times\":[], \"windows\":[], \"cwnd\":[], \"bif\":0, \"last_ack\":0, \"last_seq\":0, \"pif\":0, \"drop\":[], \"next\":0, \"retrans\":[]}\n",
    "                else:\n",
    "                    # check for Out of Order Ack (OOA)\n",
    "                    if int(packet.get(\"ack\")) <= int(flows[port][\"max_ack\"]):\n",
    "                        if int(packet.get(\"ack\")) == backAck :\n",
    "                            dupAck = True\n",
    "                            flows[port][\"DA\"].append(float(packet.get(\"frame_time_rel\")))\n",
    "                        else :\n",
    "                            ooAck = True\n",
    "                            flows[port][\"OOA\"].append(float(packet.get(\"frame_time_rel\")))\n",
    "                        backAck = int(packet.get(\"ack\"))\n",
    "                    # update max_ack\n",
    "                    flows[port][\"max_ack\"] = max(flows[port][\"max_ack\"], int(packet.get(\"ack\")))\n",
    "                    if int(packet.get(\"seq\")) < flows[port][\"max_ack\"]:\n",
    "                        reTx += int(packet.get(\"tcp_len\"))\n",
    "#                     flows[port][\"times\"].append(float(packet.get(\"frame_time_rel\")) )\n",
    "                    \n",
    "            elif packet.get(\"ip_dest\")==host_port and packet.get(\"frame_time_rel\")!='' and packet.get(\"seq\")!='':\n",
    "                #we care about this Data packet\n",
    "                validPkt=True\n",
    "                port=packet.get(\"dest_port\")\n",
    "                #PORTCHECK\n",
    "#                 if int(port) != 50468:\n",
    "#                     continue\n",
    "                seq=int(packet.get(\"seq\"))\n",
    "                tcp_len=int(packet.get(\"tcp_len\"))\n",
    "                if port not in flows:\n",
    "                    flows[port]={\"OOA\":[],\"DA\":[],\"max_seq\":int(packet.get(\"seq\")),\"loss_bif\":0,\"max_ack\":0,\"serverip\":packet.get(\"ip_src\"), \"serverport\":packet.get(\"src_port\"),\"act_times\":[], \"times\":[], \"windows\":[], \"cwnd\":[], \"bif\":0, \"last_ack\":0, \"last_seq\":0, \"pif\":0, \"drop\":[], \"next\":0, \"retrans\":[]}\n",
    "                \n",
    "                else:\n",
    "                    flows[port][\"max_seq\"] = max(flows[port][\"max_seq\"], int(packet.get(\"seq\")))\n",
    "                \n",
    "                \n",
    "                if int(packet.get(\"seq\")) < flows[port][\"max_seq\"] :\n",
    "                    retPacket = True\n",
    "                    flows[port][\"retrans\"].append(flows[port][\"times\"][-1])\n",
    "                    \n",
    "            if validPkt==True:\n",
    "                bif = 0\n",
    "                normal_est_bif = int(flows[port][\"max_seq\"]) - int(flows[port][\"max_ack\"]) + PKT_SIZE#+ reTx\n",
    "                loss_est_bif = flows[port][\"loss_bif\"]\n",
    "                if ackPkt and dupAck and len(flows[port][\"windows\"]) > 10:\n",
    "                    if dupAck:\n",
    "                        # if we have received a duplicate ack then we need to reduce the bytes in flight by packet size\n",
    "                        # we also increase max ack to correct for the consolidated ack being sent later\n",
    "                        loss_est_bif = int(flows[port][\"windows\"][-1]) - PKT_SIZE\n",
    "                        flows[port][\"max_ack\"] += PKT_SIZE\n",
    "                        \n",
    "                        bif = min( normal_est_bif, loss_est_bif)\n",
    "                        if p == \"y\" :\n",
    "                            print(green+\"Duplicate Ack\",int(packet.get(\"ack\")),\"Max Ack\",flows[port][\"max_ack\"],\"BIF\",bif)\n",
    "                        \n",
    "#                     elif ooAck:\n",
    "#                         # first out of order ack that we have recieved not a duplicated ack\n",
    "#                         # the reason would be restransimitted packet so dont need to correct for this\n",
    "                        \n",
    "                elif ackPkt :\n",
    "                    loss_est_bif = normal_est_bif \n",
    "                    bif = normal_est_bif    \n",
    "                    if ooAck :\n",
    "                        ooaCount+=1\n",
    "                        if p == \"y\":\n",
    "                            print(red+\"Out of Order Ack\",int(packet.get(\"ack\")),\"Max Ack\",flows[port][\"max_ack\"],\"BIF\",normal_est_bif)\n",
    "                        ooa.add(int(packet.get(\"ack\")))\n",
    "                    else:\n",
    "                        if p == \"y\":\n",
    "                            print(black+\"Inorder Ack\",int(packet.get(\"ack\")),\"Max Seq\",flows[port][\"max_seq\"],\"BIF\",normal_est_bif)\n",
    "                else :\n",
    "                    bif = normal_est_bif\n",
    "                    if retPacket==True:\n",
    "                        rpCount+=1\n",
    "                        rp.add(int(packet.get(\"seq\")))\n",
    "                        if p == \"y\":\n",
    "                            print(pink+\"Retransmitted Packet\",int(packet.get(\"seq\")), \"Next\", flows[port][\"max_seq\"]+PKT_SIZE, \"BIF\",bif)\n",
    "                    else :\n",
    "                        if p == \"y\":\n",
    "                            print(blue+\"Inorder Packet\", int(packet.get(\"seq\")), \"Next\", flows[port][\"max_seq\"]+PKT_SIZE, \"BIF\",bif)\n",
    "                flows[port][\"loss_bif\"] = loss_est_bif\n",
    "                flows[port][\"windows\"].append( int(bif) )\n",
    "                flows[port][\"times\"].append( float(packet.get(\"frame_time_rel\")) )\n",
    "                \n",
    "#                 if ackPkt and dupAck and len(flows[port][\"windows\"]) > 10: # we have received atleast the first window\n",
    "# #                     if len(flows[port][\"windows\"]) < 2000: # print reTx in first 200 packets\n",
    "# #                         print( packet.get(\"ack\"), flows[port][\"max_ack\"])\n",
    "#                     loss_est_bif = int(flows[port][\"windows\"][-1]) - PKT_SIZE\n",
    "#                     flows[port][\"max_ack\"] += PKT_SIZE\n",
    "#                     bif = min( normal_est_bif, loss_est_bif )\n",
    "#                 elif ackPkt:\n",
    "#                     loss_est_bif = normal_est_bif \n",
    "#                     bif = normal_est_bif\n",
    "#                 else:\n",
    "#                     bif = normal_est_bif\n",
    "#                 flows[port][\"loss_bif\"] = loss_est_bif\n",
    "#                 flows[port][\"windows\"].append( int(bif) )\n",
    "            \n",
    "            \n",
    "            line_count+=1\n",
    "            total_bytes+=int(packet.get(\"frame_len\"))\n",
    "            #print(line_count, total_bytes)\n",
    "            \n",
    "#         print(\"total bytes processed:\", total_bytes/1000, \"KBytes for\", cc, \"(unlimited)\")\n",
    "        if p == \"y\":\n",
    "            print(\"Out of Order Acks\",len(ooa),\"Retransmitted Packets\",len(rp))\n",
    "            print(\"Count Out of Order Acks\",ooaCount,\"Retransmitted Packets\",rpCount)\n",
    "            print(\"OOA\",ooa,\"RP\",rp)\n",
    "    return flows\n",
    "\n",
    "def custom_smooth_function():\n",
    "    pass\n",
    "\n",
    "def get_flow_stats(flows):\n",
    "    num=len(flows.keys())\n",
    "    print(\"FLOW STATISTICS: \\nNumber of flows: \", num)\n",
    "    print(\"------------------------------------------------------------------------------\")\n",
    "    print('%6s'%\"port\", '%15s'%\"SrcIP\", '%8s'%\"SrcPort\",  '%8s'%\"duration\",  '%8s'%\"start\",  '%8s'%\"end\", '%8s'%\"Sent (B)\", '%8s'%\"Recv (B)\",)\n",
    "    for k in flows.keys():\n",
    "        print('%6s'%k, '%15s'%flows[k][\"serverip\"], '%8s'%flows[k][\"serverport\"], '%8s'%str('%.2f'%(flows[k][\"times\"][-1]-flows[k][\"times\"][0])), '%8s'%str('%.2f'%flows[k][\"times\"][0]), '%8s'%str('%.2f'%flows[k][\"times\"][-1]), '%8s'%flows[k][\"last_seq\"], '%8s'%flows[k][\"last_ack\"])\n",
    "        #print(\"    * Flow \"+str(k)+\": \", flows[k][\"last_ack\"], \" \", flows[k][\"last_seq\"], \" bytes transfered.\")\n",
    "    return num\n",
    "\n",
    "def run(files):\n",
    "    flows = {}\n",
    "    for f in files:\n",
    "        algo_cc=f\n",
    "        #Get the data for all the flows\n",
    "        print(\"==============================================================================\")\n",
    "        print(\"opening trace ../measurements/\"+algo_cc+\".csv...\")\n",
    "        flows = process_flows(algo_cc, \"./Nebby/measurements-new-btl/50-200-2-60/\")\n",
    "        #decide on final graph layout\n",
    "        num = get_flow_stats(flows)\n",
    "\n",
    "        if ONLY_STATS:\n",
    "            sys.exit()\n",
    "\n",
    "        if num==1:\n",
    "            MULTI_GRAPH=False\n",
    "        #grid size\n",
    "        if MULTI_GRAPH:\n",
    "            size=(0,0)\n",
    "            grids={1:(2,2), 2:(2,2), 4:(2,2), 6:(2,3), 9:(3,3), 12:(3,4), 15:(3,5), 16:(4,4), 20:(5,4), 24:(6,4), 30:(6,5), 36:(6,6), 40:(8,5), 42:(8,7), 49:(7,7)}\n",
    "            g=num\n",
    "            while g<=49 and g not in grids:\n",
    "                g+=1\n",
    "            if g in grids.keys():\n",
    "                size=grids[g]\n",
    "            else:\n",
    "                size=grids[49]  \n",
    "            fig, axs = plt.subplots(size[0], size[1])\n",
    "            for i in range(size[0]):\n",
    "                for j in range(size[1]):\n",
    "                    #axs[i][j].legend(loc=\"lower right\")\n",
    "                    if i==size[0]-1:\n",
    "                        axs[i][j].set_xlabel(\"Time (s)\")\n",
    "                    if j==0:\n",
    "                        axs[i][j].set_ylabel(\"Bytes in flight\")\n",
    "        else:\n",
    "            plt.xlabel(\"Time (s)\")\n",
    "            plt.ylabel(\"Bytes in flight\")\n",
    "        counter=0\n",
    "        for port in flows.keys():\n",
    "            if MULTI_GRAPH:  \n",
    "                axs[counter%size[0]][(counter//size[0])%size[1]].scatter(flows[port][\"times\"], flows[port][\"windows\"], color=\"#858585\")\n",
    "                axs[counter%size[0]][(counter//size[0])%size[1]].plot(flows[port][\"times\"], flows[port][\"windows\"], label=str(port), linestyle=\"solid\")\n",
    "            else:\n",
    "                plt.plot(flows[port][\"times\"], flows[port][\"windows\"], label=str(port), linestyle=\"solid\")\n",
    "                plt.scatter(flows[port][\"times\"], flows[port][\"windows\"], color=\"#858585\")\n",
    "            counter+=1\n",
    "        if MULTI_GRAPH:\n",
    "            counter=0\n",
    "            for port in flows.keys():\n",
    "                axs[counter%size[0]][(counter//size[0])%size[1]].legend()\n",
    "                counter+=1\n",
    "        else:\n",
    "            plt.legend()\n",
    "        if MULTI_GRAPH:\n",
    "            fig.set_size_inches(16, 12)\n",
    "        if SHOW:\n",
    "            plt.show()\n",
    "        else:\n",
    "            plt.savefig(\"../logs/results/\"+algo_cc+\".png\", dpi=600, bbox_inches='tight', pad_inches=0)\n",
    "    return flows\n",
    "\n",
    "\n",
    "def get_window(f,p,t=1,path=PATH):\n",
    "    algo_cc = f\n",
    "    flows = process_flows(algo_cc,path,p=p)\n",
    "#     flows = process_flows(algo_cc, \"../measurements/m/\",p=p)\n",
    "#     flows = process_flows(algo_cc, \"./Nebby/measurements/\",p=p)    \n",
    "#     flows = process_flows(algo_cc, \"./Nebby/measurements-new-btl/50-200-2-60/\",p=p)\n",
    "#     flows = process_flows(algo_cc, \"./Nebby/measurements-100-150/\",p=p)\n",
    "    params = algo_cc.split(\"-\")\n",
    "    data = []\n",
    "    time = []\n",
    "    drops = []\n",
    "    retrans = []\n",
    "    OOA = []\n",
    "    DA = []\n",
    "    use_port = 0\n",
    "    maxx = 0\n",
    "    print(\"All Ports : \", flows.keys())\n",
    "    for port in flows.keys():\n",
    "        if len(flows[port]['windows']) > maxx:\n",
    "            maxx = len(flows[port]['windows'])\n",
    "            use_port = port\n",
    "    print(\"Port\",use_port)\n",
    "    data = flows[use_port]['windows']\n",
    "    time = flows[use_port]['times']\n",
    "    retrans = flows[use_port]['retrans']\n",
    "    OOA = flows[use_port]['OOA']\n",
    "    DA = flows[use_port]['DA']\n",
    "    if p == \"y\":\n",
    "        plt.plot(time, data)\n",
    "#     time_index = len(time)-1\n",
    "#     for index in range(len(flows[use_port]['times'])-1):\n",
    "#         if flows[use_port]['times'][index+1] - flows[use_port]['times'][index] > :\n",
    "#             time_index = index\n",
    "#     time_last = flows[use_port]['times'][time_index]\n",
    "#     data = data[:time_index+1]\n",
    "#     time = time[:time_index+1]\n",
    "    if t==2:\n",
    "        return data, time, retrans, OOA, DA\n",
    "    if t==1:\n",
    "        return data, time, retrans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84ed020",
   "metadata": {},
   "outputs": [],
   "source": [
    "data, time, retrans = get_window(\"reno-51-0-50-200-2-aws-88-60\",p=\"y\",t=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3de3dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data, time, retrans = get_window(\"cubic-51-0-50-200-2-aws-88-60\",p=\"y\",t=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8540b9b",
   "metadata": {},
   "source": [
    "### FFT Filter and features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d929d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.fft import rfft, rfftfreq\n",
    "from scipy.fft import irfft\n",
    "import numpy as np\n",
    "import statistics\n",
    "from statistics import mean, pstdev\n",
    "import pandas as pd\n",
    "\n",
    "def smoothen(time, data, rtt):\n",
    "    # Smoothening \n",
    "    left = 0\n",
    "    right = 0\n",
    "    run_sum = 0\n",
    "    avg_data = []\n",
    "    new_time = []\n",
    "    roll_time = time\n",
    "    roll_data = data\n",
    "    while right < len(roll_time):\n",
    "        while(right < len(roll_time) and (roll_time[right]-roll_time[left] < 2*rtt)):\n",
    "            run_sum+=roll_data[right]\n",
    "            right+=1\n",
    "        new_time.append(float(roll_time[right-1]+roll_time[left])/2)\n",
    "        avg_data.append(float(run_sum)/(right-left))\n",
    "        run_sum-=roll_data[left]\n",
    "        left+=1\n",
    "    return new_time, avg_data\n",
    "\n",
    "\n",
    "def get_fft(data):\n",
    "    n = len(data)\n",
    "    data_step = 0.005\n",
    "    yf = rfft(data)\n",
    "    xf = rfftfreq(n,data_step)\n",
    "    return yf,xf\n",
    "\n",
    "def get_fft_smoothening(data, time, ax,rtt,p):\n",
    "    rtt=rtt\n",
    "    yf, xf = get_fft(data)\n",
    "    thresh  = (1/rtt)\n",
    "    thresh_ind = 0\n",
    "    for i in range(len(xf)) :\n",
    "        freq = xf[i]\n",
    "        if(freq > thresh):\n",
    "            thresh_ind = i\n",
    "            break\n",
    "            \n",
    "    yf_clean = yf\n",
    "    yf_clean[thresh_ind+1:] = 0\n",
    "    new_f_clean = irfft(yf_clean)\n",
    "    start_len = len(time) - len(new_f_clean)\n",
    "\n",
    "    plot_data = new_f_clean\n",
    "#     if p==\"y\":\n",
    "#         ax.plot(time[start_len:], plot_data, 'k', label='FFT smoothening', linewidth=1.5)\n",
    "\n",
    "    plot_time = time[start_len:] \n",
    "    return plot_time, plot_data\n",
    "\n",
    "def plot_d(ax, time, data, c, l, alpha=1):\n",
    "    ax.plot(time, data, color=c, lw=2, label = l,alpha=alpha)\n",
    "\n",
    "\n",
    "def plot_one_bt(f, p,t=1):\n",
    "    print(f)\n",
    "    fs = f.split(\"-\")\n",
    "\n",
    "    pre = int(fs[2])\n",
    "    post = int(fs[3])\n",
    "    rtt = float(((pre+post)*2))/1000\n",
    "    ax = 0\n",
    "    if t==1:\n",
    "        data, time, retrans = get_window(f,\"n\",t,PATH)\n",
    "    elif t==2:\n",
    "        data, time, retrans, OOA, DA = get_window(f,\"n\",t,PATH)\n",
    "    if p == 'y':\n",
    "        fig, ax = plt.subplots(1,1, figsize=(15,8))\n",
    "        for t in retrans :\n",
    "            plt.axvline(x = t, color = 'm',alpha=0.5)\n",
    "        if t == 2:\n",
    "            for t in OOA :\n",
    "                plt.axvline(x = t, color = 'k', lw=2)\n",
    "            for t in DA:\n",
    "                plt.axvline(x = t, color = 'g', lw=0.5, alpha = 0.5)\n",
    "        plot_d(ax,time,data, \"r\",\"Original\")\n",
    "    time, data = get_fft_smoothening(data, time, ax,rtt,\"y\")\n",
    "\n",
    "    #         plot_d(ax,time,data,\"b\",\"FFT Smoothened\" )\n",
    "#         print(len(time), len(data))\n",
    "    time, data = smoothen(time, data, rtt)\n",
    "#         print(len(time), len(data))\n",
    "    if p == 'y':\n",
    "        plot_d(ax, time, data, \"b\", \"Smoothened\",alpha=0.5)\n",
    "        ax.legend()\n",
    "#             plt.savefig(\"./plots/\"+f+\".png\")\n",
    "        plt.show()\n",
    "#     return time, data, grad_time, grad_data, rtt\n",
    "#     print(\"Black : OOA, Green : DA, Magenta : RP\")\n",
    "    return time, data, retrans, rtt \n",
    "\n",
    "\n",
    "def get_time_features(retrans,time,rtt):\n",
    "    time_thresh = 20*rtt\n",
    "    features = []\n",
    "    for i in range(1, len(retrans)):\n",
    "        if retrans[i]-retrans[i-1] >= time_thresh:\n",
    "            features.append([retrans[i-1], retrans[i]])\n",
    "    # add a feature that finished when the experiment ends\n",
    "    if len(retrans) > 0  and time[-1] - retrans[-1] > 20*rtt :\n",
    "        features.append([retrans[-1],time[-1]])\n",
    "    return features\n",
    "\n",
    "def get_features(time, features):\n",
    "    left = 0\n",
    "    right = 0\n",
    "    feature_index = 0\n",
    "    in_feature = 0\n",
    "    index_features = []\n",
    "    while right < len(time) and feature_index < len(features): \n",
    "        if in_feature == 0 and time[right]>=features[feature_index][0]:\n",
    "            in_feature = 1\n",
    "            left = right\n",
    "        elif in_feature == 1 and time[right] > features[feature_index][1]:\n",
    "            in_feature = 0\n",
    "            index_features.append([left, right-1])\n",
    "            feature_index+=1\n",
    "        right+=1\n",
    "    if in_feature == 1:\n",
    "        index_features.append([left, right-1])\n",
    "    return index_features\n",
    "\n",
    "def get_plot_features(curr_file, p):\n",
    "    time, data, retrans, rtt = plot_one_bt(curr_file,p=p,t=1)\n",
    "    time_features = get_time_features(retrans,time,rtt)\n",
    "    features = get_features(time, time_features)    \n",
    "    if p == 'y':\n",
    "        fig, ax = plt.subplots(1,1, figsize=(15,8))\n",
    "        plot_d(ax, time, data, \"b\", \"Smoothened\")\n",
    "        for ft in features : \n",
    "#             print(time[ft[1]]-time[ft[0]])\n",
    "            ax.plot(time[ft[0]:ft[1]+1], data[ft[0]:ft[1]+1], color = 'r')\n",
    "        plt.show()\n",
    "    return time, data, features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37174354",
   "metadata": {},
   "outputs": [],
   "source": [
    "data, time, retrans = get_plot_features(\"reno-51-0-50-200-2-aws-88-60\",p=\"y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c65b1da",
   "metadata": {},
   "source": [
    "### Sampling from the features to reduce dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c11a60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bisect\n",
    "def lower_bound(arr, target):\n",
    "    index = bisect.bisect_left(arr, target)\n",
    "    return index\n",
    "\n",
    "def sample_data_time(time, data, ss, m):\n",
    "    curr_time, curr_data = adjust(time, data)\n",
    "    tp = curr_time[len(curr_time)-1] - curr_time[0]\n",
    "    step = tp/m\n",
    "    samp_time = [curr_time[0] + i*step for i in range(m)]\n",
    "    x = np.random.uniform(0,math.pi,ss)\n",
    "    tr_x = np.cos(x)\n",
    "    tr_x += 1\n",
    "    tr_x *= (m-1)/2\n",
    "    ind = [int(round(e, 0)) for e in tr_x]\n",
    "    sort_ind = sorted(ind)\n",
    "    tr_time = [samp_time[i] for i in sort_ind]\n",
    "    new_time = []\n",
    "    new_data = []\n",
    "    for t in tr_time :\n",
    "        i = lower_bound(curr_time, t)\n",
    "        temp_t = 0\n",
    "        temp_d = 0\n",
    "        if round(t,6) == round(curr_time[i],6):\n",
    "            temp_t = curr_time[i]\n",
    "            temp_d = curr_data[i]\n",
    "        else : \n",
    "            if i == 0 :\n",
    "                temp_t = curr_time[i]\n",
    "                temp_d = curr_data[i]\n",
    "            elif i == len(curr_time)-1 :\n",
    "                temp_t = curr_time[i]\n",
    "                temp_d = curr_data[i]\n",
    "            else :\n",
    "                temp_t = (curr_time[i-1] + curr_time[i])/2\n",
    "                temp_d = (curr_data[i-1] + curr_data[i])/2\n",
    "        new_time.append(temp_t)\n",
    "        new_data.append(temp_d)\n",
    "    new_data.insert(0,curr_data[0])\n",
    "    new_data.append(curr_data[-1])\n",
    "    new_time.insert(0,curr_time[0])\n",
    "    new_time.append(curr_time[-1])\n",
    "#     return curr_time, curr_data\n",
    "    return new_time, new_data\n",
    "\n",
    "def adjust(time, data):\n",
    "    start = data.index(min(data[:int(len(data)/2)]))\n",
    "    end = data.index(max(data[int(len(data)/2):]))\n",
    "#     print(\"Difference in max and min \", end-start)\n",
    "    if end - start <= 0: \n",
    "        return time, data\n",
    "    new_time = time[start:end+1]\n",
    "    new_data = data[start:end+1]\n",
    "    return new_time, new_data\n",
    "\n",
    "# Taking 100 as the threshold is fine \n",
    "# Now we have to see how the graphs look if we use it. \n",
    "# var = [\"reno\", \"cubic\", \"bbr\"]\n",
    "def getRed(files,ss=125,p=\"y\", ft_thresh=100):\n",
    "    results = []\n",
    "    for file in files :   \n",
    "        f_split = file.split(\"-\")\n",
    "        v = f_split[0] + \"-\" + f_split[1]\n",
    "        rtt = float((int(f_split[2]) + int(f_split[3]))*2)/1000\n",
    "        bdp = float(rtt*1000*int(f_split[4])*int(f_split[5]))/8\n",
    "        print(file)\n",
    "        print(\"RTT\",rtt,\"BDP\",bdp)\n",
    "        time, data, features = get_plot_features(file, p=p)\n",
    "        count = 1\n",
    "        for ft in features : \n",
    "            if count > ft_thresh:\n",
    "                break\n",
    "            curr_time = time[ft[0]:ft[1]+1]\n",
    "            curr_data = data[ft[0]:ft[1]+1]\n",
    "            tr_time, tr_data = sample_data_time(curr_time, curr_data, ss, 1000)\n",
    "            tr_time_pd = pd.DataFrame(tr_time)\n",
    "            tr_data_pd = pd.DataFrame(tr_data)\n",
    "            tr_time = list(tr_time_pd.rolling(25, center=True).mean().dropna()[0])\n",
    "            tr_data = list(tr_data_pd.rolling(25, center=True).mean().dropna()[0])\n",
    "            print(\"Feature Length \", len(tr_data))\n",
    "            if p == \"y\" :\n",
    "                plt.plot(curr_time, curr_data, c='b', alpha = 0.5, lw = 5)\n",
    "                plt.plot(tr_time, tr_data, c='r', alpha = 1)\n",
    "                plt.scatter(tr_time, tr_data, c='k')\n",
    "#                 plt.scatter(tr_time, tr_data, c='r', s=10)\n",
    "                plt.title(v)\n",
    "                plt.show()\n",
    "            results.append(\n",
    "                {v+\"_\"+\"data\"+\"_\"+str(count):tr_data,\n",
    "                     v+\"_\"+\"time\"+\"_\"+str(count):tr_time,\n",
    "                        v+\"_\"+\"rtt\"+\"_\"+str(count):rtt, \n",
    "                             v+\"_\"+\"bdp\"+\"_\"+str(count):bdp})\n",
    "            count+=1\n",
    "    return results\n",
    "# results = getRed(var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8793d5dc",
   "metadata": {},
   "source": [
    "#### ss - sample size, ft_thresh = no of features to extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fe4bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = getRed([\"cubic-51-0-50-200-2-aws-88-60\"],ss=225,p=\"y\",ft_thresh=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1076aeff",
   "metadata": {},
   "source": [
    "### Fitting polynomail to get the values, and normalizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96b5a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error as mse\n",
    "def get_degree(time,data, p=\"n\", max_deg=3,cc=\"default\"):\n",
    "    print(\"Degree to fit\",max_deg)\n",
    "    p_net = []\n",
    "    mse_l = []\n",
    "    fit_net = []\n",
    "    for d in range(1,max_deg+1):\n",
    "        p_temp = np.polyfit(time,data, d)\n",
    "        # No need of the constant term\n",
    "        p_net.append(p_temp[0:-1])\n",
    "        fit_net.append(np.polyval(p_temp,time))\n",
    "        mse_l.append(mse(data,fit_net[-1]))\n",
    "    if p =='y':\n",
    "        plt.plot(time, data,c='k',label='Truth')\n",
    "        for d in range(max_deg-1, max_deg):\n",
    "            plot_label = \"degree\"+str(d)\n",
    "            plt.plot(time, fit_net[d],label=\"degree\" + str(d+1))\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    return max_deg,p_net[max_deg-1], mse_l\n",
    "\n",
    "def normalize(time, data, rtt, bdp):\n",
    "    new_time = time - min(time)\n",
    "    new_data = data - min(data)\n",
    "    new_time = (new_time/max(new_time))*10\n",
    "    new_data = (new_data/max(new_data))*10\n",
    "    return new_time, new_data\n",
    "    \n",
    "from statistics import mean \n",
    "def get_feature_degree(files,ss=225,p='n',ft_thresh=3,max_deg=3):\n",
    "    results = getRed(files,ss,p=p,ft_thresh=ft_thresh)\n",
    "    errors = []\n",
    "    mp = {}\n",
    "    cc_mp = {}\n",
    "    count_features = 0\n",
    "    for item in results :\n",
    "        for ele in list(item.keys()):\n",
    "            name_list = ele.split(\"_\")\n",
    "            cc = name_list[0]\n",
    "            name = name_list[0] + name_list[-1]\n",
    "            if \"data\" in ele :\n",
    "                curr_data = np.array(item[ele])\n",
    "            if \"time\" in ele :\n",
    "                curr_time = np.array(item[ele])\n",
    "            if \"rtt\" in ele :\n",
    "                curr_rtt = item[ele]\n",
    "            if \"bdp\" in ele :\n",
    "                curr_bdp = item[ele]\n",
    "        curr_time, curr_data = normalize(curr_time, curr_data, curr_rtt, curr_bdp)\n",
    "        count_features += 1\n",
    "        print(\"Name :\",name)\n",
    "        degree, coeff, error_item = get_degree(curr_time, curr_data,p=p,max_deg=max_deg,cc=cc)\n",
    "        # Adding time feature here\n",
    "#         coeff_list = list(coeff)\n",
    "#         coeff_list.append(abs(curr_time[-1]-curr_time[0]))\n",
    "#         coeff = np.array(coeff_list)\n",
    "#         print(coeff)\n",
    "#         print(cc)\n",
    "#         cc_curr = cc.split(\"-\")[0]\n",
    "        # Adding another check for scalable and yeah\n",
    "#         if cc_curr in ['scalable','yeah']:\n",
    "#             # Look at the fifth coefficient\n",
    "#             if round(coeff[0],6) < 0.000015:\n",
    "#                 print(coeff[0])\n",
    "#                 continue\n",
    "#         coeff.append(abs(curr_time[-1]-curr_time[0]))\n",
    "        mp[name] = {'d':degree, 'coeff':coeff, 'error':error_item, 'data':curr_data, 'time':curr_time}\n",
    "        if cc not in cc_mp :\n",
    "            cc_mp[cc] = []\n",
    "        cc_mp[cc].append(mp[name])\n",
    "    return cc_mp\n",
    "\n",
    "def getCC(files,cc_mp, p=\"n\"):\n",
    "    # experiment change start\n",
    "    cc_coeff = {}\n",
    "    for file in files :\n",
    "#         file = v + \"-0-50-1000-2\"\n",
    "        curr_file = file\n",
    "        f_split = file.split(\"-\") \n",
    "        cc = f_split[0]\n",
    "        version = f_split[1]\n",
    "        v = cc+\"-\"+version\n",
    "        if cc not in cc_coeff:\n",
    "            cc_coeff[cc] = []\n",
    "    # experiment change end\n",
    "        time, data, retrans, rtt = plot_one_bt(curr_file, p)\n",
    "        count = 0\n",
    "        temp = []\n",
    "        if v not in cc_mp.keys():\n",
    "            continue\n",
    "        n = math.ceil(float(len(cc_mp[v]))/3)\n",
    "        for item in cc_mp[v]:\n",
    "            time = item['time']\n",
    "            data = item['data']\n",
    "            deg = item['d']\n",
    "            temp.append(item['coeff'])    \n",
    "            xlim = 0\n",
    "            ylim = 0\n",
    "            t = 1\n",
    "            while time[-1] > t:\n",
    "                t*=2\n",
    "            xlim = t\n",
    "            while data[-1] > t:\n",
    "                t*=2\n",
    "            ylim = t\n",
    "            lim = max(xlim, ylim)\n",
    "            print(lim)\n",
    "            names = []\n",
    "            bars=[]\n",
    "            for i in range(1,deg+1):\n",
    "                bars.append(i)\n",
    "                names.append(str(i))\n",
    "#             print(names)\n",
    "#             print(item['error'])\n",
    "            count+=1\n",
    "            if p == 'y':\n",
    "                print([round(x,5) for x in item['coeff']])\n",
    "                plt.plot(time,data)\n",
    "                plt.plot(time, np.polyval(item['coeff'],time))\n",
    "                plt.xlim(0,lim)\n",
    "                plt.ylim(0,lim)\n",
    "                plt.title(str(count)+\" \" + cc)\n",
    "                plt.show()\n",
    "#                 Showing the coefficient magnitude on a bar plot\n",
    "#                 plt.figure().set_figwidth(4)\n",
    "#                 plt.figure().set_figheight(2)\n",
    "#                 plt.bar([i for i in range(1,deg+2)], item['coeff'])\n",
    "#                 plt.show()\n",
    "#                 Showing the change in error magnitute on a bar plot\n",
    "                plt.figure().set_figwidth(4)\n",
    "                plt.figure().set_figheight(2)\n",
    "                plt.bar(bars, item['error'][0:deg], tick_label=names)\n",
    "                plt.show()\n",
    "        cc_coeff[cc].append(temp)\n",
    "    return cc_coeff "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2c3550",
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_mp = get_feature_degree([\"htcp-51-0-50-200-2-aws-88-60\"],ss=225,p=\"y\",ft_thresh=1,max_deg=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d84463",
   "metadata": {},
   "source": [
    "### Making groups according to the degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de75d268",
   "metadata": {},
   "outputs": [],
   "source": [
    "ccs = ['bic', 'dctcp', 'highspeed', 'htcp', 'lp', 'nv', 'scalable', 'vegas', 'veno', 'westwood', 'yeah', 'cubic', 'reno']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53df700d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_degree_all(time,data, p=\"n\", max_deg=3):\n",
    "    p_net = []\n",
    "    mse_l = []\n",
    "    fit_net = []\n",
    "    for d in range(1,max_deg+1):\n",
    "        p_temp = np.polyfit(time,data,d)\n",
    "        p_net.append(p_temp)\n",
    "        fit_net.append(np.polyval(p_temp,time))\n",
    "        mse_l.append(mse(data,fit_net[-1]))\n",
    "    if p =='y':\n",
    "#         print(\"1 \", p1, \"MSE \", mse(data, fit_l))\n",
    "        plt.plot(time, data,c='k',label='Truth')\n",
    "#         plt.plot(time, fit_l)\n",
    "        for d in range(max_deg-1, max_deg):\n",
    "            plot_label = \"degree\"+str(d)\n",
    "            plt.plot(time, fit_net[d],label=\"degree\" + str(d+1))\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    return max_deg,p_net, mse_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a32088",
   "metadata": {},
   "outputs": [],
   "source": [
    "total = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376a0de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_check = []\n",
    "for cc in ccs:\n",
    "    for i in range(1,total):\n",
    "        degree_check.append(cc+\"-\"+str(i)+\"-0-50-200-2-aws-88-60\")\n",
    "results = getRed(degree_check,ss=225,p=\"n\",ft_thresh=1)\n",
    "nmp = {}\n",
    "for item in results :\n",
    "    for ele in list(item.keys()):\n",
    "        name_list = ele.split(\"_\")\n",
    "        name= name_list[0]\n",
    "        if \"data\"==name_list[1] :\n",
    "            curr_data = np.array(item[ele])\n",
    "        if \"time\"==name_list[1] :\n",
    "            curr_time = np.array(item[ele])\n",
    "        if \"rtt\"==name_list[1] :\n",
    "            curr_rtt = item[ele]\n",
    "        if \"bdp\"==name_list[1] :\n",
    "            curr_bdp = item[ele]\n",
    "    curr_time, curr_data = normalize(curr_time, curr_data, curr_rtt, curr_bdp)\n",
    "    max_deg,p_net,mse_l = get_degree_all(curr_time, curr_data, p='n',max_deg=3)\n",
    "    nmp[name] = {\n",
    "        'data':curr_data,\n",
    "        'time':curr_time,\n",
    "        \"max_deg\":max_deg,\n",
    "        \"p_net\":p_net,\n",
    "        \"mse_l\":mse_l\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74b0510",
   "metadata": {},
   "outputs": [],
   "source": [
    "nmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b29288",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "for name in nmp.keys():\n",
    "#     print(name)\n",
    "#     curr_time, curr_data, retrans, rtt = plot_one_bt(name+\"-0-50-200-2-aws-88-60\",'y',1)\n",
    "    data = nmp[name]['data']\n",
    "    time = nmp[name]['time']\n",
    "    max_deg = nmp[name]['max_deg']\n",
    "    p_net = []\n",
    "    for coeff in nmp[name]['p_net']:\n",
    "        temp_pnet = [abs(c) for c in coeff]\n",
    "        p_net.append(temp_pnet)\n",
    "    mse_l = nmp[name]['mse_l']\n",
    "#     plt.plot(time, data,c='k',label='Truth')\n",
    "#         plt.plot(time, fit_l)\n",
    "#     for d in range(0, max_deg):\n",
    "#         plot_label = \"degree\"+str(d+1)\n",
    "#         fit_net = np.polyval(p_net[d],time)\n",
    "#         plt.plot(time, fit_net,label=\"degree\" + str(d+1))\n",
    "#     plt.legend()\n",
    "#     plt.show()\n",
    "    \n",
    "    names = []\n",
    "    loss = []\n",
    "    lambd = 0.02\n",
    "    for i in range(len(mse_l)):\n",
    "        loss.append((i+1)*sum(p_net[i])*lambd)\n",
    "#     print(\"loss\", loss)\n",
    "#     print(\"mse\", mse_l)\n",
    "    for d in range(0, max_deg):\n",
    "        names.append(\"degree \"+str(d))\n",
    "#     plt.bar(names,mse_l,color='blue',width=0.4,label=\"mse\")\n",
    "#     plt.bar(names,loss,bottom=mse_l,color='maroon',width=0.4,label=\"reg_loss\")\n",
    "    \n",
    "#     plt.legend()\n",
    "#     plt.show()\n",
    "    errors = []\n",
    "    # The code for deciding the categories\n",
    "    for i in range(0,max_deg):\n",
    "        errors.append(loss[i]+mse_l[i])\n",
    "    deg = errors.index(min(errors))+1\n",
    "#     if deg < 3:\n",
    "#         deg = mse_l.index(min(mse_l[0:2]))+1\n",
    "#     print(deg)\n",
    "    if deg not in results:\n",
    "        results[deg] = {}\n",
    "    current_cc = name.split(\"-\")[0]\n",
    "    if current_cc not in results[deg]:\n",
    "        results[deg][current_cc] = 0\n",
    "    results[deg][current_cc] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cf1ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f04c899",
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees = [1,2,3]\n",
    "matrix = []\n",
    "for d in degrees:\n",
    "    row = []\n",
    "    for cc in ccs:\n",
    "        if cc in results[d]:\n",
    "            row.append(results[d][cc])\n",
    "        else:\n",
    "            row.append(0)\n",
    "    matrix.append(row)\n",
    "df = pd.DataFrame(matrix,index=degrees,columns=ccs)\n",
    "print(df)\n",
    "cc_degree = {}\n",
    "for cc in ccs:\n",
    "    d_list = list(df[cc])\n",
    "    deg = d_list.index(max(d_list))+1\n",
    "    print(cc,deg)\n",
    "    cc_degree[cc] = deg \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef7f27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f794021b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cc in cc_degree:\n",
    "    cc_mp = get_feature_degree([cc+\"-6-0-50-200-2-aws-88-60\"],ss=225,p=\"y\",ft_thresh=1,max_deg=cc_degree[cc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1752d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Corrected version :\n",
    "# CUBIC = VENO CUBIC VEGAS CUBICQ\n",
    "# QUADRATIC = DCTCP HIGHSPEED LP WESTWOOD RENO HTCP\n",
    "# Linear = BIC SCALABLE YEAH \n",
    "# Problem with NV \n",
    "# Problem with VEGAS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c002eacb",
   "metadata": {},
   "source": [
    "### Getting the gaussian paramters according to the cc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa1a634",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCCcoeff(ccs,cc_degree,present_files,ss=225,p=\"n\",ft_thresh=1):\n",
    "    cc_coeff = {}\n",
    "    for v in ccs: \n",
    "        files = []\n",
    "        for f in present_files:\n",
    "            curr_cc = f.split(\"-\")[0]\n",
    "            if v == curr_cc :\n",
    "                files.append(f)\n",
    "        degree = cc_degree[v]\n",
    "        if len(files) > 0 :\n",
    "            cc_mp = get_feature_degree(files,ss=ss,p=p,ft_thresh=ft_thresh,max_deg=degree)\n",
    "            coeff = getCC(files, cc_mp,p=p)\n",
    "#             print(v)\n",
    "#             print(files)\n",
    "            cc_coeff[v] = coeff[v]\n",
    "        #     getRed(files,p=\"y\")\n",
    "    return cc_coeff "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd99426f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCoeff(cc_coeff):\n",
    "    vals = {}\n",
    "    degree = 0\n",
    "    for cc in cc_coeff:\n",
    "        coeff = cc_coeff[cc]\n",
    "        if cc not in vals :\n",
    "            vals[cc] = {}\n",
    "        for trace in coeff:\n",
    "            i = 1\n",
    "            for feature in trace:\n",
    "                if i not in vals[cc]:\n",
    "                    vals[cc][i] = []\n",
    "                vals[cc][i].append(feature)\n",
    "                i+=1\n",
    "    return vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c31012",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getGaussianParams(vals):\n",
    "    cc_gaussian_params = {}\n",
    "    for cc in vals :\n",
    "        # Taking the first feature only\n",
    "        if len(list(vals[cc].keys()))==0 :\n",
    "            continue\n",
    "        data = vals[cc][1]\n",
    "        n = 0\n",
    "        cc_coeff_mean = np.mean(data,axis=0)\n",
    "        # OGb\n",
    "#         coeff_var = np.cov(data, rowvar=False)\n",
    "#         iden = np.identity(len(cc_coeff_mean))\n",
    "#         cc_coeff_var = coeff_var * iden\n",
    "        #TRY\n",
    "        cc_coeff_var = np.cov(data,rowvar=False)\n",
    "        cc_gaussian_params[cc] = {\n",
    "            'mean' : cc_coeff_mean,\n",
    "            'covar' : cc_coeff_var\n",
    "        }\n",
    "    return cc_gaussian_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e46e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(var,cc_degree,present_files,ss=225):\n",
    "    cc_coeff = getCCcoeff(var,cc_degree,present_files,ss=ss,ft_thresh=1)\n",
    "    vals = getCoeff(cc_coeff)\n",
    "    cc_gaussian_params = getGaussianParams(vals)\n",
    "    return vals, cc_gaussian_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad55d313",
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8770950a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the cc_gaussian_params:\n",
    "import pickle\n",
    "with open(\"cc_degree_new.txt\",'wb') as f:\n",
    "    pickle.dump(cc_degree,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4241e37",
   "metadata": {},
   "source": [
    "### Getting corrrect training files \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be524ef",
   "metadata": {},
   "source": [
    "Use mahalonobis distance to det the files that have the features correctly descibed. This reduce undue variance within the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822a31b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_pos_def(A):\n",
    "    if np.allclose(A, A.T):\n",
    "        try:\n",
    "            np.linalg.cholesky(A)\n",
    "            return True\n",
    "        except np.linalg.LinAlgError:\n",
    "            return False\n",
    "    else:\n",
    "        return False \n",
    "\n",
    "from scipy.stats import chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cb92c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dont_use_ind = {}\n",
    "for cc in ccs:\n",
    "    temp_check = []\n",
    "    for i in range(1,151):\n",
    "        temp_check.append(cc+\"-\"+str(i)+\"-0-50-200-2-aws-88-60\")\n",
    "    cc_mp = get_feature_degree(temp_check, ss=225,p=\"n\",ft_thresh=1,max_deg=cc_degree[cc])\n",
    "    x = []\n",
    "    for key in cc_mp.keys():\n",
    "        x.append(cc_mp[key][0]['coeff'])\n",
    "    x = np.array(x)\n",
    "    x_mean = x.mean(axis=0)\n",
    "    if cc_degree[cc] == 1:\n",
    "        x_covar = np.array([[np.cov(x, rowvar=False)]])\n",
    "    else :\n",
    "        x_covar = np.cov(x, rowvar=False)\n",
    "    x_covar_inv = np.linalg.inv(x_covar)\n",
    "#     print(x_mean)\n",
    "#     print(x_covar)\n",
    "#     print(is_pos_def(x_covar))\n",
    "#     print(is_pos_def(x_covar_inv))\n",
    "    x_diff = x - x_mean\n",
    "    md = []\n",
    "    for i in range(len(x_diff)):\n",
    "        md.append(x_diff[i].dot(x_covar_inv).dot(x_diff[i]))\n",
    "    p = 1 - chi2.cdf(md, 4)\n",
    "    ind = []\n",
    "    thresh = 0.05\n",
    "    for i in range(len(md)):\n",
    "        if p[i] < thresh:\n",
    "            print(red)\n",
    "            ind.append(i)\n",
    "        print(i,md[i],p[i],p[i]<thresh)\n",
    "        print(black)\n",
    "    dont_use_ind[cc] = ind\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a22cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "usable_ind = {}\n",
    "unusable_ind = {}\n",
    "for cc in dont_use_ind.keys():\n",
    "    temp = []\n",
    "    untemp = []\n",
    "    for i in range(0,150):\n",
    "        if i not in dont_use_ind[cc]:\n",
    "            temp.append(i+1)\n",
    "        else :\n",
    "            untemp.append(i+1)\n",
    "    usable_ind[cc] = temp\n",
    "    unusable_ind[cc] = untemp\n",
    "    print(cc, len(temp), len(dont_use_ind[cc]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20085e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "unusable_cubic_files = []\n",
    "for i in unusable_ind['cubic']:\n",
    "    unusable_cubic_files.append(\"cubic-\"+str(i)+\"-0-50-200-2-aws-88-60\")\n",
    "# cc_mp = get_feature_degree(unusable_cubic_files,ss=225,p=\"y\",ft_thresh=1,max_deg=3)\n",
    "i = 1\n",
    "cubicQ = []\n",
    "for f in unusable_cubic_files:\n",
    "    cubicQ.append(\"cubicQ-\"+str(i)+\"-0-50-200-2-aws-88-60\")\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47571450",
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_mp = get_feature_degree(cubicQ,ss=225,p=\"y\",ft_thresh=1,max_deg=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059555e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_mp = get_feature_degree(cubicQ,ss=225,p=\"y\",ft_thresh=1,max_deg=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a801ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "cubicQ.pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b164f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cubicQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effb51c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Above indicates that for cubic 139 files are usable, but 11 are not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452f9487",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def generate_unique_natural_numbers(start_range, end_range, count):\n",
    "    # If the range is smaller than the required count, adjust the count\n",
    "    count = min(count, end_range - start_range + 1)\n",
    "\n",
    "    # Create a list of numbers in the given range\n",
    "    numbers = list(range(start_range, end_range + 1))\n",
    "\n",
    "    # Generate unique random numbers using random.sample\n",
    "    unique_random_numbers = random.sample(numbers, count)\n",
    "\n",
    "    return unique_random_numbers\n",
    "\n",
    "# Define the range and the count of unique natural numbers\n",
    "start_range = 1\n",
    "end_range = 100\n",
    "count_of_unique_numbers = 10\n",
    "\n",
    "# Generate the unique natural numbers\n",
    "unique_natural_numbers = generate_unique_natural_numbers(start_range, end_range, count_of_unique_numbers)\n",
    "\n",
    "# Print the result\n",
    "print(unique_natural_numbers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f37fd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files = []\n",
    "# test_files = []\n",
    "for cc in ccs:\n",
    "    if cc == 'cubicQ':\n",
    "        continue\n",
    "    temp_i = generate_unique_natural_numbers(0,len(usable_ind[cc])-1,100)\n",
    "#     test_i = []\n",
    "#     for i in range(0,len(usable_ind[cc])-1):\n",
    "#         if i not in temp_i:\n",
    "#             test_i.append(i)\n",
    "#     test_actual_i = [usable_ind[cc][x] for x in test_i] \n",
    "    actual_i = [usable_ind[cc][x] for x in temp_i]\n",
    "    for i in actual_i:\n",
    "        file = cc+\"-\"+str(i)+\"-0-50-200-2-aws-88-60\"\n",
    "        train_files.append(file)\n",
    "#     for i in test_actual_i:\n",
    "#         file = cc+\"-\"+str(i)+\"-0-50-200-2-aws-88-60\"\n",
    "#         test_files.append(file)\n",
    "#     print(len(usable_ind[cc]),len(train_files),len(test_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc24f42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1742e230",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9715b5",
   "metadata": {},
   "source": [
    "### Training Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8696aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "vals, cc_gaussian_params = train(ccs,cc_degree,train_files,ss=225)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de55d84",
   "metadata": {},
   "source": [
    "### Adding for cubicQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ece8946",
   "metadata": {},
   "outputs": [],
   "source": [
    "vals_cubicQ, cc_gaussian_params_cubicQ = train(['cubicQ'], {'cubicQ':3}, cubicQ, ss=225)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e3cb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_gaussian_params_cubicQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ce3c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_gaussian_params['cubicQ'] = cc_gaussian_params_cubicQ['cubicQ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58279132",
   "metadata": {},
   "outputs": [],
   "source": [
    "vals['cubicQ'] = vals_cubicQ['cubicQ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81253b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_degree['cubicQ'] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65893d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_gaussian_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c2b564",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cc in cc_gaussian_params:\n",
    "    if cc_gaussian_params[cc]['covar'].size == 1:\n",
    "        print(cc, \"Covar\", cc_gaussian_params[cc]['covar'])\n",
    "    else :\n",
    "        print(cc, \"Covar\", np.linalg.det(cc_gaussian_params[cc]['covar']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca077cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the cc_gaussian_params:\n",
    "import pickle\n",
    "with open(\"cc_gp_cubicQ_new.txt\",'wb') as f:\n",
    "    pickle.dump(cc_gaussian_params,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5506ab5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"vals_cubicQ_new.txt\",'wb') as f:\n",
    "    pickle.dump(vals,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5168b6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(15,8))\n",
    "for cc in cc_degree:\n",
    "    if cc_degree[cc] == 2:\n",
    "        x_points = []\n",
    "        y_points = []\n",
    "        for points in vals[cc][1]:\n",
    "            x_points.append(points[0])\n",
    "            y_points.append(points[1])\n",
    "        if cc in [\"dctcp\", \"lp\", \"reno\",\"highspeed\"]:\n",
    "            color = \"k\"\n",
    "            ax.scatter(x_points, y_points, label=cc,alpha=0.5, color=color)\n",
    "        else :\n",
    "            ax.scatter(x_points, y_points, label=cc,alpha=0.5)\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb16773",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d069fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 3D figure\n",
    "fig = plt.figure(figsize=(15,8))\n",
    "\n",
    "# Create a 3D axis\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "for cc in cc_degree:\n",
    "    if cc == 'nv' or cc == 'vegas':\n",
    "        continue\n",
    "#     if cc != \"cubic\":\n",
    "#         continue\n",
    "    if cc_degree[cc] == 3:\n",
    "        x_points = []\n",
    "        y_points = []\n",
    "        z_points = []\n",
    "        for points in vals[cc][1]:\n",
    "            x_points.append(points[0])\n",
    "            y_points.append(points[1])\n",
    "            z_points.append(points[2])\n",
    "        x_points = np.array(x_points)\n",
    "        y_points = np.array(y_points)\n",
    "        z_points = np.array(z_points)\n",
    "        surf = ax.scatter(x_points, y_points, z_points, label=cc)\n",
    "plt.legend()\n",
    "# Plot the 3D surface\n",
    "# surf = ax.plot_surface(x, y, z, cmap='viridis')\n",
    "\n",
    "# Add a color bar which maps values to colors\n",
    "# fig.colorbar(surf)\n",
    "\n",
    "# Set axis labels\n",
    "ax.set_xlabel('X-axis')\n",
    "ax.set_ylabel('Y-axis')\n",
    "ax.set_zlabel('Z-axis')\n",
    "\n",
    "# Set a title\n",
    "plt.title('3D Surface Plot')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ec52ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(15,8))\n",
    "for cc in cc_degree:\n",
    "    if cc == 'cubicQ':\n",
    "        continue\n",
    "    if cc_degree[cc] == 1:\n",
    "        x_points = []\n",
    "        for points in vals[cc][1]:\n",
    "            x_points.append(points[0])\n",
    "        ax.scatter(x_points,x_points, label=cc,alpha=0.5)\n",
    "    plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
